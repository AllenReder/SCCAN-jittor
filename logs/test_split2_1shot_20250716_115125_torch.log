[38;5;2m[i 0716 11:51:25.707280 52 compiler.py:956] Jittor(1.3.9.14) src: /data1/fanlyu/anaconda3/envs/jittor/lib/python3.7/site-packages/jittor[m
[38;5;2m[i 0716 11:51:25.709861 52 compiler.py:957] g++ at /usr/bin/g++(11.4.0)[m
[38;5;2m[i 0716 11:51:25.709907 52 compiler.py:958] cache_path: /data1/fanlyu/.cache/jittor/jt1.3.9/g++11.4.0/py3.7.16/Linux-5.15.0-1xe3/IntelRXeonRGolx62/efa5/default[m
[38;5;2m[i 0716 11:51:25.776894 52 install_cuda.py:96] cuda_driver_version: [12, 4][m
[38;5;2m[i 0716 11:51:25.777149 52 install_cuda.py:84] restart /data1/fanlyu/anaconda3/envs/jittor/bin/python ['test_sccan.py', '--config=config/pascal/pascal_split2_resnet50_torch.yaml', '--viz'][m
[38;5;2m[i 0716 11:51:25.960085 12 compiler.py:956] Jittor(1.3.9.14) src: /data1/fanlyu/anaconda3/envs/jittor/lib/python3.7/site-packages/jittor[m
[38;5;2m[i 0716 11:51:25.962873 12 compiler.py:957] g++ at /usr/bin/g++(11.4.0)[m
[38;5;2m[i 0716 11:51:25.962918 12 compiler.py:958] cache_path: /data1/fanlyu/.cache/jittor/jt1.3.9/g++11.4.0/py3.7.16/Linux-5.15.0-1xe3/IntelRXeonRGolx62/efa5/default[m
[38;5;2m[i 0716 11:51:26.004586 12 install_cuda.py:96] cuda_driver_version: [12, 4][m
[38;5;2m[i 0716 11:51:26.009100 12 __init__.py:412] Found /data1/fanlyu/.cache/jittor/jtcuda/cuda12.2_cudnn8_linux/bin/nvcc(12.2.140) at /data1/fanlyu/.cache/jittor/jtcuda/cuda12.2_cudnn8_linux/bin/nvcc.[m
[38;5;2m[i 0716 11:51:26.012276 12 __init__.py:412] Found addr2line(2.38) at /usr/bin/addr2line.[m
[38;5;2m[i 0716 11:51:26.137817 12 compiler.py:1013] cuda key:cu12.2.140_sm_89[m
[38;5;2m[i 0716 11:51:26.464619 12 __init__.py:227] Total mem: 503.54GB, using 16 procs for compiling.[m
[38;5;2m[i 0716 11:51:26.533586 12 jit_compiler.cc:28] Load cc_path: /usr/bin/g++[m
[38;5;2m[i 0716 11:51:26.628856 12 init.cc:63] Found cuda archs: [89,][m
[2025-07-16 11:51:28,373 INFO test_sccan.py line 102 2197486] => creating model ...
[38;5;2m[i 0716 11:51:28.613344 12 cuda_flags.cc:49] CUDA enabled.[m
[2025-07-16 11:51:28,715 INFO test_sccan.py line 66 2197486] => loading checkpoint 'exp/pascal/SCCAN/split2_1shot/resnet50/snapshot/torch_train_epoch_60_0.6634.pth'
[2025-07-16 11:51:28,819 INFO test_sccan.py line 76 2197486] => loaded checkpoint 'exp/pascal/SCCAN/split2_1shot/resnet50/snapshot/torch_train_epoch_60_0.6634.pth' (epoch 60)
[2025-07-16 11:51:33,827 INFO test_sccan.py line 104 2197486] OneModel(
    criterion: CrossEntropyLoss(None, ignore_index=255)
    criterion_dice: WeightedDiceLoss(1.0, reduction=sum)
    backbone: Backbone(
        backbone: ResNet(
            conv1: Conv(3, 64, (3, 3), (2, 2), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
            bn1: FrozenBatchNorm2d(None)
            relu1: relu()
            conv2: Conv(64, 64, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
            bn2: FrozenBatchNorm2d(None)
            relu2: relu()
            conv3: Conv(64, 128, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
            bn3: FrozenBatchNorm2d(None)
            relu3: relu()
            maxpool: Pool((3, 3), (2, 2), (1, 1), dilation=None, return_indices=None, ceil_mode=False, count_include_pad=True, op=maximum, item=None)
            layer1: Sequential(
                0: Bottleneck(
                    conv1: Conv(128, 64, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(64, 64, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(64, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                    downsample: Sequential(
                        0: Conv(128, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                        1: FrozenBatchNorm2d(None)
                    )
                )
                1: Bottleneck(
                    conv1: Conv(256, 64, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(64, 64, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(64, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                2: Bottleneck(
                    conv1: Conv(256, 64, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(64, 64, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(64, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
            )
            layer2: Sequential(
                0: Bottleneck(
                    conv1: Conv(256, 128, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(128, 128, (3, 3), (2, 2), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(128, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                    downsample: Sequential(
                        0: Conv(256, 512, (1, 1), (2, 2), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                        1: FrozenBatchNorm2d(None)
                    )
                )
                1: Bottleneck(
                    conv1: Conv(512, 128, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(128, 128, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(128, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                2: Bottleneck(
                    conv1: Conv(512, 128, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(128, 128, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(128, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                3: Bottleneck(
                    conv1: Conv(512, 128, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(128, 128, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(128, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
            )
            layer3: Sequential(
                0: Bottleneck(
                    conv1: Conv(512, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(256, 256, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(256, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                    downsample: Sequential(
                        0: Conv(512, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                        1: FrozenBatchNorm2d(None)
                    )
                )
                1: Bottleneck(
                    conv1: Conv(1024, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(256, 256, (3, 3), (1, 1), (2, 2), (2, 2), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(256, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                2: Bottleneck(
                    conv1: Conv(1024, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(256, 256, (3, 3), (1, 1), (2, 2), (2, 2), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(256, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                3: Bottleneck(
                    conv1: Conv(1024, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(256, 256, (3, 3), (1, 1), (2, 2), (2, 2), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(256, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                4: Bottleneck(
                    conv1: Conv(1024, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(256, 256, (3, 3), (1, 1), (2, 2), (2, 2), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(256, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                5: Bottleneck(
                    conv1: Conv(1024, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(256, 256, (3, 3), (1, 1), (2, 2), (2, 2), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(256, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
            )
            layer4: Sequential(
                0: Bottleneck(
                    conv1: Conv(1024, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(512, 512, (3, 3), (1, 1), (2, 2), (2, 2), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(512, 2048, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                    downsample: Sequential(
                        0: Conv(1024, 2048, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                        1: FrozenBatchNorm2d(None)
                    )
                )
                1: Bottleneck(
                    conv1: Conv(2048, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(512, 512, (3, 3), (1, 1), (4, 4), (4, 4), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(512, 2048, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                2: Bottleneck(
                    conv1: Conv(2048, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(512, 512, (3, 3), (1, 1), (4, 4), (4, 4), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(512, 2048, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
            )
            avgpool: AdaptiveAvgPool2d((1, 1))
            fc: Linear(2048, 1000, float32[1000,], None)
        )
    )
    down_query: Sequential(
        0: Conv(1536, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
        1: relu()
        2: Dropout2d(0.5, is_train=False)
    )
    down_supp: Sequential(
        0: Conv(1536, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
        1: relu()
        2: Dropout2d(0.5, is_train=False)
    )
    init_merge_query: Sequential(
        0: Conv(513, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
    )
    init_merge_supp: Sequential(
        0: Conv(513, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
    )
    transformer: SwinTransformer(
        pos_drop: Dropout(0.0, is_train=False)
        layers: Sequential(
            0: BasicLayer(
                blocks: Sequential(
                    0: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    1: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    2: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    3: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    4: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    5: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    6: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    7: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                )
            )
        )
        norm0: LayerNorm((256,), 1e-05, elementwise_affine=True)
    )
    ASPP_meta: ASPP(
        layer6_0: Sequential(
            0: Conv(256, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, float32[256,], None, None, Kw=None, fan=None, i=None, bound=None)
            1: relu()
        )
        layer6_1: Sequential(
            0: Conv(256, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, float32[256,], None, None, Kw=None, fan=None, i=None, bound=None)
            1: relu()
        )
        layer6_2: Sequential(
            0: Conv(256, 256, (3, 3), (1, 1), (6, 6), (6, 6), 1, float32[256,], None, None, Kw=None, fan=None, i=None, bound=None)
            1: relu()
        )
        layer6_3: Sequential(
            0: Conv(256, 256, (3, 3), (1, 1), (12, 12), (12, 12), 1, float32[256,], None, None, Kw=None, fan=None, i=None, bound=None)
            1: relu()
        )
        layer6_4: Sequential(
            0: Conv(256, 256, (3, 3), (1, 1), (18, 18), (18, 18), 1, float32[256,], None, None, Kw=None, fan=None, i=None, bound=None)
            1: relu()
        )
    )
    res1_meta: Sequential(
        0: Conv(1280, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
        1: relu()
    )
    res2_meta: Sequential(
        0: Conv(256, 256, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
        1: relu()
        2: Conv(256, 256, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
        3: relu()
    )
    cls_meta: Sequential(
        0: Conv(256, 256, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
        1: relu()
        2: Dropout2d(0.1, is_train=False)
        3: Conv(256, 2, (1, 1), (1, 1), (0, 0), (1, 1), 1, float32[2,], None, None, Kw=None, fan=None, i=None, bound=None)
    )
    relu: relu()
)
[2025-07-16 11:51:33,833 INFO test_sccan.py line 166 2197486] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
SubEpoch_val: True
ann_type: mask
arch: SCCAN
aux_weight1: 1.0
aux_weight2: 1.0
base_lr: 0.005
batch_size: 8
batch_size_val: 1
classes: 2
data_root: ../data/VOCdevkit2012/VOC2012
data_set: pascal
epochs: 200
evaluate: True
fix_bn: True
fix_random_seed_val: True
ignore_label: 255
index_split: -1
kshot_trans_dim: 2
layers: 50
low_fea: layer2
manual_seed: 321
merge: final
merge_tau: 0.9
momentum: 0.9
opts: None
ori_resize: True
padding_label: 255
power: 0.9
print_freq: 10
resized_val: True
resume: None
rotate_max: 10
rotate_min: -10
save_freq: 10
scale_max: 1.1
scale_min: 0.9
seed_deterministic: False
shot: 1
split: 2
start_epoch: 0
stop_interval: 75
train_h: 473
train_list: ./lists/pascal/voc_sbd_merge_noduplicate.txt
train_w: 473
use_split_coco: False
val_list: ./lists/pascal/val.txt
val_size: 473
vgg: False
viz: True
warmup: False
weight: torch_train_epoch_60_0.6634.pth
weight_decay: 0.0001
workers: 8
zoom_factor: 8
Number of Parameters: 37107626
Number of Learnable Parameters: 11373314
sub_list:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 16, 17, 18, 19, 20]
sub_val_list:  [11, 12, 13, 14, 15]
Val: [1/1] 	 Seed: 321
[2025-07-16 11:51:37,596 INFO test_sccan.py line 251 2197486] Test: [10/1000] Data 0.001 (0.226) Batch 0.050 (0.376) Remain 00:06:12 Loss 0.1156 (0.6604) Accuracy 0.9784.
[2025-07-16 11:51:38,092 INFO test_sccan.py line 251 2197486] Test: [20/1000] Data 0.001 (0.114) Batch 0.050 (0.213) Remain 00:03:28 Loss 3.1327 (0.7379) Accuracy 0.6073.
[2025-07-16 11:51:38,591 INFO test_sccan.py line 251 2197486] Test: [30/1000] Data 0.001 (0.076) Batch 0.051 (0.158) Remain 00:02:33 Loss 0.1574 (0.6941) Accuracy 0.9796.
[2025-07-16 11:51:39,084 INFO test_sccan.py line 251 2197486] Test: [40/1000] Data 0.001 (0.057) Batch 0.048 (0.131) Remain 00:02:05 Loss 0.3694 (0.6959) Accuracy 0.9189.
[2025-07-16 11:51:39,577 INFO test_sccan.py line 251 2197486] Test: [50/1000] Data 0.001 (0.046) Batch 0.049 (0.115) Remain 00:01:49 Loss 0.5115 (0.6653) Accuracy 0.8057.
[2025-07-16 11:51:40,066 INFO test_sccan.py line 251 2197486] Test: [60/1000] Data 0.001 (0.038) Batch 0.051 (0.104) Remain 00:01:37 Loss 2.7292 (0.8440) Accuracy 0.7673.
[2025-07-16 11:51:40,552 INFO test_sccan.py line 251 2197486] Test: [70/1000] Data 0.001 (0.033) Batch 0.049 (0.096) Remain 00:01:29 Loss 1.6785 (0.8082) Accuracy 0.7835.
[2025-07-16 11:51:41,037 INFO test_sccan.py line 251 2197486] Test: [80/1000] Data 0.001 (0.029) Batch 0.047 (0.090) Remain 00:01:22 Loss 0.8961 (0.7783) Accuracy 0.8894.
[2025-07-16 11:51:41,526 INFO test_sccan.py line 251 2197486] Test: [90/1000] Data 0.001 (0.026) Batch 0.049 (0.085) Remain 00:01:17 Loss 0.7076 (0.8656) Accuracy 0.7640.
[2025-07-16 11:51:42,038 INFO test_sccan.py line 251 2197486] Test: [100/1000] Data 0.001 (0.023) Batch 0.062 (0.082) Remain 00:01:13 Loss 1.4219 (0.8793) Accuracy 0.7943.
[2025-07-16 11:51:42,534 INFO test_sccan.py line 251 2197486] Test: [110/1000] Data 0.001 (0.021) Batch 0.049 (0.079) Remain 00:01:10 Loss 1.1258 (0.8840) Accuracy 0.9137.
[2025-07-16 11:51:43,033 INFO test_sccan.py line 251 2197486] Test: [120/1000] Data 0.001 (0.020) Batch 0.047 (0.077) Remain 00:01:07 Loss 0.0467 (0.9321) Accuracy 0.9825.
[2025-07-16 11:51:43,531 INFO test_sccan.py line 251 2197486] Test: [130/1000] Data 0.001 (0.018) Batch 0.049 (0.075) Remain 00:01:04 Loss 0.1413 (0.8942) Accuracy 0.9723.
[2025-07-16 11:51:44,046 INFO test_sccan.py line 251 2197486] Test: [140/1000] Data 0.001 (0.017) Batch 0.062 (0.073) Remain 00:01:02 Loss 3.0360 (0.9357) Accuracy 0.5343.
[2025-07-16 11:51:44,542 INFO test_sccan.py line 251 2197486] Test: [150/1000] Data 0.001 (0.016) Batch 0.048 (0.071) Remain 00:01:00 Loss 0.5106 (0.9206) Accuracy 0.8577.
[2025-07-16 11:51:45,039 INFO test_sccan.py line 251 2197486] Test: [160/1000] Data 0.001 (0.015) Batch 0.047 (0.070) Remain 00:00:58 Loss 0.0387 (0.9419) Accuracy 0.9902.
[2025-07-16 11:51:45,538 INFO test_sccan.py line 251 2197486] Test: [170/1000] Data 0.001 (0.014) Batch 0.048 (0.069) Remain 00:00:57 Loss 1.1506 (0.9040) Accuracy 0.6453.
[2025-07-16 11:51:46,049 INFO test_sccan.py line 251 2197486] Test: [180/1000] Data 0.001 (0.013) Batch 0.063 (0.068) Remain 00:00:55 Loss 2.7360 (0.9430) Accuracy 0.4045.
[2025-07-16 11:51:46,544 INFO test_sccan.py line 251 2197486] Test: [190/1000] Data 0.001 (0.013) Batch 0.049 (0.067) Remain 00:00:54 Loss 0.0158 (0.9217) Accuracy 0.9936.
[2025-07-16 11:51:47,041 INFO test_sccan.py line 251 2197486] Test: [200/1000] Data 0.001 (0.012) Batch 0.047 (0.066) Remain 00:00:52 Loss 0.3398 (0.9063) Accuracy 0.9284.
[2025-07-16 11:51:47,540 INFO test_sccan.py line 251 2197486] Test: [210/1000] Data 0.001 (0.012) Batch 0.049 (0.065) Remain 00:00:51 Loss 0.5437 (0.9083) Accuracy 0.7683.
[2025-07-16 11:51:48,055 INFO test_sccan.py line 251 2197486] Test: [220/1000] Data 0.001 (0.011) Batch 0.064 (0.065) Remain 00:00:50 Loss 1.1136 (0.9101) Accuracy 0.6757.
[2025-07-16 11:51:48,550 INFO test_sccan.py line 251 2197486] Test: [230/1000] Data 0.001 (0.011) Batch 0.048 (0.064) Remain 00:00:49 Loss 2.6509 (0.9036) Accuracy 0.6147.
[2025-07-16 11:51:49,052 INFO test_sccan.py line 251 2197486] Test: [240/1000] Data 0.001 (0.010) Batch 0.047 (0.063) Remain 00:00:48 Loss 4.7885 (0.9010) Accuracy 0.5169.
[2025-07-16 11:51:49,552 INFO test_sccan.py line 251 2197486] Test: [250/1000] Data 0.001 (0.010) Batch 0.049 (0.063) Remain 00:00:47 Loss 0.2930 (0.8720) Accuracy 0.9539.
[2025-07-16 11:51:50,067 INFO test_sccan.py line 251 2197486] Test: [260/1000] Data 0.001 (0.010) Batch 0.064 (0.062) Remain 00:00:46 Loss 0.8245 (0.8526) Accuracy 0.9132.
[2025-07-16 11:51:50,566 INFO test_sccan.py line 251 2197486] Test: [270/1000] Data 0.001 (0.009) Batch 0.048 (0.062) Remain 00:00:45 Loss 0.0350 (0.8409) Accuracy 0.9918.
[2025-07-16 11:51:51,066 INFO test_sccan.py line 251 2197486] Test: [280/1000] Data 0.001 (0.009) Batch 0.047 (0.062) Remain 00:00:44 Loss 0.1552 (0.8246) Accuracy 0.9472.
[2025-07-16 11:51:51,564 INFO test_sccan.py line 251 2197486] Test: [290/1000] Data 0.001 (0.009) Batch 0.048 (0.061) Remain 00:00:43 Loss 0.0925 (0.8368) Accuracy 0.9577.
[2025-07-16 11:51:52,079 INFO test_sccan.py line 251 2197486] Test: [300/1000] Data 0.001 (0.008) Batch 0.063 (0.061) Remain 00:00:42 Loss 0.2936 (0.8237) Accuracy 0.9078.
[2025-07-16 11:51:52,577 INFO test_sccan.py line 251 2197486] Test: [310/1000] Data 0.001 (0.008) Batch 0.050 (0.060) Remain 00:00:41 Loss 0.0098 (0.8239) Accuracy 0.9961.
[2025-07-16 11:51:53,074 INFO test_sccan.py line 251 2197486] Test: [320/1000] Data 0.001 (0.008) Batch 0.047 (0.060) Remain 00:00:40 Loss 0.3684 (0.8194) Accuracy 0.8499.
[2025-07-16 11:51:53,570 INFO test_sccan.py line 251 2197486] Test: [330/1000] Data 0.001 (0.008) Batch 0.049 (0.060) Remain 00:00:40 Loss 2.3745 (0.8177) Accuracy 0.6036.
[2025-07-16 11:51:54,084 INFO test_sccan.py line 251 2197486] Test: [340/1000] Data 0.001 (0.008) Batch 0.062 (0.060) Remain 00:00:39 Loss 0.0440 (0.8030) Accuracy 0.9852.
[2025-07-16 11:51:54,580 INFO test_sccan.py line 251 2197486] Test: [350/1000] Data 0.001 (0.007) Batch 0.049 (0.059) Remain 00:00:38 Loss 0.4073 (0.8037) Accuracy 0.9202.
[2025-07-16 11:51:55,079 INFO test_sccan.py line 251 2197486] Test: [360/1000] Data 0.001 (0.007) Batch 0.049 (0.059) Remain 00:00:37 Loss 0.1502 (0.8166) Accuracy 0.9764.
[2025-07-16 11:51:55,577 INFO test_sccan.py line 251 2197486] Test: [370/1000] Data 0.001 (0.007) Batch 0.049 (0.059) Remain 00:00:37 Loss 0.3029 (0.8101) Accuracy 0.9199.
[2025-07-16 11:51:56,089 INFO test_sccan.py line 251 2197486] Test: [380/1000] Data 0.001 (0.007) Batch 0.062 (0.059) Remain 00:00:36 Loss 0.0029 (0.7960) Accuracy 0.9988.
[2025-07-16 11:51:56,581 INFO test_sccan.py line 251 2197486] Test: [390/1000] Data 0.001 (0.007) Batch 0.047 (0.058) Remain 00:00:35 Loss 0.6558 (0.8021) Accuracy 0.9498.
[2025-07-16 11:51:57,079 INFO test_sccan.py line 251 2197486] Test: [400/1000] Data 0.001 (0.007) Batch 0.049 (0.058) Remain 00:00:34 Loss 0.2504 (0.7995) Accuracy 0.9160.
[2025-07-16 11:51:57,577 INFO test_sccan.py line 251 2197486] Test: [410/1000] Data 0.001 (0.006) Batch 0.048 (0.058) Remain 00:00:34 Loss 2.3672 (0.7988) Accuracy 0.7375.
[2025-07-16 11:51:58,090 INFO test_sccan.py line 251 2197486] Test: [420/1000] Data 0.001 (0.006) Batch 0.062 (0.058) Remain 00:00:33 Loss 0.0472 (0.7932) Accuracy 0.9810.
[2025-07-16 11:51:58,593 INFO test_sccan.py line 251 2197486] Test: [430/1000] Data 0.001 (0.006) Batch 0.049 (0.058) Remain 00:00:32 Loss 0.0134 (0.7891) Accuracy 0.9959.
[2025-07-16 11:51:59,096 INFO test_sccan.py line 251 2197486] Test: [440/1000] Data 0.001 (0.006) Batch 0.049 (0.057) Remain 00:00:32 Loss 5.4899 (0.8041) Accuracy 0.3614.
[2025-07-16 11:51:59,599 INFO test_sccan.py line 251 2197486] Test: [450/1000] Data 0.001 (0.006) Batch 0.048 (0.057) Remain 00:00:31 Loss 0.6401 (0.8065) Accuracy 0.8143.
[2025-07-16 11:52:00,115 INFO test_sccan.py line 251 2197486] Test: [460/1000] Data 0.001 (0.006) Batch 0.062 (0.057) Remain 00:00:30 Loss 0.6407 (0.8153) Accuracy 0.8720.
[2025-07-16 11:52:00,618 INFO test_sccan.py line 251 2197486] Test: [470/1000] Data 0.001 (0.006) Batch 0.048 (0.057) Remain 00:00:30 Loss 0.0515 (0.8137) Accuracy 0.9902.
[2025-07-16 11:52:01,118 INFO test_sccan.py line 251 2197486] Test: [480/1000] Data 0.001 (0.006) Batch 0.049 (0.057) Remain 00:00:29 Loss 0.0156 (0.8117) Accuracy 0.9947.
[2025-07-16 11:52:01,621 INFO test_sccan.py line 251 2197486] Test: [490/1000] Data 0.001 (0.006) Batch 0.049 (0.057) Remain 00:00:28 Loss 0.0738 (0.8091) Accuracy 0.9731.
[2025-07-16 11:52:02,147 INFO test_sccan.py line 251 2197486] Test: [500/1000] Data 0.001 (0.005) Batch 0.061 (0.057) Remain 00:00:28 Loss 0.8102 (0.8058) Accuracy 0.7495.
[2025-07-16 11:52:02,661 INFO test_sccan.py line 251 2197486] Test: [510/1000] Data 0.001 (0.005) Batch 0.049 (0.057) Remain 00:00:27 Loss 0.0173 (0.8034) Accuracy 0.9946.
[2025-07-16 11:52:03,171 INFO test_sccan.py line 251 2197486] Test: [520/1000] Data 0.001 (0.005) Batch 0.049 (0.056) Remain 00:00:27 Loss 1.0511 (0.8090) Accuracy 0.8464.
[2025-07-16 11:52:03,692 INFO test_sccan.py line 251 2197486] Test: [530/1000] Data 0.001 (0.005) Batch 0.047 (0.056) Remain 00:00:26 Loss 1.6956 (0.8099) Accuracy 0.6387.
[2025-07-16 11:52:04,215 INFO test_sccan.py line 251 2197486] Test: [540/1000] Data 0.001 (0.005) Batch 0.060 (0.056) Remain 00:00:25 Loss 0.0067 (0.8101) Accuracy 0.9979.
[2025-07-16 11:52:04,725 INFO test_sccan.py line 251 2197486] Test: [550/1000] Data 0.001 (0.005) Batch 0.049 (0.056) Remain 00:00:25 Loss 0.0094 (0.8090) Accuracy 0.9967.
[2025-07-16 11:52:05,237 INFO test_sccan.py line 251 2197486] Test: [560/1000] Data 0.001 (0.005) Batch 0.049 (0.056) Remain 00:00:24 Loss 0.7250 (0.8004) Accuracy 0.8447.
[2025-07-16 11:52:05,759 INFO test_sccan.py line 251 2197486] Test: [570/1000] Data 0.001 (0.005) Batch 0.047 (0.056) Remain 00:00:24 Loss 0.0253 (0.7907) Accuracy 0.9918.
[2025-07-16 11:52:06,285 INFO test_sccan.py line 251 2197486] Test: [580/1000] Data 0.001 (0.005) Batch 0.062 (0.056) Remain 00:00:23 Loss 0.1652 (0.7897) Accuracy 0.9437.
[2025-07-16 11:52:06,797 INFO test_sccan.py line 251 2197486] Test: [590/1000] Data 0.001 (0.005) Batch 0.049 (0.056) Remain 00:00:22 Loss 2.9266 (0.7875) Accuracy 0.6549.
[2025-07-16 11:52:07,421 INFO test_sccan.py line 251 2197486] Test: [602/1000] Data 0.001 (0.005) Batch 0.047 (0.056) Remain 00:00:22 Loss 0.1437 (0.7812) Accuracy 0.9729.
[2025-07-16 11:52:07,945 INFO test_sccan.py line 251 2197486] Test: [612/1000] Data 0.001 (0.005) Batch 0.063 (0.056) Remain 00:00:21 Loss 3.1219 (0.7822) Accuracy 0.6143.
[2025-07-16 11:52:08,454 INFO test_sccan.py line 251 2197486] Test: [622/1000] Data 0.001 (0.005) Batch 0.049 (0.056) Remain 00:00:21 Loss 0.1889 (0.7775) Accuracy 0.9797.
[2025-07-16 11:52:08,963 INFO test_sccan.py line 251 2197486] Test: [632/1000] Data 0.001 (0.004) Batch 0.049 (0.056) Remain 00:00:20 Loss 0.4865 (0.7814) Accuracy 0.8740.
[2025-07-16 11:52:09,486 INFO test_sccan.py line 251 2197486] Test: [642/1000] Data 0.001 (0.004) Batch 0.047 (0.056) Remain 00:00:19 Loss 0.9525 (0.7824) Accuracy 0.7518.
[2025-07-16 11:52:10,012 INFO test_sccan.py line 251 2197486] Test: [652/1000] Data 0.001 (0.004) Batch 0.062 (0.055) Remain 00:00:19 Loss 1.8561 (0.7864) Accuracy 0.7823.
[2025-07-16 11:52:10,525 INFO test_sccan.py line 251 2197486] Test: [662/1000] Data 0.001 (0.004) Batch 0.049 (0.055) Remain 00:00:18 Loss 1.1063 (0.7847) Accuracy 0.8599.
[2025-07-16 11:52:11,035 INFO test_sccan.py line 251 2197486] Test: [672/1000] Data 0.001 (0.004) Batch 0.049 (0.055) Remain 00:00:18 Loss 0.9081 (0.7812) Accuracy 0.8844.
[2025-07-16 11:52:11,558 INFO test_sccan.py line 251 2197486] Test: [682/1000] Data 0.001 (0.004) Batch 0.047 (0.055) Remain 00:00:17 Loss 1.7057 (0.7854) Accuracy 0.7249.
[2025-07-16 11:52:12,084 INFO test_sccan.py line 251 2197486] Test: [692/1000] Data 0.001 (0.004) Batch 0.062 (0.055) Remain 00:00:17 Loss 0.3298 (0.7856) Accuracy 0.9185.
[2025-07-16 11:52:12,597 INFO test_sccan.py line 251 2197486] Test: [702/1000] Data 0.001 (0.004) Batch 0.050 (0.055) Remain 00:00:16 Loss 1.0754 (0.7915) Accuracy 0.9138.
[2025-07-16 11:52:13,110 INFO test_sccan.py line 251 2197486] Test: [712/1000] Data 0.001 (0.004) Batch 0.049 (0.055) Remain 00:00:15 Loss 0.0684 (0.8019) Accuracy 0.9770.
[2025-07-16 11:52:13,634 INFO test_sccan.py line 251 2197486] Test: [722/1000] Data 0.001 (0.004) Batch 0.047 (0.055) Remain 00:00:15 Loss 0.6258 (0.7972) Accuracy 0.9126.
[2025-07-16 11:52:14,159 INFO test_sccan.py line 251 2197486] Test: [732/1000] Data 0.001 (0.004) Batch 0.063 (0.055) Remain 00:00:14 Loss 3.4127 (0.8003) Accuracy 0.5214.
[2025-07-16 11:52:14,673 INFO test_sccan.py line 251 2197486] Test: [742/1000] Data 0.001 (0.004) Batch 0.050 (0.055) Remain 00:00:14 Loss 2.6465 (0.8021) Accuracy 0.4541.
[2025-07-16 11:52:15,185 INFO test_sccan.py line 251 2197486] Test: [752/1000] Data 0.001 (0.004) Batch 0.049 (0.055) Remain 00:00:13 Loss 0.0136 (0.7995) Accuracy 0.9935.
[2025-07-16 11:52:15,708 INFO test_sccan.py line 251 2197486] Test: [762/1000] Data 0.001 (0.004) Batch 0.047 (0.055) Remain 00:00:13 Loss 1.1622 (0.7923) Accuracy 0.6986.
[2025-07-16 11:52:16,232 INFO test_sccan.py line 251 2197486] Test: [772/1000] Data 0.001 (0.004) Batch 0.061 (0.055) Remain 00:00:12 Loss 3.0160 (0.8017) Accuracy 0.6058.
[2025-07-16 11:52:16,745 INFO test_sccan.py line 251 2197486] Test: [782/1000] Data 0.001 (0.004) Batch 0.049 (0.055) Remain 00:00:11 Loss 0.0223 (0.8005) Accuracy 0.9918.
[2025-07-16 11:52:17,261 INFO test_sccan.py line 251 2197486] Test: [792/1000] Data 0.001 (0.004) Batch 0.050 (0.055) Remain 00:00:11 Loss 0.6562 (0.8033) Accuracy 0.9232.
[2025-07-16 11:52:17,783 INFO test_sccan.py line 251 2197486] Test: [802/1000] Data 0.001 (0.004) Batch 0.047 (0.055) Remain 00:00:10 Loss 0.4065 (0.8016) Accuracy 0.7556.
[2025-07-16 11:52:18,311 INFO test_sccan.py line 251 2197486] Test: [812/1000] Data 0.001 (0.004) Batch 0.064 (0.055) Remain 00:00:10 Loss 1.1616 (0.8047) Accuracy 0.5997.
[2025-07-16 11:52:18,823 INFO test_sccan.py line 251 2197486] Test: [822/1000] Data 0.001 (0.004) Batch 0.049 (0.055) Remain 00:00:09 Loss 2.9903 (0.8019) Accuracy 0.6135.
[2025-07-16 11:52:19,332 INFO test_sccan.py line 251 2197486] Test: [832/1000] Data 0.001 (0.004) Batch 0.049 (0.055) Remain 00:00:09 Loss 4.0101 (0.8042) Accuracy 0.6383.
[2025-07-16 11:52:19,856 INFO test_sccan.py line 251 2197486] Test: [842/1000] Data 0.001 (0.004) Batch 0.047 (0.055) Remain 00:00:08 Loss 0.2316 (0.7977) Accuracy 0.9529.
[2025-07-16 11:52:20,384 INFO test_sccan.py line 251 2197486] Test: [852/1000] Data 0.001 (0.004) Batch 0.062 (0.055) Remain 00:00:08 Loss 0.7753 (0.7936) Accuracy 0.9165.
[2025-07-16 11:52:20,895 INFO test_sccan.py line 251 2197486] Test: [862/1000] Data 0.001 (0.004) Batch 0.049 (0.055) Remain 00:00:07 Loss 0.0315 (0.7931) Accuracy 0.9923.
[2025-07-16 11:52:21,405 INFO test_sccan.py line 251 2197486] Test: [872/1000] Data 0.001 (0.004) Batch 0.049 (0.055) Remain 00:00:06 Loss 0.5593 (0.7883) Accuracy 0.9416.
[2025-07-16 11:52:21,931 INFO test_sccan.py line 251 2197486] Test: [882/1000] Data 0.001 (0.003) Batch 0.048 (0.055) Remain 00:00:06 Loss 0.0999 (0.7894) Accuracy 0.9533.
[2025-07-16 11:52:22,456 INFO test_sccan.py line 251 2197486] Test: [892/1000] Data 0.001 (0.003) Batch 0.062 (0.055) Remain 00:00:05 Loss 1.2267 (0.7857) Accuracy 0.6014.
[2025-07-16 11:52:22,970 INFO test_sccan.py line 251 2197486] Test: [902/1000] Data 0.001 (0.003) Batch 0.049 (0.054) Remain 00:00:05 Loss 0.0246 (0.7881) Accuracy 0.9922.
[2025-07-16 11:52:23,479 INFO test_sccan.py line 251 2197486] Test: [912/1000] Data 0.001 (0.003) Batch 0.049 (0.054) Remain 00:00:04 Loss 0.6514 (0.7884) Accuracy 0.8164.
[2025-07-16 11:52:23,999 INFO test_sccan.py line 251 2197486] Test: [922/1000] Data 0.001 (0.003) Batch 0.047 (0.054) Remain 00:00:04 Loss 2.7376 (0.7857) Accuracy 0.6034.
[2025-07-16 11:52:24,526 INFO test_sccan.py line 251 2197486] Test: [932/1000] Data 0.001 (0.003) Batch 0.062 (0.054) Remain 00:00:03 Loss 0.0484 (0.7833) Accuracy 0.9839.
[2025-07-16 11:52:25,041 INFO test_sccan.py line 251 2197486] Test: [942/1000] Data 0.001 (0.003) Batch 0.049 (0.054) Remain 00:00:03 Loss 0.3905 (0.7851) Accuracy 0.9186.
[2025-07-16 11:52:25,550 INFO test_sccan.py line 251 2197486] Test: [952/1000] Data 0.001 (0.003) Batch 0.050 (0.054) Remain 00:00:02 Loss 0.1525 (0.7913) Accuracy 0.9774.
[2025-07-16 11:52:26,077 INFO test_sccan.py line 251 2197486] Test: [962/1000] Data 0.001 (0.003) Batch 0.049 (0.054) Remain 00:00:02 Loss 0.5689 (0.7884) Accuracy 0.8975.
[2025-07-16 11:52:26,607 INFO test_sccan.py line 251 2197486] Test: [972/1000] Data 0.001 (0.003) Batch 0.062 (0.054) Remain 00:00:01 Loss 0.0040 (0.7829) Accuracy 0.9983.
[2025-07-16 11:52:27,121 INFO test_sccan.py line 251 2197486] Test: [982/1000] Data 0.001 (0.003) Batch 0.049 (0.054) Remain 00:00:00 Loss 0.6720 (0.7862) Accuracy 0.9501.
[2025-07-16 11:52:27,634 INFO test_sccan.py line 251 2197486] Test: [992/1000] Data 0.001 (0.003) Batch 0.047 (0.054) Remain 00:00:00 Loss 2.3415 (0.7848) Accuracy 0.7133.
[2025-07-16 11:52:28,052 INFO test_sccan.py line 265 2197486] meanIoU---Val result: mIoU 0.6561.
[2025-07-16 11:52:28,052 INFO test_sccan.py line 266 2197486] <<<<<<< Novel Results <<<<<<<
[2025-07-16 11:52:28,052 INFO test_sccan.py line 268 2197486] Class_1 Result: iou 0.2826.
[2025-07-16 11:52:28,052 INFO test_sccan.py line 268 2197486] Class_2 Result: iou 0.8370.
[2025-07-16 11:52:28,052 INFO test_sccan.py line 268 2197486] Class_3 Result: iou 0.8777.
[2025-07-16 11:52:28,052 INFO test_sccan.py line 268 2197486] Class_4 Result: iou 0.7988.
[2025-07-16 11:52:28,052 INFO test_sccan.py line 268 2197486] Class_5 Result: iou 0.4846.
[2025-07-16 11:52:28,052 INFO test_sccan.py line 270 2197486] FBIoU---Val result: FBIoU 0.7002.
[2025-07-16 11:52:28,052 INFO test_sccan.py line 272 2197486] Class_0 Result: iou_f 0.8332.
[2025-07-16 11:52:28,052 INFO test_sccan.py line 272 2197486] Class_1 Result: iou_f 0.5671.
[2025-07-16 11:52:28,052 INFO test_sccan.py line 273 2197486] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
total time: 54.2153, avg inference time: 0.0126, count: 1000

Total running time: 00h 00m 54s
Seed0: 123
Seed:  [321]
mIoU:  [0.6561]
FBIoU: [0.7002]
pIoU:  [0.5671]
-------------------------------------------
Best_Seed_m: 321 	 Best_Seed_F: 321 	 Best_Seed_p: 321
Best_mIoU: 0.6561 	 Best_FBIoU: 0.7002 	 Best_pIoU: 0.5671
Mean_mIoU: 0.6561 	 Mean_FBIoU: 0.7002 	 Mean_pIoU: 0.5671
