[i 0718 09:46:17.863133 32 compiler.py:956] Jittor(1.3.9.14) src: /data1/fanlyu/anaconda3/envs/jittor/lib/python3.7/site-packages/jittor
[i 0718 09:46:17.872673 32 compiler.py:957] g++ at /usr/bin/g++(11.4.0)
[i 0718 09:46:17.872731 32 compiler.py:958] cache_path: /data1/fanlyu/.cache/jittor/jt1.3.9/g++11.4.0/py3.7.16/Linux-5.15.0-1xe3/IntelRXeonRGolx62/efa5/default
[i 0718 09:46:17.946813 32 install_cuda.py:96] cuda_driver_version: [12, 4]
[i 0718 09:46:17.947417 32 install_cuda.py:84] restart /data1/fanlyu/anaconda3/envs/jittor/bin/python ['test_sccan.py', '--config=config/pascal/pascal_split3_resnet50.yaml', '--viz']
[i 0718 09:46:18.910554 48 compiler.py:956] Jittor(1.3.9.14) src: /data1/fanlyu/anaconda3/envs/jittor/lib/python3.7/site-packages/jittor
[i 0718 09:46:18.920537 48 compiler.py:957] g++ at /usr/bin/g++(11.4.0)
[i 0718 09:46:18.920588 48 compiler.py:958] cache_path: /data1/fanlyu/.cache/jittor/jt1.3.9/g++11.4.0/py3.7.16/Linux-5.15.0-1xe3/IntelRXeonRGolx62/efa5/default
[i 0718 09:46:19.002711 48 install_cuda.py:96] cuda_driver_version: [12, 4]
[i 0718 09:46:19.013757 48 __init__.py:412] Found /data1/fanlyu/.cache/jittor/jtcuda/cuda12.2_cudnn8_linux/bin/nvcc(12.2.140) at /data1/fanlyu/.cache/jittor/jtcuda/cuda12.2_cudnn8_linux/bin/nvcc.
[i 0718 09:46:19.022402 48 __init__.py:412] Found addr2line(2.38) at /usr/bin/addr2line.
[i 0718 09:46:19.167336 48 compiler.py:1013] cuda key:cu12.2.140_sm_89
[i 0718 09:46:19.515133 48 __init__.py:227] Total mem: 503.54GB, using 16 procs for compiling.
[i 0718 09:46:19.619843 48 jit_compiler.cc:28] Load cc_path: /usr/bin/g++
[i 0718 09:46:19.746632 48 init.cc:63] Found cuda archs: [89,]
[2025-07-18 09:46:20,976 INFO test_sccan.py line 101 3326584] => creating model ...
[i 0718 09:46:21.217349 48 cuda_flags.cc:49] CUDA enabled.
[2025-07-18 09:46:21,340 INFO test_sccan.py line 65 3326584] => loading checkpoint 'exp/pascal/SCCAN/split3_1shot/resnet50/snapshot/train_epoch_127_0.5978.pth'
[2025-07-18 09:46:21,517 INFO test_sccan.py line 75 3326584] => loaded checkpoint 'exp/pascal/SCCAN/split3_1shot/resnet50/snapshot/train_epoch_127_0.5978.pth' (epoch 127)
[2025-07-18 09:46:26,526 INFO test_sccan.py line 103 3326584] OneModel(
    criterion: CrossEntropyLoss(None, ignore_index=255)
    criterion_dice: WeightedDiceLoss(1.0, reduction=sum)
    backbone: Backbone(
        backbone: ResNet(
            conv1: Conv(3, 64, (3, 3), (2, 2), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
            bn1: FrozenBatchNorm2d(None)
            relu1: relu()
            conv2: Conv(64, 64, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
            bn2: FrozenBatchNorm2d(None)
            relu2: relu()
            conv3: Conv(64, 128, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
            bn3: FrozenBatchNorm2d(None)
            relu3: relu()
            maxpool: Pool((3, 3), (2, 2), (1, 1), dilation=None, return_indices=None, ceil_mode=False, count_include_pad=True, op=maximum, item=None)
            layer1: Sequential(
                0: Bottleneck(
                    conv1: Conv(128, 64, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(64, 64, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(64, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                    downsample: Sequential(
                        0: Conv(128, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                        1: FrozenBatchNorm2d(None)
                    )
                )
                1: Bottleneck(
                    conv1: Conv(256, 64, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(64, 64, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(64, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                2: Bottleneck(
                    conv1: Conv(256, 64, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(64, 64, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(64, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
            )
            layer2: Sequential(
                0: Bottleneck(
                    conv1: Conv(256, 128, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(128, 128, (3, 3), (2, 2), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(128, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                    downsample: Sequential(
                        0: Conv(256, 512, (1, 1), (2, 2), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                        1: FrozenBatchNorm2d(None)
                    )
                )
                1: Bottleneck(
                    conv1: Conv(512, 128, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(128, 128, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(128, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                2: Bottleneck(
                    conv1: Conv(512, 128, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(128, 128, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(128, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                3: Bottleneck(
                    conv1: Conv(512, 128, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(128, 128, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(128, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
            )
            layer3: Sequential(
                0: Bottleneck(
                    conv1: Conv(512, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(256, 256, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(256, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                    downsample: Sequential(
                        0: Conv(512, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                        1: FrozenBatchNorm2d(None)
                    )
                )
                1: Bottleneck(
                    conv1: Conv(1024, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(256, 256, (3, 3), (1, 1), (2, 2), (2, 2), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(256, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                2: Bottleneck(
                    conv1: Conv(1024, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(256, 256, (3, 3), (1, 1), (2, 2), (2, 2), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(256, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                3: Bottleneck(
                    conv1: Conv(1024, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(256, 256, (3, 3), (1, 1), (2, 2), (2, 2), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(256, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                4: Bottleneck(
                    conv1: Conv(1024, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(256, 256, (3, 3), (1, 1), (2, 2), (2, 2), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(256, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                5: Bottleneck(
                    conv1: Conv(1024, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(256, 256, (3, 3), (1, 1), (2, 2), (2, 2), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(256, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
            )
            layer4: Sequential(
                0: Bottleneck(
                    conv1: Conv(1024, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(512, 512, (3, 3), (1, 1), (2, 2), (2, 2), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(512, 2048, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                    downsample: Sequential(
                        0: Conv(1024, 2048, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                        1: FrozenBatchNorm2d(None)
                    )
                )
                1: Bottleneck(
                    conv1: Conv(2048, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(512, 512, (3, 3), (1, 1), (4, 4), (4, 4), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(512, 2048, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                2: Bottleneck(
                    conv1: Conv(2048, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(512, 512, (3, 3), (1, 1), (4, 4), (4, 4), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(512, 2048, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
            )
            avgpool: AdaptiveAvgPool2d((1, 1))
            fc: Linear(2048, 1000, float32[1000,], None)
        )
    )
    down_query: Sequential(
        0: Conv(1536, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
        1: relu()
        2: Dropout2d(0.5, is_train=False)
    )
    down_supp: Sequential(
        0: Conv(1536, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
        1: relu()
        2: Dropout2d(0.5, is_train=False)
    )
    init_merge_query: Sequential(
        0: Conv(513, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
    )
    init_merge_supp: Sequential(
        0: Conv(513, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
    )
    transformer: SwinTransformer(
        pos_drop: Dropout(0.0, is_train=False)
        layers: Sequential(
            0: BasicLayer(
                blocks: Sequential(
                    0: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    1: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    2: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    3: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    4: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    5: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    6: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    7: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                )
            )
        )
        norm0: LayerNorm((256,), 1e-05, elementwise_affine=True)
    )
    ASPP_meta: ASPP(
        layer6_0: Sequential(
            0: Conv(256, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, float32[256,], None, None, Kw=None, fan=None, i=None, bound=None)
            1: relu()
        )
        layer6_1: Sequential(
            0: Conv(256, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, float32[256,], None, None, Kw=None, fan=None, i=None, bound=None)
            1: relu()
        )
        layer6_2: Sequential(
            0: Conv(256, 256, (3, 3), (1, 1), (6, 6), (6, 6), 1, float32[256,], None, None, Kw=None, fan=None, i=None, bound=None)
            1: relu()
        )
        layer6_3: Sequential(
            0: Conv(256, 256, (3, 3), (1, 1), (12, 12), (12, 12), 1, float32[256,], None, None, Kw=None, fan=None, i=None, bound=None)
            1: relu()
        )
        layer6_4: Sequential(
            0: Conv(256, 256, (3, 3), (1, 1), (18, 18), (18, 18), 1, float32[256,], None, None, Kw=None, fan=None, i=None, bound=None)
            1: relu()
        )
    )
    res1_meta: Sequential(
        0: Conv(1280, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
        1: relu()
    )
    res2_meta: Sequential(
        0: Conv(256, 256, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
        1: relu()
        2: Conv(256, 256, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
        3: relu()
    )
    cls_meta: Sequential(
        0: Conv(256, 256, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
        1: relu()
        2: Dropout2d(0.1, is_train=False)
        3: Conv(256, 2, (1, 1), (1, 1), (0, 0), (1, 1), 1, float32[2,], None, None, Kw=None, fan=None, i=None, bound=None)
    )
    relu: relu()
)
[2025-07-18 09:46:26,531 INFO test_sccan.py line 165 3326584] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
SubEpoch_val: True
ann_type: mask
arch: SCCAN
aux_weight1: 1.0
aux_weight2: 1.0
base_lr: 0.005
batch_size: 8
batch_size_val: 1
classes: 2
data_root: ../data/VOCdevkit2012/VOC2012
data_set: pascal
epochs: 200
evaluate: True
fix_bn: True
fix_random_seed_val: True
ignore_label: 255
index_split: -1
kshot_trans_dim: 2
layers: 50
low_fea: layer2
manual_seed: 321
merge: final
merge_tau: 0.9
momentum: 0.9
opts: None
ori_resize: True
padding_label: 255
power: 0.9
print_freq: 10
resized_val: True
resume: None
rotate_max: 10
rotate_min: -10
save_freq: 10
scale_max: 1.1
scale_min: 0.9
seed_deterministic: False
shot: 1
split: 3
start_epoch: 0
stop_interval: 75
train_h: 473
train_list: ./lists/pascal/voc_sbd_merge_noduplicate.txt
train_w: 473
use_split_coco: False
val_list: ./lists/pascal/val.txt
val_size: 473
vgg: False
viz: True
warmup: False
weight: train_epoch_127_0.5978.pth
weight_decay: 0.0001
workers: 8
zoom_factor: 8
Number of Parameters: 37107626
Number of Learnable Parameters: 11373314
sub_list:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
sub_val_list:  [16, 17, 18, 19, 20]
Val: [1/1] 	 Seed: 321
[2025-07-18 09:48:27,663 INFO test_sccan.py line 250 3326584] Test: [10/1000] Data 0.001 (11.961) Batch 0.057 (12.113) Remain 03:19:51 Loss 0.0047 (0.2840) Accuracy 0.9981.
[2025-07-18 09:48:28,184 INFO test_sccan.py line 250 3326584] Test: [20/1000] Data 0.001 (5.981) Batch 0.059 (6.083) Remain 01:39:20 Loss 0.0682 (0.2133) Accuracy 0.9860.
[2025-07-18 09:48:28,686 INFO test_sccan.py line 250 3326584] Test: [30/1000] Data 0.001 (3.988) Batch 0.051 (4.072) Remain 01:05:49 Loss 0.4477 (0.4014) Accuracy 0.9205.
[2025-07-18 09:48:29,189 INFO test_sccan.py line 250 3326584] Test: [40/1000] Data 0.001 (2.991) Batch 0.048 (3.066) Remain 00:49:03 Loss 0.3671 (0.4255) Accuracy 0.9343.
[2025-07-18 09:48:29,685 INFO test_sccan.py line 250 3326584] Test: [50/1000] Data 0.001 (2.393) Batch 0.051 (2.463) Remain 00:38:59 Loss 0.2888 (0.4108) Accuracy 0.8717.
[2025-07-18 09:48:30,171 INFO test_sccan.py line 250 3326584] Test: [60/1000] Data 0.001 (1.994) Batch 0.048 (2.061) Remain 00:32:16 Loss 0.1266 (0.4169) Accuracy 0.9583.
[2025-07-18 09:48:30,663 INFO test_sccan.py line 250 3326584] Test: [70/1000] Data 0.001 (1.709) Batch 0.050 (1.773) Remain 00:27:29 Loss 0.0616 (0.4501) Accuracy 0.9925.
[2025-07-18 09:48:31,149 INFO test_sccan.py line 250 3326584] Test: [80/1000] Data 0.001 (1.496) Batch 0.047 (1.558) Remain 00:23:53 Loss 2.6240 (0.4805) Accuracy 0.7135.
[2025-07-18 09:48:31,639 INFO test_sccan.py line 250 3326584] Test: [90/1000] Data 0.001 (1.330) Batch 0.050 (1.390) Remain 00:21:04 Loss 1.1780 (0.4956) Accuracy 0.6013.
[2025-07-18 09:48:32,128 INFO test_sccan.py line 250 3326584] Test: [100/1000] Data 0.001 (1.197) Batch 0.049 (1.256) Remain 00:18:50 Loss 0.2522 (0.5017) Accuracy 0.9237.
[2025-07-18 09:48:32,622 INFO test_sccan.py line 250 3326584] Test: [110/1000] Data 0.001 (1.088) Batch 0.048 (1.146) Remain 00:17:00 Loss 0.2842 (0.5091) Accuracy 0.8867.
[2025-07-18 09:48:33,110 INFO test_sccan.py line 250 3326584] Test: [120/1000] Data 0.001 (0.998) Batch 0.049 (1.055) Remain 00:15:28 Loss 1.4742 (0.5367) Accuracy 0.7582.
[2025-07-18 09:48:33,603 INFO test_sccan.py line 250 3326584] Test: [130/1000] Data 0.001 (0.921) Batch 0.050 (0.977) Remain 00:14:10 Loss 1.7779 (0.5486) Accuracy 0.7220.
[2025-07-18 09:48:34,096 INFO test_sccan.py line 250 3326584] Test: [140/1000] Data 0.001 (0.855) Batch 0.049 (0.911) Remain 00:13:03 Loss 0.2828 (0.5478) Accuracy 0.8909.
[2025-07-18 09:48:34,591 INFO test_sccan.py line 250 3326584] Test: [150/1000] Data 0.001 (0.798) Batch 0.050 (0.854) Remain 00:12:05 Loss 0.0456 (0.5331) Accuracy 0.9903.
[2025-07-18 09:48:35,084 INFO test_sccan.py line 250 3326584] Test: [160/1000] Data 0.001 (0.748) Batch 0.048 (0.803) Remain 00:11:14 Loss 0.2366 (0.5096) Accuracy 0.8936.
[2025-07-18 09:48:35,579 INFO test_sccan.py line 250 3326584] Test: [170/1000] Data 0.001 (0.704) Batch 0.050 (0.759) Remain 00:10:30 Loss 0.2091 (0.4905) Accuracy 0.9176.
[2025-07-18 09:48:36,072 INFO test_sccan.py line 250 3326584] Test: [180/1000] Data 0.001 (0.665) Batch 0.049 (0.720) Remain 00:09:50 Loss 3.4864 (0.4907) Accuracy 0.5492.
[2025-07-18 09:48:36,568 INFO test_sccan.py line 250 3326584] Test: [190/1000] Data 0.001 (0.630) Batch 0.051 (0.684) Remain 00:09:14 Loss 0.0285 (0.4746) Accuracy 0.9858.
[2025-07-18 09:48:37,061 INFO test_sccan.py line 250 3326584] Test: [200/1000] Data 0.001 (0.599) Batch 0.051 (0.653) Remain 00:08:42 Loss 0.4602 (0.4938) Accuracy 0.8693.
[2025-07-18 09:48:37,585 INFO test_sccan.py line 250 3326584] Test: [210/1000] Data 0.001 (0.570) Batch 0.052 (0.624) Remain 00:08:12 Loss 0.2314 (0.4936) Accuracy 0.9333.
[2025-07-18 09:48:38,113 INFO test_sccan.py line 250 3326584] Test: [220/1000] Data 0.001 (0.545) Batch 0.054 (0.598) Remain 00:07:46 Loss 0.0194 (0.5194) Accuracy 0.9920.
[2025-07-18 09:48:38,629 INFO test_sccan.py line 250 3326584] Test: [230/1000] Data 0.001 (0.521) Batch 0.050 (0.574) Remain 00:07:22 Loss 0.0490 (0.5076) Accuracy 0.9761.
[2025-07-18 09:48:39,121 INFO test_sccan.py line 250 3326584] Test: [240/1000] Data 0.001 (0.499) Batch 0.049 (0.552) Remain 00:06:59 Loss 0.0113 (0.5064) Accuracy 0.9957.
[2025-07-18 09:48:39,614 INFO test_sccan.py line 250 3326584] Test: [250/1000] Data 0.001 (0.479) Batch 0.050 (0.532) Remain 00:06:39 Loss 0.0182 (0.4926) Accuracy 0.9923.
[2025-07-18 09:48:40,108 INFO test_sccan.py line 250 3326584] Test: [260/1000] Data 0.001 (0.461) Batch 0.049 (0.514) Remain 00:06:20 Loss 1.3848 (0.4981) Accuracy 0.7071.
[2025-07-18 09:48:40,607 INFO test_sccan.py line 250 3326584] Test: [270/1000] Data 0.001 (0.444) Batch 0.050 (0.497) Remain 00:06:02 Loss 0.0428 (0.5015) Accuracy 0.9899.
[2025-07-18 09:48:41,105 INFO test_sccan.py line 250 3326584] Test: [280/1000] Data 0.001 (0.428) Batch 0.049 (0.481) Remain 00:05:46 Loss 0.2804 (0.5263) Accuracy 0.9157.
[2025-07-18 09:48:41,606 INFO test_sccan.py line 250 3326584] Test: [290/1000] Data 0.001 (0.413) Batch 0.050 (0.466) Remain 00:05:30 Loss 0.0280 (0.5288) Accuracy 0.9917.
[2025-07-18 09:48:42,109 INFO test_sccan.py line 250 3326584] Test: [300/1000] Data 0.001 (0.400) Batch 0.051 (0.452) Remain 00:05:16 Loss 0.4222 (0.5240) Accuracy 0.9007.
[2025-07-18 09:48:42,609 INFO test_sccan.py line 250 3326584] Test: [310/1000] Data 0.001 (0.387) Batch 0.050 (0.439) Remain 00:05:02 Loss 0.2567 (0.5137) Accuracy 0.9237.
[2025-07-18 09:48:43,105 INFO test_sccan.py line 250 3326584] Test: [320/1000] Data 0.001 (0.375) Batch 0.049 (0.427) Remain 00:04:50 Loss 0.5016 (0.5094) Accuracy 0.8479.
[2025-07-18 09:48:43,606 INFO test_sccan.py line 250 3326584] Test: [330/1000] Data 0.001 (0.363) Batch 0.052 (0.415) Remain 00:04:38 Loss 0.5892 (0.5048) Accuracy 0.8053.
[2025-07-18 09:48:44,426 INFO test_sccan.py line 250 3326584] Test: [346/1000] Data 0.001 (0.347) Batch 0.051 (0.399) Remain 00:04:20 Loss 0.0048 (0.4951) Accuracy 0.9980.
[2025-07-18 09:48:44,935 INFO test_sccan.py line 250 3326584] Test: [356/1000] Data 0.001 (0.337) Batch 0.050 (0.389) Remain 00:04:10 Loss 0.0683 (0.4852) Accuracy 0.9862.
[2025-07-18 09:48:45,455 INFO test_sccan.py line 250 3326584] Test: [366/1000] Data 0.001 (0.328) Batch 0.060 (0.380) Remain 00:04:00 Loss 0.5014 (0.4878) Accuracy 0.9225.
[2025-07-18 09:48:45,970 INFO test_sccan.py line 250 3326584] Test: [376/1000] Data 0.001 (0.319) Batch 0.054 (0.371) Remain 00:03:51 Loss 0.2131 (0.4816) Accuracy 0.9357.
[2025-07-18 09:48:46,481 INFO test_sccan.py line 250 3326584] Test: [386/1000] Data 0.001 (0.311) Batch 0.050 (0.363) Remain 00:03:42 Loss 0.1317 (0.4800) Accuracy 0.9289.
[2025-07-18 09:48:46,976 INFO test_sccan.py line 250 3326584] Test: [396/1000] Data 0.001 (0.303) Batch 0.049 (0.355) Remain 00:03:34 Loss 0.1822 (0.4802) Accuracy 0.9376.
[2025-07-18 09:48:47,477 INFO test_sccan.py line 250 3326584] Test: [406/1000] Data 0.001 (0.295) Batch 0.051 (0.347) Remain 00:03:26 Loss 0.0483 (0.4820) Accuracy 0.9924.
[2025-07-18 09:48:47,974 INFO test_sccan.py line 250 3326584] Test: [416/1000] Data 0.001 (0.288) Batch 0.050 (0.340) Remain 00:03:18 Loss 2.6240 (0.4875) Accuracy 0.7135.
[2025-07-18 09:48:48,473 INFO test_sccan.py line 250 3326584] Test: [426/1000] Data 0.001 (0.282) Batch 0.050 (0.333) Remain 00:03:11 Loss 0.1745 (0.4873) Accuracy 0.9143.
[2025-07-18 09:48:48,968 INFO test_sccan.py line 250 3326584] Test: [436/1000] Data 0.001 (0.275) Batch 0.048 (0.327) Remain 00:03:04 Loss 0.0786 (0.4902) Accuracy 0.9607.
[2025-07-18 09:48:49,469 INFO test_sccan.py line 250 3326584] Test: [446/1000] Data 0.001 (0.269) Batch 0.051 (0.320) Remain 00:02:57 Loss 0.1425 (0.4930) Accuracy 0.9465.
[2025-07-18 09:48:49,971 INFO test_sccan.py line 250 3326584] Test: [456/1000] Data 0.001 (0.263) Batch 0.050 (0.315) Remain 00:02:51 Loss 0.3534 (0.5005) Accuracy 0.9451.
[2025-07-18 09:48:50,471 INFO test_sccan.py line 250 3326584] Test: [466/1000] Data 0.001 (0.258) Batch 0.050 (0.309) Remain 00:02:44 Loss 1.4584 (0.5040) Accuracy 0.7604.
[2025-07-18 09:48:50,975 INFO test_sccan.py line 250 3326584] Test: [476/1000] Data 0.001 (0.252) Batch 0.052 (0.303) Remain 00:02:39 Loss 0.2829 (0.5043) Accuracy 0.8893.
[2025-07-18 09:48:51,477 INFO test_sccan.py line 250 3326584] Test: [486/1000] Data 0.001 (0.247) Batch 0.051 (0.298) Remain 00:02:33 Loss 0.0452 (0.5005) Accuracy 0.9906.
[2025-07-18 09:48:51,974 INFO test_sccan.py line 250 3326584] Test: [496/1000] Data 0.001 (0.242) Batch 0.050 (0.293) Remain 00:02:27 Loss 0.0983 (0.4933) Accuracy 0.9745.
[2025-07-18 09:48:52,478 INFO test_sccan.py line 250 3326584] Test: [506/1000] Data 0.001 (0.237) Batch 0.050 (0.288) Remain 00:02:22 Loss 0.4901 (0.4892) Accuracy 0.7421.
[2025-07-18 09:48:52,979 INFO test_sccan.py line 250 3326584] Test: [516/1000] Data 0.001 (0.233) Batch 0.049 (0.284) Remain 00:02:17 Loss 3.3428 (0.4889) Accuracy 0.5105.
[2025-07-18 09:48:53,481 INFO test_sccan.py line 250 3326584] Test: [526/1000] Data 0.001 (0.228) Batch 0.050 (0.279) Remain 00:02:12 Loss 0.0387 (0.4855) Accuracy 0.9827.
[2025-07-18 09:48:53,978 INFO test_sccan.py line 250 3326584] Test: [536/1000] Data 0.001 (0.224) Batch 0.050 (0.275) Remain 00:02:07 Loss 0.3954 (0.4936) Accuracy 0.9027.
[2025-07-18 09:48:54,478 INFO test_sccan.py line 250 3326584] Test: [546/1000] Data 0.001 (0.220) Batch 0.050 (0.271) Remain 00:02:03 Loss 0.2065 (0.4923) Accuracy 0.9429.
[2025-07-18 09:48:54,979 INFO test_sccan.py line 250 3326584] Test: [556/1000] Data 0.001 (0.216) Batch 0.051 (0.267) Remain 00:01:58 Loss 0.0192 (0.5083) Accuracy 0.9918.
[2025-07-18 09:48:55,479 INFO test_sccan.py line 250 3326584] Test: [566/1000] Data 0.001 (0.212) Batch 0.050 (0.263) Remain 00:01:54 Loss 0.0466 (0.5039) Accuracy 0.9785.
[2025-07-18 09:48:55,978 INFO test_sccan.py line 250 3326584] Test: [576/1000] Data 0.001 (0.208) Batch 0.050 (0.259) Remain 00:01:50 Loss 0.0116 (0.5034) Accuracy 0.9956.
[2025-07-18 09:48:56,478 INFO test_sccan.py line 250 3326584] Test: [586/1000] Data 0.001 (0.205) Batch 0.050 (0.256) Remain 00:01:45 Loss 0.0187 (0.4973) Accuracy 0.9922.
[2025-07-18 09:48:56,980 INFO test_sccan.py line 250 3326584] Test: [596/1000] Data 0.001 (0.202) Batch 0.051 (0.252) Remain 00:01:41 Loss 3.0935 (0.5062) Accuracy 0.5079.
[2025-07-18 09:48:57,494 INFO test_sccan.py line 250 3326584] Test: [606/1000] Data 0.001 (0.198) Batch 0.062 (0.249) Remain 00:01:38 Loss 0.0427 (0.5127) Accuracy 0.9898.
[2025-07-18 09:48:58,002 INFO test_sccan.py line 250 3326584] Test: [616/1000] Data 0.001 (0.195) Batch 0.049 (0.246) Remain 00:01:34 Loss 0.2899 (0.5162) Accuracy 0.8842.
[2025-07-18 09:48:58,501 INFO test_sccan.py line 250 3326584] Test: [626/1000] Data 0.001 (0.192) Batch 0.049 (0.243) Remain 00:01:30 Loss 0.0278 (0.5165) Accuracy 0.9914.
[2025-07-18 09:48:59,003 INFO test_sccan.py line 250 3326584] Test: [636/1000] Data 0.001 (0.189) Batch 0.052 (0.240) Remain 00:01:27 Loss 0.4037 (0.5148) Accuracy 0.9014.
[2025-07-18 09:48:59,506 INFO test_sccan.py line 250 3326584] Test: [646/1000] Data 0.001 (0.186) Batch 0.050 (0.237) Remain 00:01:23 Loss 0.2013 (0.5145) Accuracy 0.9034.
[2025-07-18 09:49:00,001 INFO test_sccan.py line 250 3326584] Test: [656/1000] Data 0.001 (0.183) Batch 0.049 (0.234) Remain 00:01:20 Loss 0.3334 (0.5121) Accuracy 0.8558.
[2025-07-18 09:49:00,500 INFO test_sccan.py line 250 3326584] Test: [666/1000] Data 0.001 (0.180) Batch 0.048 (0.231) Remain 00:01:17 Loss 0.5300 (0.5118) Accuracy 0.8147.
[2025-07-18 09:49:01,298 INFO test_sccan.py line 250 3326584] Test: [682/1000] Data 0.001 (0.176) Batch 0.049 (0.227) Remain 00:01:12 Loss 0.0047 (0.5095) Accuracy 0.9981.
[2025-07-18 09:49:01,803 INFO test_sccan.py line 250 3326584] Test: [692/1000] Data 0.001 (0.174) Batch 0.052 (0.224) Remain 00:01:09 Loss 0.0676 (0.5038) Accuracy 0.9860.
[2025-07-18 09:49:02,306 INFO test_sccan.py line 250 3326584] Test: [702/1000] Data 0.001 (0.171) Batch 0.050 (0.222) Remain 00:01:06 Loss 0.5168 (0.5025) Accuracy 0.9184.
[2025-07-18 09:49:02,801 INFO test_sccan.py line 250 3326584] Test: [712/1000] Data 0.001 (0.169) Batch 0.047 (0.219) Remain 00:01:03 Loss 0.2074 (0.4979) Accuracy 0.9379.
[2025-07-18 09:49:03,295 INFO test_sccan.py line 250 3326584] Test: [722/1000] Data 0.001 (0.167) Batch 0.051 (0.217) Remain 00:01:00 Loss 0.1590 (0.4956) Accuracy 0.9177.
[2025-07-18 09:49:03,792 INFO test_sccan.py line 250 3326584] Test: [732/1000] Data 0.001 (0.164) Batch 0.050 (0.215) Remain 00:00:57 Loss 0.1372 (0.4941) Accuracy 0.9577.
[2025-07-18 09:49:04,294 INFO test_sccan.py line 250 3326584] Test: [742/1000] Data 0.001 (0.162) Batch 0.051 (0.213) Remain 00:00:54 Loss 0.0692 (0.4983) Accuracy 0.9925.
[2025-07-18 09:49:04,787 INFO test_sccan.py line 250 3326584] Test: [752/1000] Data 0.001 (0.160) Batch 0.047 (0.210) Remain 00:00:52 Loss 0.1335 (0.4986) Accuracy 0.9809.
[2025-07-18 09:49:05,289 INFO test_sccan.py line 250 3326584] Test: [762/1000] Data 0.001 (0.158) Batch 0.049 (0.208) Remain 00:00:49 Loss 1.0867 (0.4988) Accuracy 0.5278.
[2025-07-18 09:49:05,787 INFO test_sccan.py line 250 3326584] Test: [772/1000] Data 0.001 (0.156) Batch 0.050 (0.206) Remain 00:00:47 Loss 0.0539 (0.5000) Accuracy 0.9725.
[2025-07-18 09:49:06,289 INFO test_sccan.py line 250 3326584] Test: [782/1000] Data 0.001 (0.154) Batch 0.050 (0.204) Remain 00:00:44 Loss 0.2099 (0.5030) Accuracy 0.9323.
[2025-07-18 09:49:06,791 INFO test_sccan.py line 250 3326584] Test: [792/1000] Data 0.001 (0.152) Batch 0.050 (0.202) Remain 00:00:42 Loss 1.0000 (0.5083) Accuracy 0.7722.
[2025-07-18 09:49:07,294 INFO test_sccan.py line 250 3326584] Test: [802/1000] Data 0.001 (0.150) Batch 0.050 (0.200) Remain 00:00:39 Loss 1.2625 (0.5103) Accuracy 0.7492.
[2025-07-18 09:49:07,794 INFO test_sccan.py line 250 3326584] Test: [812/1000] Data 0.001 (0.148) Batch 0.051 (0.199) Remain 00:00:37 Loss 0.2510 (0.5120) Accuracy 0.8962.
[2025-07-18 09:49:08,297 INFO test_sccan.py line 250 3326584] Test: [822/1000] Data 0.001 (0.146) Batch 0.050 (0.197) Remain 00:00:35 Loss 0.0458 (0.5091) Accuracy 0.9907.
[2025-07-18 09:49:08,799 INFO test_sccan.py line 250 3326584] Test: [832/1000] Data 0.001 (0.145) Batch 0.050 (0.195) Remain 00:00:32 Loss 0.0734 (0.5058) Accuracy 0.9780.
[2025-07-18 09:49:09,298 INFO test_sccan.py line 250 3326584] Test: [842/1000] Data 0.001 (0.143) Batch 0.050 (0.193) Remain 00:00:30 Loss 0.2133 (0.5036) Accuracy 0.9345.
[2025-07-18 09:49:09,802 INFO test_sccan.py line 250 3326584] Test: [852/1000] Data 0.001 (0.141) Batch 0.051 (0.192) Remain 00:00:28 Loss 3.8936 (0.5034) Accuracy 0.5127.
[2025-07-18 09:49:10,306 INFO test_sccan.py line 250 3326584] Test: [862/1000] Data 0.001 (0.140) Batch 0.050 (0.190) Remain 00:00:26 Loss 0.0373 (0.5001) Accuracy 0.9832.
[2025-07-18 09:49:10,803 INFO test_sccan.py line 250 3326584] Test: [872/1000] Data 0.001 (0.138) Batch 0.050 (0.188) Remain 00:00:24 Loss 0.4749 (0.5034) Accuracy 0.8370.
[2025-07-18 09:49:11,301 INFO test_sccan.py line 250 3326584] Test: [882/1000] Data 0.001 (0.136) Batch 0.049 (0.187) Remain 00:00:22 Loss 0.2314 (0.5005) Accuracy 0.9333.
[2025-07-18 09:49:11,805 INFO test_sccan.py line 250 3326584] Test: [892/1000] Data 0.001 (0.135) Batch 0.051 (0.185) Remain 00:00:20 Loss 0.0249 (0.5090) Accuracy 0.9891.
[2025-07-18 09:49:12,309 INFO test_sccan.py line 250 3326584] Test: [902/1000] Data 0.001 (0.133) Batch 0.050 (0.184) Remain 00:00:18 Loss 0.0494 (0.5055) Accuracy 0.9776.
[2025-07-18 09:49:12,810 INFO test_sccan.py line 250 3326584] Test: [912/1000] Data 0.001 (0.132) Batch 0.049 (0.182) Remain 00:00:16 Loss 0.0118 (0.5058) Accuracy 0.9956.
[2025-07-18 09:49:13,310 INFO test_sccan.py line 250 3326584] Test: [922/1000] Data 0.001 (0.131) Batch 0.050 (0.181) Remain 00:00:14 Loss 0.0180 (0.5017) Accuracy 0.9923.
[2025-07-18 09:49:13,813 INFO test_sccan.py line 250 3326584] Test: [932/1000] Data 0.001 (0.129) Batch 0.051 (0.179) Remain 00:00:12 Loss 1.2981 (0.5047) Accuracy 0.7030.
[2025-07-18 09:49:14,315 INFO test_sccan.py line 250 3326584] Test: [942/1000] Data 0.001 (0.128) Batch 0.050 (0.178) Remain 00:00:10 Loss 0.0426 (0.5083) Accuracy 0.9899.
[2025-07-18 09:49:14,815 INFO test_sccan.py line 250 3326584] Test: [952/1000] Data 0.001 (0.126) Batch 0.051 (0.177) Remain 00:00:08 Loss 0.2396 (0.5140) Accuracy 0.9232.
[2025-07-18 09:49:15,313 INFO test_sccan.py line 250 3326584] Test: [962/1000] Data 0.001 (0.125) Batch 0.049 (0.175) Remain 00:00:06 Loss 0.0276 (0.5141) Accuracy 0.9917.
[2025-07-18 09:49:15,816 INFO test_sccan.py line 250 3326584] Test: [972/1000] Data 0.001 (0.124) Batch 0.051 (0.174) Remain 00:00:04 Loss 0.1997 (0.5127) Accuracy 0.9146.
[2025-07-18 09:49:16,318 INFO test_sccan.py line 250 3326584] Test: [982/1000] Data 0.001 (0.123) Batch 0.050 (0.173) Remain 00:00:03 Loss 0.1869 (0.5121) Accuracy 0.9477.
[2025-07-18 09:49:16,819 INFO test_sccan.py line 250 3326584] Test: [992/1000] Data 0.001 (0.121) Batch 0.049 (0.172) Remain 00:00:01 Loss 0.5247 (0.5092) Accuracy 0.8531.
[2025-07-18 09:49:17,224 INFO test_sccan.py line 264 3326584] meanIoU---Val result: mIoU 0.5985.
[2025-07-18 09:49:17,224 INFO test_sccan.py line 265 3326584] <<<<<<< Novel Results <<<<<<<
[2025-07-18 09:49:17,224 INFO test_sccan.py line 267 3326584] Class_1 Result: iou 0.2365.
[2025-07-18 09:49:17,224 INFO test_sccan.py line 267 3326584] Class_2 Result: iou 0.9311.
[2025-07-18 09:49:17,224 INFO test_sccan.py line 267 3326584] Class_3 Result: iou 0.6465.
[2025-07-18 09:49:17,224 INFO test_sccan.py line 267 3326584] Class_4 Result: iou 0.8196.
[2025-07-18 09:49:17,224 INFO test_sccan.py line 267 3326584] Class_5 Result: iou 0.3586.
[2025-07-18 09:49:17,224 INFO test_sccan.py line 269 3326584] FBIoU---Val result: FBIoU 0.7219.
[2025-07-18 09:49:17,224 INFO test_sccan.py line 271 3326584] Class_0 Result: iou_f 0.8558.
[2025-07-18 09:49:17,224 INFO test_sccan.py line 271 3326584] Class_1 Result: iou_f 0.5881.
[2025-07-18 09:49:17,224 INFO test_sccan.py line 272 3326584] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
total time: 170.6893, avg inference time: 0.0114, count: 1000

Total running time: 00h 02m 50s
Seed0: 123
Seed:  [321]
mIoU:  [0.5985]
FBIoU: [0.7219]
pIoU:  [0.5881]
-------------------------------------------
Best_Seed_m: 321 	 Best_Seed_F: 321 	 Best_Seed_p: 321
Best_mIoU: 0.5985 	 Best_FBIoU: 0.7219 	 Best_pIoU: 0.5881
Mean_mIoU: 0.5985 	 Mean_FBIoU: 0.7219 	 Mean_pIoU: 0.5881
