[38;5;2m[i 0718 09:45:53.884368 20 compiler.py:956] Jittor(1.3.9.14) src: /data1/fanlyu/anaconda3/envs/jittor/lib/python3.7/site-packages/jittor[m
[38;5;2m[i 0718 09:45:53.892049 20 compiler.py:957] g++ at /usr/bin/g++(11.4.0)[m
[38;5;2m[i 0718 09:45:53.892094 20 compiler.py:958] cache_path: /data1/fanlyu/.cache/jittor/jt1.3.9/g++11.4.0/py3.7.16/Linux-5.15.0-1xe3/IntelRXeonRGolx62/efa5/default[m
[38;5;2m[i 0718 09:45:53.946599 20 install_cuda.py:96] cuda_driver_version: [12, 4][m
[38;5;2m[i 0718 09:45:53.947129 20 install_cuda.py:84] restart /data1/fanlyu/anaconda3/envs/jittor/bin/python ['test_sccan.py', '--config=config/pascal/pascal_split0_resnet50.yaml', '--viz'][m
[38;5;2m[i 0718 09:45:54.802013 92 compiler.py:956] Jittor(1.3.9.14) src: /data1/fanlyu/anaconda3/envs/jittor/lib/python3.7/site-packages/jittor[m
[38;5;2m[i 0718 09:45:54.807807 92 compiler.py:957] g++ at /usr/bin/g++(11.4.0)[m
[38;5;2m[i 0718 09:45:54.807853 92 compiler.py:958] cache_path: /data1/fanlyu/.cache/jittor/jt1.3.9/g++11.4.0/py3.7.16/Linux-5.15.0-1xe3/IntelRXeonRGolx62/efa5/default[m
[38;5;2m[i 0718 09:45:54.861097 92 install_cuda.py:96] cuda_driver_version: [12, 4][m
[38;5;2m[i 0718 09:45:54.869319 92 __init__.py:412] Found /data1/fanlyu/.cache/jittor/jtcuda/cuda12.2_cudnn8_linux/bin/nvcc(12.2.140) at /data1/fanlyu/.cache/jittor/jtcuda/cuda12.2_cudnn8_linux/bin/nvcc.[m
[38;5;2m[i 0718 09:45:54.875092 92 __init__.py:412] Found addr2line(2.38) at /usr/bin/addr2line.[m
[38;5;2m[i 0718 09:45:55.005416 92 compiler.py:1013] cuda key:cu12.2.140_sm_89[m
[38;5;2m[i 0718 09:45:55.339471 92 __init__.py:227] Total mem: 503.54GB, using 16 procs for compiling.[m
[38;5;2m[i 0718 09:45:55.422287 92 jit_compiler.cc:28] Load cc_path: /usr/bin/g++[m
[38;5;2m[i 0718 09:45:55.505823 92 init.cc:63] Found cuda archs: [89,][m
[2025-07-18 09:45:56,639 INFO test_sccan.py line 101 3325662] => creating model ...
[38;5;2m[i 0718 09:45:56.880057 92 cuda_flags.cc:49] CUDA enabled.[m
[2025-07-18 09:45:56,980 INFO test_sccan.py line 65 3325662] => loading checkpoint 'exp/pascal/SCCAN/split0_1shot/resnet50/snapshot/train_epoch_33_0.6631.pth'
[2025-07-18 09:45:57,108 INFO test_sccan.py line 75 3325662] => loaded checkpoint 'exp/pascal/SCCAN/split0_1shot/resnet50/snapshot/train_epoch_33_0.6631.pth' (epoch 33)
[2025-07-18 09:46:02,117 INFO test_sccan.py line 103 3325662] OneModel(
    criterion: CrossEntropyLoss(None, ignore_index=255)
    criterion_dice: WeightedDiceLoss(1.0, reduction=sum)
    backbone: Backbone(
        backbone: ResNet(
            conv1: Conv(3, 64, (3, 3), (2, 2), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
            bn1: FrozenBatchNorm2d(None)
            relu1: relu()
            conv2: Conv(64, 64, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
            bn2: FrozenBatchNorm2d(None)
            relu2: relu()
            conv3: Conv(64, 128, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
            bn3: FrozenBatchNorm2d(None)
            relu3: relu()
            maxpool: Pool((3, 3), (2, 2), (1, 1), dilation=None, return_indices=None, ceil_mode=False, count_include_pad=True, op=maximum, item=None)
            layer1: Sequential(
                0: Bottleneck(
                    conv1: Conv(128, 64, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(64, 64, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(64, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                    downsample: Sequential(
                        0: Conv(128, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                        1: FrozenBatchNorm2d(None)
                    )
                )
                1: Bottleneck(
                    conv1: Conv(256, 64, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(64, 64, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(64, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                2: Bottleneck(
                    conv1: Conv(256, 64, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(64, 64, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(64, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
            )
            layer2: Sequential(
                0: Bottleneck(
                    conv1: Conv(256, 128, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(128, 128, (3, 3), (2, 2), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(128, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                    downsample: Sequential(
                        0: Conv(256, 512, (1, 1), (2, 2), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                        1: FrozenBatchNorm2d(None)
                    )
                )
                1: Bottleneck(
                    conv1: Conv(512, 128, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(128, 128, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(128, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                2: Bottleneck(
                    conv1: Conv(512, 128, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(128, 128, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(128, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                3: Bottleneck(
                    conv1: Conv(512, 128, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(128, 128, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(128, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
            )
            layer3: Sequential(
                0: Bottleneck(
                    conv1: Conv(512, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(256, 256, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(256, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                    downsample: Sequential(
                        0: Conv(512, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                        1: FrozenBatchNorm2d(None)
                    )
                )
                1: Bottleneck(
                    conv1: Conv(1024, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(256, 256, (3, 3), (1, 1), (2, 2), (2, 2), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(256, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                2: Bottleneck(
                    conv1: Conv(1024, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(256, 256, (3, 3), (1, 1), (2, 2), (2, 2), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(256, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                3: Bottleneck(
                    conv1: Conv(1024, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(256, 256, (3, 3), (1, 1), (2, 2), (2, 2), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(256, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                4: Bottleneck(
                    conv1: Conv(1024, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(256, 256, (3, 3), (1, 1), (2, 2), (2, 2), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(256, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                5: Bottleneck(
                    conv1: Conv(1024, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(256, 256, (3, 3), (1, 1), (2, 2), (2, 2), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(256, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
            )
            layer4: Sequential(
                0: Bottleneck(
                    conv1: Conv(1024, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(512, 512, (3, 3), (1, 1), (2, 2), (2, 2), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(512, 2048, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                    downsample: Sequential(
                        0: Conv(1024, 2048, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                        1: FrozenBatchNorm2d(None)
                    )
                )
                1: Bottleneck(
                    conv1: Conv(2048, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(512, 512, (3, 3), (1, 1), (4, 4), (4, 4), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(512, 2048, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                2: Bottleneck(
                    conv1: Conv(2048, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(512, 512, (3, 3), (1, 1), (4, 4), (4, 4), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(512, 2048, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
            )
            avgpool: AdaptiveAvgPool2d((1, 1))
            fc: Linear(2048, 1000, float32[1000,], None)
        )
    )
    down_query: Sequential(
        0: Conv(1536, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
        1: relu()
        2: Dropout2d(0.5, is_train=False)
    )
    down_supp: Sequential(
        0: Conv(1536, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
        1: relu()
        2: Dropout2d(0.5, is_train=False)
    )
    init_merge_query: Sequential(
        0: Conv(513, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
    )
    init_merge_supp: Sequential(
        0: Conv(513, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
    )
    transformer: SwinTransformer(
        pos_drop: Dropout(0.0, is_train=False)
        layers: Sequential(
            0: BasicLayer(
                blocks: Sequential(
                    0: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    1: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    2: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    3: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    4: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    5: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    6: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    7: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                )
            )
        )
        norm0: LayerNorm((256,), 1e-05, elementwise_affine=True)
    )
    ASPP_meta: ASPP(
        layer6_0: Sequential(
            0: Conv(256, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, float32[256,], None, None, Kw=None, fan=None, i=None, bound=None)
            1: relu()
        )
        layer6_1: Sequential(
            0: Conv(256, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, float32[256,], None, None, Kw=None, fan=None, i=None, bound=None)
            1: relu()
        )
        layer6_2: Sequential(
            0: Conv(256, 256, (3, 3), (1, 1), (6, 6), (6, 6), 1, float32[256,], None, None, Kw=None, fan=None, i=None, bound=None)
            1: relu()
        )
        layer6_3: Sequential(
            0: Conv(256, 256, (3, 3), (1, 1), (12, 12), (12, 12), 1, float32[256,], None, None, Kw=None, fan=None, i=None, bound=None)
            1: relu()
        )
        layer6_4: Sequential(
            0: Conv(256, 256, (3, 3), (1, 1), (18, 18), (18, 18), 1, float32[256,], None, None, Kw=None, fan=None, i=None, bound=None)
            1: relu()
        )
    )
    res1_meta: Sequential(
        0: Conv(1280, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
        1: relu()
    )
    res2_meta: Sequential(
        0: Conv(256, 256, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
        1: relu()
        2: Conv(256, 256, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
        3: relu()
    )
    cls_meta: Sequential(
        0: Conv(256, 256, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
        1: relu()
        2: Dropout2d(0.1, is_train=False)
        3: Conv(256, 2, (1, 1), (1, 1), (0, 0), (1, 1), 1, float32[2,], None, None, Kw=None, fan=None, i=None, bound=None)
    )
    relu: relu()
)
[2025-07-18 09:46:02,121 INFO test_sccan.py line 165 3325662] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
SubEpoch_val: True
ann_type: mask
arch: SCCAN
aux_weight1: 1.0
aux_weight2: 1.0
base_lr: 0.005
batch_size: 8
batch_size_val: 1
classes: 2
data_root: ../data/VOCdevkit2012/VOC2012
data_set: pascal
epochs: 200
evaluate: True
fix_bn: True
fix_random_seed_val: True
ignore_label: 255
index_split: -1
kshot_trans_dim: 2
layers: 50
low_fea: layer2
manual_seed: 321
merge: final
merge_tau: 0.9
momentum: 0.9
opts: None
ori_resize: True
padding_label: 255
power: 0.9
print_freq: 10
resized_val: True
resume: None
rotate_max: 10
rotate_min: -10
save_freq: 10
scale_max: 1.1
scale_min: 0.9
seed_deterministic: False
shot: 1
split: 0
start_epoch: 0
stop_interval: 75
train_h: 473
train_list: ./lists/pascal/voc_sbd_merge_noduplicate.txt
train_w: 473
use_split_coco: False
val_list: ./lists/pascal/val.txt
val_size: 473
vgg: False
viz: True
warmup: False
weight: train_epoch_33_0.6631.pth
weight_decay: 0.0001
workers: 8
zoom_factor: 8
Number of Parameters: 37107626
Number of Learnable Parameters: 11373314
sub_list:  [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
sub_val_list:  [1, 2, 3, 4, 5]
Val: [1/1] 	 Seed: 321
[2025-07-18 09:46:05,745 INFO test_sccan.py line 250 3325662] Test: [10/1000] Data 0.001 (0.217) Batch 0.048 (0.362) Remain 00:05:58 Loss 0.0811 (0.2504) Accuracy 0.9838.
[2025-07-18 09:46:06,239 INFO test_sccan.py line 250 3325662] Test: [20/1000] Data 0.001 (0.109) Batch 0.050 (0.206) Remain 00:03:21 Loss 0.0976 (0.2345) Accuracy 0.9740.
[2025-07-18 09:46:06,731 INFO test_sccan.py line 250 3325662] Test: [30/1000] Data 0.001 (0.073) Batch 0.047 (0.154) Remain 00:02:28 Loss 0.0667 (0.1815) Accuracy 0.9838.
[2025-07-18 09:46:07,223 INFO test_sccan.py line 250 3325662] Test: [40/1000] Data 0.001 (0.055) Batch 0.049 (0.127) Remain 00:02:02 Loss 0.2743 (0.2185) Accuracy 0.9480.
[2025-07-18 09:46:07,732 INFO test_sccan.py line 250 3325662] Test: [50/1000] Data 0.001 (0.044) Batch 0.047 (0.112) Remain 00:01:46 Loss 0.1177 (0.2082) Accuracy 0.9811.
[2025-07-18 09:46:08,229 INFO test_sccan.py line 250 3325662] Test: [60/1000] Data 0.001 (0.037) Batch 0.050 (0.102) Remain 00:01:35 Loss 1.9110 (0.2839) Accuracy 0.6721.
[2025-07-18 09:46:08,728 INFO test_sccan.py line 250 3325662] Test: [70/1000] Data 0.001 (0.032) Batch 0.046 (0.094) Remain 00:01:27 Loss 0.0504 (0.2661) Accuracy 0.9918.
[2025-07-18 09:46:09,218 INFO test_sccan.py line 250 3325662] Test: [80/1000] Data 0.001 (0.028) Batch 0.048 (0.089) Remain 00:01:21 Loss 0.0746 (0.2415) Accuracy 0.9835.
[2025-07-18 09:46:09,702 INFO test_sccan.py line 250 3325662] Test: [90/1000] Data 0.001 (0.025) Batch 0.046 (0.084) Remain 00:01:16 Loss 0.0616 (0.2279) Accuracy 0.9763.
[2025-07-18 09:46:10,206 INFO test_sccan.py line 250 3325662] Test: [100/1000] Data 0.001 (0.022) Batch 0.048 (0.081) Remain 00:01:12 Loss 0.1548 (0.2310) Accuracy 0.9344.
[2025-07-18 09:46:10,703 INFO test_sccan.py line 250 3325662] Test: [110/1000] Data 0.001 (0.021) Batch 0.048 (0.078) Remain 00:01:09 Loss 0.4736 (0.2314) Accuracy 0.9428.
[2025-07-18 09:46:11,212 INFO test_sccan.py line 250 3325662] Test: [120/1000] Data 0.001 (0.019) Batch 0.046 (0.076) Remain 00:01:06 Loss 0.0575 (0.2321) Accuracy 0.9828.
[2025-07-18 09:46:11,724 INFO test_sccan.py line 250 3325662] Test: [130/1000] Data 0.001 (0.018) Batch 0.046 (0.074) Remain 00:01:04 Loss 0.0123 (0.2295) Accuracy 0.9974.
[2025-07-18 09:46:12,237 INFO test_sccan.py line 250 3325662] Test: [140/1000] Data 0.001 (0.016) Batch 0.049 (0.072) Remain 00:01:02 Loss 0.1115 (0.2368) Accuracy 0.9596.
[2025-07-18 09:46:12,746 INFO test_sccan.py line 250 3325662] Test: [150/1000] Data 0.001 (0.016) Batch 0.050 (0.071) Remain 00:01:00 Loss 0.4756 (0.2368) Accuracy 0.8659.
[2025-07-18 09:46:13,233 INFO test_sccan.py line 250 3325662] Test: [160/1000] Data 0.001 (0.015) Batch 0.048 (0.069) Remain 00:00:58 Loss 0.1991 (0.2302) Accuracy 0.9072.
[2025-07-18 09:46:13,710 INFO test_sccan.py line 250 3325662] Test: [170/1000] Data 0.001 (0.014) Batch 0.046 (0.068) Remain 00:00:56 Loss 0.0271 (0.2315) Accuracy 0.9893.
[2025-07-18 09:46:14,200 INFO test_sccan.py line 250 3325662] Test: [180/1000] Data 0.001 (0.013) Batch 0.050 (0.067) Remain 00:00:55 Loss 0.0595 (0.2304) Accuracy 0.9912.
[2025-07-18 09:46:14,693 INFO test_sccan.py line 250 3325662] Test: [190/1000] Data 0.001 (0.012) Batch 0.049 (0.066) Remain 00:00:53 Loss 0.6531 (0.2232) Accuracy 0.8186.
[2025-07-18 09:46:15,182 INFO test_sccan.py line 250 3325662] Test: [200/1000] Data 0.001 (0.012) Batch 0.050 (0.065) Remain 00:00:52 Loss 7.4546 (0.2663) Accuracy 0.1422.
[2025-07-18 09:46:15,688 INFO test_sccan.py line 250 3325662] Test: [210/1000] Data 0.001 (0.011) Batch 0.051 (0.065) Remain 00:00:51 Loss 1.1766 (0.2632) Accuracy 0.6618.
[2025-07-18 09:46:16,203 INFO test_sccan.py line 250 3325662] Test: [220/1000] Data 0.001 (0.011) Batch 0.053 (0.064) Remain 00:00:49 Loss 0.2082 (0.2621) Accuracy 0.8741.
[2025-07-18 09:46:16,719 INFO test_sccan.py line 250 3325662] Test: [230/1000] Data 0.001 (0.010) Batch 0.048 (0.063) Remain 00:00:48 Loss 0.0848 (0.2593) Accuracy 0.9568.
[2025-07-18 09:46:17,229 INFO test_sccan.py line 250 3325662] Test: [240/1000] Data 0.001 (0.010) Batch 0.049 (0.063) Remain 00:00:47 Loss 0.0327 (0.2528) Accuracy 0.9881.
[2025-07-18 09:46:17,714 INFO test_sccan.py line 250 3325662] Test: [250/1000] Data 0.001 (0.010) Batch 0.047 (0.062) Remain 00:00:46 Loss 0.0115 (0.2474) Accuracy 0.9963.
[2025-07-18 09:46:18,238 INFO test_sccan.py line 250 3325662] Test: [260/1000] Data 0.001 (0.009) Batch 0.049 (0.062) Remain 00:00:45 Loss 4.6543 (0.2595) Accuracy 0.6185.
[2025-07-18 09:46:18,727 INFO test_sccan.py line 250 3325662] Test: [270/1000] Data 0.001 (0.009) Batch 0.049 (0.061) Remain 00:00:44 Loss 0.2211 (0.2566) Accuracy 0.9608.
[2025-07-18 09:46:19,219 INFO test_sccan.py line 250 3325662] Test: [280/1000] Data 0.001 (0.009) Batch 0.049 (0.061) Remain 00:00:43 Loss 0.0194 (0.2533) Accuracy 0.9933.
[2025-07-18 09:46:19,705 INFO test_sccan.py line 250 3325662] Test: [290/1000] Data 0.001 (0.009) Batch 0.049 (0.061) Remain 00:00:43 Loss 0.1797 (0.2563) Accuracy 0.9412.
[2025-07-18 09:46:20,211 INFO test_sccan.py line 250 3325662] Test: [300/1000] Data 0.001 (0.008) Batch 0.050 (0.060) Remain 00:00:42 Loss 0.0453 (0.2538) Accuracy 0.9826.
[2025-07-18 09:46:20,703 INFO test_sccan.py line 250 3325662] Test: [310/1000] Data 0.001 (0.008) Batch 0.049 (0.060) Remain 00:00:41 Loss 0.0052 (0.2551) Accuracy 0.9980.
[2025-07-18 09:46:21,209 INFO test_sccan.py line 250 3325662] Test: [320/1000] Data 0.001 (0.008) Batch 0.050 (0.060) Remain 00:00:40 Loss 1.1192 (0.2547) Accuracy 0.7876.
[2025-07-18 09:46:21,727 INFO test_sccan.py line 250 3325662] Test: [330/1000] Data 0.001 (0.008) Batch 0.051 (0.059) Remain 00:00:39 Loss 0.3700 (0.2567) Accuracy 0.8796.
[2025-07-18 09:46:22,247 INFO test_sccan.py line 250 3325662] Test: [340/1000] Data 0.001 (0.007) Batch 0.053 (0.059) Remain 00:00:39 Loss 0.0579 (0.2631) Accuracy 0.9784.
[2025-07-18 09:46:22,765 INFO test_sccan.py line 250 3325662] Test: [350/1000] Data 0.001 (0.007) Batch 0.048 (0.059) Remain 00:00:38 Loss 0.2334 (0.2693) Accuracy 0.9190.
[2025-07-18 09:46:23,258 INFO test_sccan.py line 250 3325662] Test: [360/1000] Data 0.001 (0.007) Batch 0.050 (0.059) Remain 00:00:37 Loss 0.1753 (0.2664) Accuracy 0.9699.
[2025-07-18 09:46:23,941 INFO test_sccan.py line 250 3325662] Test: [374/1000] Data 0.001 (0.007) Batch 0.047 (0.058) Remain 00:00:36 Loss 0.0787 (0.2681) Accuracy 0.9846.
[2025-07-18 09:46:24,435 INFO test_sccan.py line 250 3325662] Test: [384/1000] Data 0.001 (0.007) Batch 0.050 (0.058) Remain 00:00:35 Loss 0.0964 (0.2730) Accuracy 0.9741.
[2025-07-18 09:46:24,924 INFO test_sccan.py line 250 3325662] Test: [394/1000] Data 0.001 (0.007) Batch 0.047 (0.058) Remain 00:00:35 Loss 0.0818 (0.2679) Accuracy 0.9827.
[2025-07-18 09:46:25,414 INFO test_sccan.py line 250 3325662] Test: [404/1000] Data 0.001 (0.006) Batch 0.050 (0.058) Remain 00:00:34 Loss 0.2464 (0.2754) Accuracy 0.9487.
[2025-07-18 09:46:25,898 INFO test_sccan.py line 250 3325662] Test: [414/1000] Data 0.001 (0.006) Batch 0.048 (0.057) Remain 00:00:33 Loss 0.1073 (0.2713) Accuracy 0.9813.
[2025-07-18 09:46:26,387 INFO test_sccan.py line 250 3325662] Test: [424/1000] Data 0.001 (0.006) Batch 0.051 (0.057) Remain 00:00:32 Loss 2.4000 (0.2799) Accuracy 0.6502.
[2025-07-18 09:46:26,880 INFO test_sccan.py line 250 3325662] Test: [434/1000] Data 0.001 (0.006) Batch 0.049 (0.057) Remain 00:00:32 Loss 0.0650 (0.2772) Accuracy 0.9904.
[2025-07-18 09:46:27,377 INFO test_sccan.py line 250 3325662] Test: [444/1000] Data 0.001 (0.006) Batch 0.050 (0.057) Remain 00:00:31 Loss 0.0741 (0.2739) Accuracy 0.9843.
[2025-07-18 09:46:27,873 INFO test_sccan.py line 250 3325662] Test: [454/1000] Data 0.001 (0.006) Batch 0.050 (0.057) Remain 00:00:30 Loss 0.0601 (0.2697) Accuracy 0.9737.
[2025-07-18 09:46:28,370 INFO test_sccan.py line 250 3325662] Test: [464/1000] Data 0.001 (0.006) Batch 0.051 (0.057) Remain 00:00:30 Loss 0.0774 (0.2690) Accuracy 0.9665.
[2025-07-18 09:46:28,860 INFO test_sccan.py line 250 3325662] Test: [474/1000] Data 0.001 (0.006) Batch 0.049 (0.056) Remain 00:00:29 Loss 0.5414 (0.2684) Accuracy 0.9053.
[2025-07-18 09:46:29,345 INFO test_sccan.py line 250 3325662] Test: [484/1000] Data 0.001 (0.005) Batch 0.049 (0.056) Remain 00:00:29 Loss 0.0562 (0.2670) Accuracy 0.9828.
[2025-07-18 09:46:29,833 INFO test_sccan.py line 250 3325662] Test: [494/1000] Data 0.001 (0.005) Batch 0.047 (0.056) Remain 00:00:28 Loss 0.0250 (0.2778) Accuracy 0.9928.
[2025-07-18 09:46:30,321 INFO test_sccan.py line 250 3325662] Test: [504/1000] Data 0.001 (0.005) Batch 0.049 (0.056) Remain 00:00:27 Loss 0.0486 (0.2796) Accuracy 0.9804.
[2025-07-18 09:46:30,808 INFO test_sccan.py line 250 3325662] Test: [514/1000] Data 0.001 (0.005) Batch 0.049 (0.056) Remain 00:00:27 Loss 0.2839 (0.2782) Accuracy 0.9189.
[2025-07-18 09:46:31,294 INFO test_sccan.py line 250 3325662] Test: [524/1000] Data 0.001 (0.005) Batch 0.049 (0.056) Remain 00:00:26 Loss 0.0259 (0.2748) Accuracy 0.9952.
[2025-07-18 09:46:31,776 INFO test_sccan.py line 250 3325662] Test: [534/1000] Data 0.001 (0.005) Batch 0.048 (0.056) Remain 00:00:25 Loss 0.0221 (0.2742) Accuracy 0.9925.
[2025-07-18 09:46:32,261 INFO test_sccan.py line 250 3325662] Test: [544/1000] Data 0.001 (0.005) Batch 0.048 (0.055) Remain 00:00:25 Loss 0.0585 (0.2823) Accuracy 0.9912.
[2025-07-18 09:46:32,750 INFO test_sccan.py line 250 3325662] Test: [554/1000] Data 0.001 (0.005) Batch 0.049 (0.055) Remain 00:00:24 Loss 0.6442 (0.2795) Accuracy 0.8645.
[2025-07-18 09:46:33,241 INFO test_sccan.py line 250 3325662] Test: [564/1000] Data 0.001 (0.005) Batch 0.049 (0.055) Remain 00:00:24 Loss 6.5375 (0.2916) Accuracy 0.1478.
[2025-07-18 09:46:33,726 INFO test_sccan.py line 250 3325662] Test: [574/1000] Data 0.001 (0.005) Batch 0.047 (0.055) Remain 00:00:23 Loss 1.1302 (0.2893) Accuracy 0.6711.
[2025-07-18 09:46:34,214 INFO test_sccan.py line 250 3325662] Test: [584/1000] Data 0.001 (0.005) Batch 0.048 (0.055) Remain 00:00:22 Loss 1.2092 (0.2898) Accuracy 0.7758.
[2025-07-18 09:46:34,700 INFO test_sccan.py line 250 3325662] Test: [594/1000] Data 0.001 (0.005) Batch 0.049 (0.055) Remain 00:00:22 Loss 0.0557 (0.2907) Accuracy 0.9791.
[2025-07-18 09:46:35,188 INFO test_sccan.py line 250 3325662] Test: [604/1000] Data 0.001 (0.005) Batch 0.049 (0.055) Remain 00:00:21 Loss 0.0354 (0.2881) Accuracy 0.9876.
[2025-07-18 09:46:35,695 INFO test_sccan.py line 250 3325662] Test: [614/1000] Data 0.001 (0.004) Batch 0.048 (0.055) Remain 00:00:21 Loss 0.0106 (0.2854) Accuracy 0.9963.
[2025-07-18 09:46:36,191 INFO test_sccan.py line 250 3325662] Test: [624/1000] Data 0.001 (0.004) Batch 0.050 (0.055) Remain 00:00:20 Loss 0.0203 (0.2877) Accuracy 0.9934.
[2025-07-18 09:46:36,683 INFO test_sccan.py line 250 3325662] Test: [634/1000] Data 0.001 (0.004) Batch 0.049 (0.055) Remain 00:00:19 Loss 0.1826 (0.2864) Accuracy 0.9658.
[2025-07-18 09:46:37,178 INFO test_sccan.py line 250 3325662] Test: [644/1000] Data 0.001 (0.004) Batch 0.050 (0.054) Remain 00:00:19 Loss 1.7270 (0.2872) Accuracy 0.7750.
[2025-07-18 09:46:37,669 INFO test_sccan.py line 250 3325662] Test: [654/1000] Data 0.001 (0.004) Batch 0.049 (0.054) Remain 00:00:18 Loss 0.1866 (0.2883) Accuracy 0.9443.
[2025-07-18 09:46:38,168 INFO test_sccan.py line 250 3325662] Test: [664/1000] Data 0.001 (0.004) Batch 0.050 (0.054) Remain 00:00:18 Loss 0.0854 (0.2859) Accuracy 0.9679.
[2025-07-18 09:46:38,659 INFO test_sccan.py line 250 3325662] Test: [674/1000] Data 0.001 (0.004) Batch 0.050 (0.054) Remain 00:00:17 Loss 0.0045 (0.2838) Accuracy 0.9982.
[2025-07-18 09:46:39,153 INFO test_sccan.py line 250 3325662] Test: [684/1000] Data 0.001 (0.004) Batch 0.049 (0.054) Remain 00:00:17 Loss 1.1211 (0.2832) Accuracy 0.6704.
[2025-07-18 09:46:39,644 INFO test_sccan.py line 250 3325662] Test: [694/1000] Data 0.001 (0.004) Batch 0.049 (0.054) Remain 00:00:16 Loss 0.4256 (0.2827) Accuracy 0.8463.
[2025-07-18 09:46:40,140 INFO test_sccan.py line 250 3325662] Test: [704/1000] Data 0.001 (0.004) Batch 0.052 (0.054) Remain 00:00:15 Loss 0.1291 (0.2796) Accuracy 0.9472.
[2025-07-18 09:46:40,635 INFO test_sccan.py line 250 3325662] Test: [714/1000] Data 0.001 (0.004) Batch 0.049 (0.054) Remain 00:00:15 Loss 0.2122 (0.2803) Accuracy 0.9440.
[2025-07-18 09:46:41,126 INFO test_sccan.py line 250 3325662] Test: [724/1000] Data 0.001 (0.004) Batch 0.049 (0.054) Remain 00:00:14 Loss 0.9299 (0.2800) Accuracy 0.8623.
[2025-07-18 09:46:41,815 INFO test_sccan.py line 250 3325662] Test: [738/1000] Data 0.001 (0.004) Batch 0.049 (0.054) Remain 00:00:14 Loss 0.0789 (0.2813) Accuracy 0.9839.
[2025-07-18 09:46:42,308 INFO test_sccan.py line 250 3325662] Test: [748/1000] Data 0.001 (0.004) Batch 0.050 (0.054) Remain 00:00:13 Loss 0.0760 (0.2798) Accuracy 0.9757.
[2025-07-18 09:46:42,800 INFO test_sccan.py line 250 3325662] Test: [758/1000] Data 0.001 (0.004) Batch 0.048 (0.054) Remain 00:00:12 Loss 0.0724 (0.2770) Accuracy 0.9818.
[2025-07-18 09:46:43,293 INFO test_sccan.py line 250 3325662] Test: [768/1000] Data 0.001 (0.004) Batch 0.049 (0.054) Remain 00:00:12 Loss 0.2846 (0.2795) Accuracy 0.9482.
[2025-07-18 09:46:43,787 INFO test_sccan.py line 250 3325662] Test: [778/1000] Data 0.001 (0.004) Batch 0.050 (0.054) Remain 00:00:11 Loss 0.1151 (0.2779) Accuracy 0.9811.
[2025-07-18 09:46:44,282 INFO test_sccan.py line 250 3325662] Test: [788/1000] Data 0.001 (0.004) Batch 0.050 (0.053) Remain 00:00:11 Loss 1.8666 (0.2806) Accuracy 0.7446.
[2025-07-18 09:46:44,774 INFO test_sccan.py line 250 3325662] Test: [798/1000] Data 0.001 (0.004) Batch 0.049 (0.053) Remain 00:00:10 Loss 0.0590 (0.2786) Accuracy 0.9919.
[2025-07-18 09:46:45,269 INFO test_sccan.py line 250 3325662] Test: [808/1000] Data 0.001 (0.004) Batch 0.050 (0.053) Remain 00:00:10 Loss 0.0744 (0.2768) Accuracy 0.9829.
[2025-07-18 09:46:45,757 INFO test_sccan.py line 250 3325662] Test: [818/1000] Data 0.001 (0.004) Batch 0.049 (0.053) Remain 00:00:09 Loss 0.1121 (0.2743) Accuracy 0.9531.
[2025-07-18 09:46:46,247 INFO test_sccan.py line 250 3325662] Test: [828/1000] Data 0.001 (0.004) Batch 0.050 (0.053) Remain 00:00:09 Loss 0.1554 (0.2753) Accuracy 0.9354.
[2025-07-18 09:46:46,733 INFO test_sccan.py line 250 3325662] Test: [838/1000] Data 0.001 (0.004) Batch 0.048 (0.053) Remain 00:00:08 Loss 0.5223 (0.2748) Accuracy 0.9307.
[2025-07-18 09:46:47,221 INFO test_sccan.py line 250 3325662] Test: [848/1000] Data 0.001 (0.004) Batch 0.049 (0.053) Remain 00:00:08 Loss 0.2141 (0.2743) Accuracy 0.9450.
[2025-07-18 09:46:47,705 INFO test_sccan.py line 250 3325662] Test: [858/1000] Data 0.001 (0.003) Batch 0.048 (0.053) Remain 00:00:07 Loss 0.0144 (0.2777) Accuracy 0.9972.
[2025-07-18 09:46:48,188 INFO test_sccan.py line 250 3325662] Test: [868/1000] Data 0.001 (0.003) Batch 0.048 (0.053) Remain 00:00:07 Loss 0.1380 (0.2783) Accuracy 0.9609.
[2025-07-18 09:46:48,671 INFO test_sccan.py line 250 3325662] Test: [878/1000] Data 0.001 (0.003) Batch 0.047 (0.053) Remain 00:00:06 Loss 0.5520 (0.2772) Accuracy 0.8514.
[2025-07-18 09:46:49,157 INFO test_sccan.py line 250 3325662] Test: [888/1000] Data 0.001 (0.003) Batch 0.048 (0.053) Remain 00:00:05 Loss 0.0201 (0.2752) Accuracy 0.9949.
[2025-07-18 09:46:49,638 INFO test_sccan.py line 250 3325662] Test: [898/1000] Data 0.001 (0.003) Batch 0.048 (0.053) Remain 00:00:05 Loss 0.0152 (0.2751) Accuracy 0.9973.
[2025-07-18 09:46:50,127 INFO test_sccan.py line 250 3325662] Test: [908/1000] Data 0.001 (0.003) Batch 0.049 (0.053) Remain 00:00:04 Loss 0.0519 (0.2743) Accuracy 0.9913.
[2025-07-18 09:46:50,609 INFO test_sccan.py line 250 3325662] Test: [918/1000] Data 0.001 (0.003) Batch 0.048 (0.053) Remain 00:00:04 Loss 0.6387 (0.2728) Accuracy 0.8421.
[2025-07-18 09:46:51,098 INFO test_sccan.py line 250 3325662] Test: [928/1000] Data 0.001 (0.003) Batch 0.049 (0.053) Remain 00:00:03 Loss 6.0821 (0.2783) Accuracy 0.1619.
[2025-07-18 09:46:51,584 INFO test_sccan.py line 250 3325662] Test: [938/1000] Data 0.001 (0.003) Batch 0.048 (0.053) Remain 00:00:03 Loss 1.3657 (0.2773) Accuracy 0.6421.
[2025-07-18 09:46:52,075 INFO test_sccan.py line 250 3325662] Test: [948/1000] Data 0.001 (0.003) Batch 0.050 (0.053) Remain 00:00:02 Loss 0.4531 (0.2766) Accuracy 0.8119.
[2025-07-18 09:46:52,558 INFO test_sccan.py line 250 3325662] Test: [958/1000] Data 0.001 (0.003) Batch 0.048 (0.053) Remain 00:00:02 Loss 0.0674 (0.2776) Accuracy 0.9751.
[2025-07-18 09:46:53,046 INFO test_sccan.py line 250 3325662] Test: [968/1000] Data 0.001 (0.003) Batch 0.048 (0.053) Remain 00:00:01 Loss 0.0357 (0.2758) Accuracy 0.9862.
[2025-07-18 09:46:53,532 INFO test_sccan.py line 250 3325662] Test: [978/1000] Data 0.001 (0.003) Batch 0.050 (0.053) Remain 00:00:01 Loss 0.0117 (0.2740) Accuracy 0.9963.
[2025-07-18 09:46:54,022 INFO test_sccan.py line 250 3325662] Test: [988/1000] Data 0.001 (0.003) Batch 0.050 (0.053) Remain 00:00:00 Loss 4.6543 (0.2769) Accuracy 0.6185.
[2025-07-18 09:46:54,506 INFO test_sccan.py line 250 3325662] Test: [998/1000] Data 0.001 (0.003) Batch 0.048 (0.052) Remain 00:00:00 Loss 0.2560 (0.2761) Accuracy 0.9581.
[2025-07-18 09:46:54,607 INFO test_sccan.py line 264 3325662] meanIoU---Val result: mIoU 0.6631.
[2025-07-18 09:46:54,607 INFO test_sccan.py line 265 3325662] <<<<<<< Novel Results <<<<<<<
[2025-07-18 09:46:54,607 INFO test_sccan.py line 267 3325662] Class_1 Result: iou 0.8243.
[2025-07-18 09:46:54,607 INFO test_sccan.py line 267 3325662] Class_2 Result: iou 0.3904.
[2025-07-18 09:46:54,607 INFO test_sccan.py line 267 3325662] Class_3 Result: iou 0.8314.
[2025-07-18 09:46:54,607 INFO test_sccan.py line 267 3325662] Class_4 Result: iou 0.7249.
[2025-07-18 09:46:54,607 INFO test_sccan.py line 267 3325662] Class_5 Result: iou 0.5443.
[2025-07-18 09:46:54,607 INFO test_sccan.py line 269 3325662] FBIoU---Val result: FBIoU 0.7977.
[2025-07-18 09:46:54,607 INFO test_sccan.py line 271 3325662] Class_0 Result: iou_f 0.9348.
[2025-07-18 09:46:54,607 INFO test_sccan.py line 271 3325662] Class_1 Result: iou_f 0.6607.
[2025-07-18 09:46:54,607 INFO test_sccan.py line 272 3325662] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
total time: 52.4825, avg inference time: 0.0114, count: 1000

Total running time: 00h 00m 52s
Seed0: 123
Seed:  [321]
mIoU:  [0.6631]
FBIoU: [0.7977]
pIoU:  [0.6607]
-------------------------------------------
Best_Seed_m: 321 	 Best_Seed_F: 321 	 Best_Seed_p: 321
Best_mIoU: 0.6631 	 Best_FBIoU: 0.7977 	 Best_pIoU: 0.6607
Mean_mIoU: 0.6631 	 Mean_FBIoU: 0.7977 	 Mean_pIoU: 0.6607
