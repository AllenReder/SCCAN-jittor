[38;5;2m[i 0716 11:45:34.475355 32 compiler.py:956] Jittor(1.3.9.14) src: /data1/fanlyu/anaconda3/envs/jittor/lib/python3.7/site-packages/jittor[m
[38;5;2m[i 0716 11:45:34.478240 32 compiler.py:957] g++ at /usr/bin/g++(11.4.0)[m
[38;5;2m[i 0716 11:45:34.478287 32 compiler.py:958] cache_path: /data1/fanlyu/.cache/jittor/jt1.3.9/g++11.4.0/py3.7.16/Linux-5.15.0-1xe3/IntelRXeonRGolx62/efa5/default[m
[38;5;2m[i 0716 11:45:34.523237 32 install_cuda.py:96] cuda_driver_version: [12, 4][m
[38;5;2m[i 0716 11:45:34.523420 32 install_cuda.py:84] restart /data1/fanlyu/anaconda3/envs/jittor/bin/python ['test_sccan.py', '--config=config/pascal/pascal_split1_resnet50_torch.yaml', '--viz'][m
[38;5;2m[i 0716 11:45:34.676733 12 compiler.py:956] Jittor(1.3.9.14) src: /data1/fanlyu/anaconda3/envs/jittor/lib/python3.7/site-packages/jittor[m
[38;5;2m[i 0716 11:45:34.679854 12 compiler.py:957] g++ at /usr/bin/g++(11.4.0)[m
[38;5;2m[i 0716 11:45:34.679901 12 compiler.py:958] cache_path: /data1/fanlyu/.cache/jittor/jt1.3.9/g++11.4.0/py3.7.16/Linux-5.15.0-1xe3/IntelRXeonRGolx62/efa5/default[m
[38;5;2m[i 0716 11:45:34.724729 12 install_cuda.py:96] cuda_driver_version: [12, 4][m
[38;5;2m[i 0716 11:45:34.728539 12 __init__.py:412] Found /data1/fanlyu/.cache/jittor/jtcuda/cuda12.2_cudnn8_linux/bin/nvcc(12.2.140) at /data1/fanlyu/.cache/jittor/jtcuda/cuda12.2_cudnn8_linux/bin/nvcc.[m
[38;5;2m[i 0716 11:45:34.732149 12 __init__.py:412] Found addr2line(2.38) at /usr/bin/addr2line.[m
[38;5;2m[i 0716 11:45:34.861921 12 compiler.py:1013] cuda key:cu12.2.140_sm_89[m
[38;5;2m[i 0716 11:45:35.177261 12 __init__.py:227] Total mem: 503.54GB, using 16 procs for compiling.[m
[38;5;2m[i 0716 11:45:35.245435 12 jit_compiler.cc:28] Load cc_path: /usr/bin/g++[m
[38;5;2m[i 0716 11:45:35.357916 12 init.cc:63] Found cuda archs: [89,][m
[2025-07-16 11:45:37,115 INFO test_sccan.py line 102 2195023] => creating model ...
[38;5;2m[i 0716 11:45:37.356653 12 cuda_flags.cc:49] CUDA enabled.[m
[2025-07-16 11:45:37,457 INFO test_sccan.py line 66 2195023] => loading checkpoint 'exp/pascal/SCCAN/split1_1shot/resnet50/snapshot/torch_train_epoch_162_0.7287.pth'
[2025-07-16 11:45:37,561 INFO test_sccan.py line 76 2195023] => loaded checkpoint 'exp/pascal/SCCAN/split1_1shot/resnet50/snapshot/torch_train_epoch_162_0.7287.pth' (epoch 162)
[2025-07-16 11:45:42,566 INFO test_sccan.py line 104 2195023] OneModel(
    criterion: CrossEntropyLoss(None, ignore_index=255)
    criterion_dice: WeightedDiceLoss(1.0, reduction=sum)
    backbone: Backbone(
        backbone: ResNet(
            conv1: Conv(3, 64, (3, 3), (2, 2), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
            bn1: FrozenBatchNorm2d(None)
            relu1: relu()
            conv2: Conv(64, 64, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
            bn2: FrozenBatchNorm2d(None)
            relu2: relu()
            conv3: Conv(64, 128, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
            bn3: FrozenBatchNorm2d(None)
            relu3: relu()
            maxpool: Pool((3, 3), (2, 2), (1, 1), dilation=None, return_indices=None, ceil_mode=False, count_include_pad=True, op=maximum, item=None)
            layer1: Sequential(
                0: Bottleneck(
                    conv1: Conv(128, 64, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(64, 64, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(64, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                    downsample: Sequential(
                        0: Conv(128, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                        1: FrozenBatchNorm2d(None)
                    )
                )
                1: Bottleneck(
                    conv1: Conv(256, 64, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(64, 64, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(64, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                2: Bottleneck(
                    conv1: Conv(256, 64, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(64, 64, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(64, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
            )
            layer2: Sequential(
                0: Bottleneck(
                    conv1: Conv(256, 128, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(128, 128, (3, 3), (2, 2), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(128, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                    downsample: Sequential(
                        0: Conv(256, 512, (1, 1), (2, 2), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                        1: FrozenBatchNorm2d(None)
                    )
                )
                1: Bottleneck(
                    conv1: Conv(512, 128, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(128, 128, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(128, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                2: Bottleneck(
                    conv1: Conv(512, 128, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(128, 128, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(128, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                3: Bottleneck(
                    conv1: Conv(512, 128, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(128, 128, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(128, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
            )
            layer3: Sequential(
                0: Bottleneck(
                    conv1: Conv(512, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(256, 256, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(256, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                    downsample: Sequential(
                        0: Conv(512, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                        1: FrozenBatchNorm2d(None)
                    )
                )
                1: Bottleneck(
                    conv1: Conv(1024, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(256, 256, (3, 3), (1, 1), (2, 2), (2, 2), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(256, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                2: Bottleneck(
                    conv1: Conv(1024, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(256, 256, (3, 3), (1, 1), (2, 2), (2, 2), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(256, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                3: Bottleneck(
                    conv1: Conv(1024, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(256, 256, (3, 3), (1, 1), (2, 2), (2, 2), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(256, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                4: Bottleneck(
                    conv1: Conv(1024, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(256, 256, (3, 3), (1, 1), (2, 2), (2, 2), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(256, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                5: Bottleneck(
                    conv1: Conv(1024, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(256, 256, (3, 3), (1, 1), (2, 2), (2, 2), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(256, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
            )
            layer4: Sequential(
                0: Bottleneck(
                    conv1: Conv(1024, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(512, 512, (3, 3), (1, 1), (2, 2), (2, 2), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(512, 2048, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                    downsample: Sequential(
                        0: Conv(1024, 2048, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                        1: FrozenBatchNorm2d(None)
                    )
                )
                1: Bottleneck(
                    conv1: Conv(2048, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(512, 512, (3, 3), (1, 1), (4, 4), (4, 4), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(512, 2048, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                2: Bottleneck(
                    conv1: Conv(2048, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(512, 512, (3, 3), (1, 1), (4, 4), (4, 4), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(512, 2048, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
            )
            avgpool: AdaptiveAvgPool2d((1, 1))
            fc: Linear(2048, 1000, float32[1000,], None)
        )
    )
    down_query: Sequential(
        0: Conv(1536, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
        1: relu()
        2: Dropout2d(0.5, is_train=False)
    )
    down_supp: Sequential(
        0: Conv(1536, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
        1: relu()
        2: Dropout2d(0.5, is_train=False)
    )
    init_merge_query: Sequential(
        0: Conv(513, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
    )
    init_merge_supp: Sequential(
        0: Conv(513, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
    )
    transformer: SwinTransformer(
        pos_drop: Dropout(0.0, is_train=False)
        layers: Sequential(
            0: BasicLayer(
                blocks: Sequential(
                    0: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    1: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    2: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    3: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    4: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    5: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    6: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    7: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                )
            )
        )
        norm0: LayerNorm((256,), 1e-05, elementwise_affine=True)
    )
    ASPP_meta: ASPP(
        layer6_0: Sequential(
            0: Conv(256, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, float32[256,], None, None, Kw=None, fan=None, i=None, bound=None)
            1: relu()
        )
        layer6_1: Sequential(
            0: Conv(256, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, float32[256,], None, None, Kw=None, fan=None, i=None, bound=None)
            1: relu()
        )
        layer6_2: Sequential(
            0: Conv(256, 256, (3, 3), (1, 1), (6, 6), (6, 6), 1, float32[256,], None, None, Kw=None, fan=None, i=None, bound=None)
            1: relu()
        )
        layer6_3: Sequential(
            0: Conv(256, 256, (3, 3), (1, 1), (12, 12), (12, 12), 1, float32[256,], None, None, Kw=None, fan=None, i=None, bound=None)
            1: relu()
        )
        layer6_4: Sequential(
            0: Conv(256, 256, (3, 3), (1, 1), (18, 18), (18, 18), 1, float32[256,], None, None, Kw=None, fan=None, i=None, bound=None)
            1: relu()
        )
    )
    res1_meta: Sequential(
        0: Conv(1280, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
        1: relu()
    )
    res2_meta: Sequential(
        0: Conv(256, 256, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
        1: relu()
        2: Conv(256, 256, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
        3: relu()
    )
    cls_meta: Sequential(
        0: Conv(256, 256, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
        1: relu()
        2: Dropout2d(0.1, is_train=False)
        3: Conv(256, 2, (1, 1), (1, 1), (0, 0), (1, 1), 1, float32[2,], None, None, Kw=None, fan=None, i=None, bound=None)
    )
    relu: relu()
)
[2025-07-16 11:45:42,570 INFO test_sccan.py line 166 2195023] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
SubEpoch_val: True
ann_type: mask
arch: SCCAN
aux_weight1: 1.0
aux_weight2: 1.0
base_lr: 0.005
batch_size: 8
batch_size_val: 1
classes: 2
data_root: ../data/VOCdevkit2012/VOC2012
data_set: pascal
epochs: 200
evaluate: True
fix_bn: True
fix_random_seed_val: True
ignore_label: 255
index_split: -1
kshot_trans_dim: 2
layers: 50
low_fea: layer2
manual_seed: 321
merge: final
merge_tau: 0.9
momentum: 0.9
opts: None
ori_resize: True
padding_label: 255
power: 0.9
print_freq: 10
resized_val: True
resume: None
rotate_max: 10
rotate_min: -10
save_freq: 10
scale_max: 1.1
scale_min: 0.9
seed_deterministic: False
shot: 1
split: 1
start_epoch: 0
stop_interval: 75
train_h: 473
train_list: ./lists/pascal/voc_sbd_merge_noduplicate.txt
train_w: 473
use_split_coco: False
val_list: ./lists/pascal/val.txt
val_size: 473
vgg: False
viz: True
warmup: False
weight: torch_train_epoch_162_0.7287.pth
weight_decay: 0.0001
workers: 8
zoom_factor: 8
Number of Parameters: 37107626
Number of Learnable Parameters: 11373314
sub_list:  [1, 2, 3, 4, 5, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
sub_val_list:  [6, 7, 8, 9, 10]
Val: [1/1] 	 Seed: 321
[2025-07-16 11:45:46,165 INFO test_sccan.py line 251 2195023] Test: [10/1000] Data 0.001 (0.225) Batch 0.056 (0.359) Remain 00:05:55 Loss 0.1673 (1.4969) Accuracy 0.9821.
[2025-07-16 11:45:46,743 INFO test_sccan.py line 251 2195023] Test: [20/1000] Data 0.001 (0.113) Batch 0.059 (0.209) Remain 00:03:24 Loss 0.1880 (1.0936) Accuracy 0.9129.
[2025-07-16 11:45:47,323 INFO test_sccan.py line 251 2195023] Test: [30/1000] Data 0.001 (0.076) Batch 0.059 (0.158) Remain 00:02:33 Loss 0.0182 (0.8582) Accuracy 0.9935.
[2025-07-16 11:45:47,900 INFO test_sccan.py line 251 2195023] Test: [40/1000] Data 0.001 (0.057) Batch 0.058 (0.133) Remain 00:02:07 Loss 0.5133 (0.6914) Accuracy 0.9360.
[2025-07-16 11:45:48,476 INFO test_sccan.py line 251 2195023] Test: [50/1000] Data 0.001 (0.046) Batch 0.057 (0.118) Remain 00:01:52 Loss 0.0050 (0.7368) Accuracy 0.9989.
[2025-07-16 11:45:49,054 INFO test_sccan.py line 251 2195023] Test: [60/1000] Data 0.001 (0.038) Batch 0.058 (0.108) Remain 00:01:41 Loss 0.0178 (0.7735) Accuracy 0.9914.
[2025-07-16 11:45:49,627 INFO test_sccan.py line 251 2195023] Test: [70/1000] Data 0.001 (0.033) Batch 0.058 (0.101) Remain 00:01:33 Loss 0.0487 (0.7238) Accuracy 0.9813.
[2025-07-16 11:45:50,198 INFO test_sccan.py line 251 2195023] Test: [80/1000] Data 0.001 (0.029) Batch 0.057 (0.095) Remain 00:01:27 Loss 0.1906 (0.6628) Accuracy 0.9559.
[2025-07-16 11:45:50,768 INFO test_sccan.py line 251 2195023] Test: [90/1000] Data 0.001 (0.026) Batch 0.055 (0.091) Remain 00:01:22 Loss 2.2284 (0.6714) Accuracy 0.5527.
[2025-07-16 11:45:51,340 INFO test_sccan.py line 251 2195023] Test: [100/1000] Data 0.001 (0.023) Batch 0.059 (0.088) Remain 00:01:18 Loss 0.0222 (0.6149) Accuracy 0.9922.
[2025-07-16 11:45:51,917 INFO test_sccan.py line 251 2195023] Test: [110/1000] Data 0.001 (0.021) Batch 0.059 (0.085) Remain 00:01:15 Loss 0.0447 (0.5795) Accuracy 0.9812.
[2025-07-16 11:45:52,512 INFO test_sccan.py line 251 2195023] Test: [120/1000] Data 0.001 (0.020) Batch 0.070 (0.083) Remain 00:01:12 Loss 1.7348 (0.5571) Accuracy 0.7581.
[2025-07-16 11:45:53,080 INFO test_sccan.py line 251 2195023] Test: [130/1000] Data 0.001 (0.018) Batch 0.055 (0.081) Remain 00:01:10 Loss 0.0376 (0.5528) Accuracy 0.9864.
[2025-07-16 11:45:53,651 INFO test_sccan.py line 251 2195023] Test: [140/1000] Data 0.001 (0.017) Batch 0.058 (0.079) Remain 00:01:08 Loss 0.0123 (0.5316) Accuracy 0.9965.
[2025-07-16 11:45:54,222 INFO test_sccan.py line 251 2195023] Test: [150/1000] Data 0.001 (0.016) Batch 0.058 (0.078) Remain 00:01:06 Loss 0.0072 (0.5102) Accuracy 0.9975.
[2025-07-16 11:45:54,786 INFO test_sccan.py line 251 2195023] Test: [160/1000] Data 0.001 (0.015) Batch 0.055 (0.076) Remain 00:01:04 Loss 0.5554 (0.5092) Accuracy 0.8773.
[2025-07-16 11:45:55,349 INFO test_sccan.py line 251 2195023] Test: [170/1000] Data 0.001 (0.014) Batch 0.055 (0.075) Remain 00:01:02 Loss 0.3717 (0.5114) Accuracy 0.9223.
[2025-07-16 11:45:55,919 INFO test_sccan.py line 251 2195023] Test: [180/1000] Data 0.001 (0.013) Batch 0.058 (0.074) Remain 00:01:00 Loss 0.0173 (0.5163) Accuracy 0.9963.
[2025-07-16 11:45:56,490 INFO test_sccan.py line 251 2195023] Test: [190/1000] Data 0.001 (0.013) Batch 0.058 (0.073) Remain 00:00:59 Loss 1.0674 (0.5164) Accuracy 0.8581.
[2025-07-16 11:45:57,054 INFO test_sccan.py line 251 2195023] Test: [200/1000] Data 0.001 (0.012) Batch 0.055 (0.072) Remain 00:00:57 Loss 0.1320 (0.5138) Accuracy 0.9852.
[2025-07-16 11:45:57,618 INFO test_sccan.py line 251 2195023] Test: [210/1000] Data 0.001 (0.012) Batch 0.055 (0.072) Remain 00:00:56 Loss 0.0268 (0.5144) Accuracy 0.9898.
[2025-07-16 11:45:58,189 INFO test_sccan.py line 251 2195023] Test: [220/1000] Data 0.001 (0.011) Batch 0.060 (0.071) Remain 00:00:55 Loss 0.0957 (0.5211) Accuracy 0.9781.
[2025-07-16 11:45:58,761 INFO test_sccan.py line 251 2195023] Test: [230/1000] Data 0.001 (0.011) Batch 0.059 (0.070) Remain 00:00:54 Loss 0.1811 (0.5133) Accuracy 0.9291.
[2025-07-16 11:45:59,328 INFO test_sccan.py line 251 2195023] Test: [240/1000] Data 0.001 (0.010) Batch 0.055 (0.070) Remain 00:00:53 Loss 1.9524 (0.5086) Accuracy 0.7837.
[2025-07-16 11:45:59,892 INFO test_sccan.py line 251 2195023] Test: [250/1000] Data 0.001 (0.010) Batch 0.056 (0.069) Remain 00:00:51 Loss 0.0213 (0.5028) Accuracy 0.9940.
[2025-07-16 11:46:00,461 INFO test_sccan.py line 251 2195023] Test: [260/1000] Data 0.001 (0.009) Batch 0.059 (0.069) Remain 00:00:50 Loss 0.0076 (0.5013) Accuracy 0.9975.
[2025-07-16 11:46:01,034 INFO test_sccan.py line 251 2195023] Test: [270/1000] Data 0.001 (0.009) Batch 0.058 (0.068) Remain 00:00:49 Loss 0.1187 (0.4896) Accuracy 0.9889.
[2025-07-16 11:46:01,601 INFO test_sccan.py line 251 2195023] Test: [280/1000] Data 0.001 (0.009) Batch 0.056 (0.068) Remain 00:00:48 Loss 0.0074 (0.4779) Accuracy 0.9979.
[2025-07-16 11:46:02,174 INFO test_sccan.py line 251 2195023] Test: [290/1000] Data 0.001 (0.009) Batch 0.058 (0.068) Remain 00:00:47 Loss 0.0355 (0.4798) Accuracy 0.9863.
[2025-07-16 11:46:02,745 INFO test_sccan.py line 251 2195023] Test: [300/1000] Data 0.001 (0.008) Batch 0.058 (0.067) Remain 00:00:47 Loss 0.0194 (0.4696) Accuracy 0.9958.
[2025-07-16 11:46:03,316 INFO test_sccan.py line 251 2195023] Test: [310/1000] Data 0.001 (0.008) Batch 0.058 (0.067) Remain 00:00:46 Loss 0.1147 (0.4598) Accuracy 0.9611.
[2025-07-16 11:46:03,882 INFO test_sccan.py line 251 2195023] Test: [320/1000] Data 0.001 (0.008) Batch 0.056 (0.067) Remain 00:00:45 Loss 0.9348 (0.4633) Accuracy 0.7503.
[2025-07-16 11:46:04,446 INFO test_sccan.py line 251 2195023] Test: [330/1000] Data 0.001 (0.008) Batch 0.055 (0.066) Remain 00:00:44 Loss 0.9640 (0.4755) Accuracy 0.8699.
[2025-07-16 11:46:05,015 INFO test_sccan.py line 251 2195023] Test: [340/1000] Data 0.001 (0.007) Batch 0.058 (0.066) Remain 00:00:43 Loss 0.1072 (0.4779) Accuracy 0.9817.
[2025-07-16 11:46:05,584 INFO test_sccan.py line 251 2195023] Test: [350/1000] Data 0.001 (0.007) Batch 0.058 (0.066) Remain 00:00:42 Loss 0.4910 (0.4707) Accuracy 0.9007.
[2025-07-16 11:46:06,151 INFO test_sccan.py line 251 2195023] Test: [360/1000] Data 0.001 (0.007) Batch 0.056 (0.065) Remain 00:00:41 Loss 0.0075 (0.4645) Accuracy 0.9979.
[2025-07-16 11:46:06,716 INFO test_sccan.py line 251 2195023] Test: [370/1000] Data 0.001 (0.007) Batch 0.055 (0.065) Remain 00:00:41 Loss 0.0384 (0.4614) Accuracy 0.9921.
[2025-07-16 11:46:07,285 INFO test_sccan.py line 251 2195023] Test: [380/1000] Data 0.001 (0.007) Batch 0.058 (0.065) Remain 00:00:40 Loss 3.8836 (0.4672) Accuracy 0.5574.
[2025-07-16 11:46:07,855 INFO test_sccan.py line 251 2195023] Test: [390/1000] Data 0.001 (0.007) Batch 0.058 (0.065) Remain 00:00:39 Loss 0.4417 (0.4721) Accuracy 0.8885.
[2025-07-16 11:46:08,421 INFO test_sccan.py line 251 2195023] Test: [400/1000] Data 0.001 (0.006) Batch 0.055 (0.065) Remain 00:00:38 Loss 0.2651 (0.4646) Accuracy 0.9684.
[2025-07-16 11:46:08,987 INFO test_sccan.py line 251 2195023] Test: [410/1000] Data 0.001 (0.006) Batch 0.056 (0.064) Remain 00:00:38 Loss 0.0187 (0.4572) Accuracy 0.9940.
[2025-07-16 11:46:09,556 INFO test_sccan.py line 251 2195023] Test: [420/1000] Data 0.001 (0.006) Batch 0.058 (0.064) Remain 00:00:37 Loss 0.0167 (0.4517) Accuracy 0.9937.
[2025-07-16 11:46:10,127 INFO test_sccan.py line 251 2195023] Test: [430/1000] Data 0.001 (0.006) Batch 0.058 (0.064) Remain 00:00:36 Loss 1.5727 (0.4628) Accuracy 0.7839.
[2025-07-16 11:46:10,692 INFO test_sccan.py line 251 2195023] Test: [440/1000] Data 0.001 (0.006) Batch 0.055 (0.064) Remain 00:00:35 Loss 2.8974 (0.4695) Accuracy 0.7189.
[2025-07-16 11:46:11,545 INFO test_sccan.py line 251 2195023] Test: [455/1000] Data 0.001 (0.006) Batch 0.056 (0.064) Remain 00:00:34 Loss 0.5150 (0.4658) Accuracy 0.8336.
[2025-07-16 11:46:12,109 INFO test_sccan.py line 251 2195023] Test: [465/1000] Data 0.001 (0.006) Batch 0.055 (0.064) Remain 00:00:33 Loss 0.1537 (0.4702) Accuracy 0.9276.
[2025-07-16 11:46:12,676 INFO test_sccan.py line 251 2195023] Test: [475/1000] Data 0.001 (0.006) Batch 0.058 (0.063) Remain 00:00:33 Loss 0.0186 (0.4765) Accuracy 0.9934.
[2025-07-16 11:46:13,246 INFO test_sccan.py line 251 2195023] Test: [485/1000] Data 0.001 (0.005) Batch 0.058 (0.063) Remain 00:00:32 Loss 0.5777 (0.4757) Accuracy 0.9360.
[2025-07-16 11:46:13,815 INFO test_sccan.py line 251 2195023] Test: [495/1000] Data 0.001 (0.005) Batch 0.057 (0.063) Remain 00:00:31 Loss 0.0036 (0.4849) Accuracy 0.9988.
[2025-07-16 11:46:14,380 INFO test_sccan.py line 251 2195023] Test: [505/1000] Data 0.001 (0.005) Batch 0.055 (0.063) Remain 00:00:31 Loss 0.0213 (0.4949) Accuracy 0.9902.
[2025-07-16 11:46:14,947 INFO test_sccan.py line 251 2195023] Test: [515/1000] Data 0.001 (0.005) Batch 0.057 (0.063) Remain 00:00:30 Loss 0.0490 (0.4933) Accuracy 0.9817.
[2025-07-16 11:46:15,520 INFO test_sccan.py line 251 2195023] Test: [525/1000] Data 0.001 (0.005) Batch 0.059 (0.063) Remain 00:00:29 Loss 0.2459 (0.4892) Accuracy 0.9215.
[2025-07-16 11:46:16,088 INFO test_sccan.py line 251 2195023] Test: [535/1000] Data 0.001 (0.005) Batch 0.057 (0.063) Remain 00:00:29 Loss 2.1125 (0.4932) Accuracy 0.5928.
[2025-07-16 11:46:16,654 INFO test_sccan.py line 251 2195023] Test: [545/1000] Data 0.001 (0.005) Batch 0.055 (0.063) Remain 00:00:28 Loss 0.2557 (0.4866) Accuracy 0.9165.
[2025-07-16 11:46:17,220 INFO test_sccan.py line 251 2195023] Test: [555/1000] Data 0.001 (0.005) Batch 0.057 (0.062) Remain 00:00:27 Loss 0.0563 (0.4821) Accuracy 0.9766.
[2025-07-16 11:46:17,790 INFO test_sccan.py line 251 2195023] Test: [565/1000] Data 0.001 (0.005) Batch 0.059 (0.062) Remain 00:00:27 Loss 1.8559 (0.4787) Accuracy 0.7487.
[2025-07-16 11:46:18,364 INFO test_sccan.py line 251 2195023] Test: [575/1000] Data 0.001 (0.005) Batch 0.058 (0.062) Remain 00:00:26 Loss 0.0365 (0.4796) Accuracy 0.9860.
[2025-07-16 11:46:18,933 INFO test_sccan.py line 251 2195023] Test: [585/1000] Data 0.001 (0.005) Batch 0.055 (0.062) Remain 00:00:25 Loss 0.0124 (0.4741) Accuracy 0.9965.
[2025-07-16 11:46:19,503 INFO test_sccan.py line 251 2195023] Test: [595/1000] Data 0.001 (0.005) Batch 0.058 (0.062) Remain 00:00:25 Loss 0.0074 (0.4697) Accuracy 0.9974.
[2025-07-16 11:46:20,078 INFO test_sccan.py line 251 2195023] Test: [605/1000] Data 0.001 (0.005) Batch 0.058 (0.062) Remain 00:00:24 Loss 0.7358 (0.4725) Accuracy 0.8666.
[2025-07-16 11:46:20,654 INFO test_sccan.py line 251 2195023] Test: [615/1000] Data 0.001 (0.004) Batch 0.058 (0.062) Remain 00:00:23 Loss 0.3595 (0.4732) Accuracy 0.9222.
[2025-07-16 11:46:21,226 INFO test_sccan.py line 251 2195023] Test: [625/1000] Data 0.001 (0.004) Batch 0.056 (0.062) Remain 00:00:23 Loss 0.0177 (0.4762) Accuracy 0.9961.
[2025-07-16 11:46:21,796 INFO test_sccan.py line 251 2195023] Test: [635/1000] Data 0.001 (0.004) Batch 0.057 (0.062) Remain 00:00:22 Loss 1.0242 (0.4765) Accuracy 0.8594.
[2025-07-16 11:46:22,370 INFO test_sccan.py line 251 2195023] Test: [645/1000] Data 0.001 (0.004) Batch 0.058 (0.062) Remain 00:00:21 Loss 0.1340 (0.4768) Accuracy 0.9853.
[2025-07-16 11:46:22,945 INFO test_sccan.py line 251 2195023] Test: [655/1000] Data 0.001 (0.004) Batch 0.058 (0.062) Remain 00:00:21 Loss 0.0287 (0.4900) Accuracy 0.9893.
[2025-07-16 11:46:23,515 INFO test_sccan.py line 251 2195023] Test: [665/1000] Data 0.001 (0.004) Batch 0.055 (0.062) Remain 00:00:20 Loss 0.1064 (0.4892) Accuracy 0.9774.
[2025-07-16 11:46:24,081 INFO test_sccan.py line 251 2195023] Test: [675/1000] Data 0.001 (0.004) Batch 0.057 (0.061) Remain 00:00:19 Loss 0.1768 (0.4861) Accuracy 0.9418.
[2025-07-16 11:46:24,653 INFO test_sccan.py line 251 2195023] Test: [685/1000] Data 0.001 (0.004) Batch 0.058 (0.061) Remain 00:00:19 Loss 2.1860 (0.4849) Accuracy 0.7751.
[2025-07-16 11:46:25,223 INFO test_sccan.py line 251 2195023] Test: [695/1000] Data 0.001 (0.004) Batch 0.057 (0.061) Remain 00:00:18 Loss 0.0288 (0.4829) Accuracy 0.9927.
[2025-07-16 11:46:25,789 INFO test_sccan.py line 251 2195023] Test: [705/1000] Data 0.001 (0.004) Batch 0.055 (0.061) Remain 00:00:18 Loss 0.0072 (0.4828) Accuracy 0.9975.
[2025-07-16 11:46:26,359 INFO test_sccan.py line 251 2195023] Test: [715/1000] Data 0.001 (0.004) Batch 0.057 (0.061) Remain 00:00:17 Loss 0.1254 (0.4785) Accuracy 0.9889.
[2025-07-16 11:46:26,931 INFO test_sccan.py line 251 2195023] Test: [725/1000] Data 0.001 (0.004) Batch 0.058 (0.061) Remain 00:00:16 Loss 0.0075 (0.4786) Accuracy 0.9980.
[2025-07-16 11:46:27,501 INFO test_sccan.py line 251 2195023] Test: [735/1000] Data 0.001 (0.004) Batch 0.057 (0.061) Remain 00:00:16 Loss 0.0365 (0.4790) Accuracy 0.9860.
[2025-07-16 11:46:28,066 INFO test_sccan.py line 251 2195023] Test: [745/1000] Data 0.001 (0.004) Batch 0.055 (0.061) Remain 00:00:15 Loss 0.0189 (0.4769) Accuracy 0.9960.
[2025-07-16 11:46:28,634 INFO test_sccan.py line 251 2195023] Test: [755/1000] Data 0.001 (0.004) Batch 0.057 (0.061) Remain 00:00:14 Loss 3.3711 (0.4768) Accuracy 0.5926.
[2025-07-16 11:46:29,208 INFO test_sccan.py line 251 2195023] Test: [765/1000] Data 0.001 (0.004) Batch 0.058 (0.061) Remain 00:00:14 Loss 1.1549 (0.4771) Accuracy 0.6782.
[2025-07-16 11:46:29,777 INFO test_sccan.py line 251 2195023] Test: [775/1000] Data 0.001 (0.004) Batch 0.057 (0.061) Remain 00:00:13 Loss 0.9285 (0.4814) Accuracy 0.8697.
[2025-07-16 11:46:30,344 INFO test_sccan.py line 251 2195023] Test: [785/1000] Data 0.001 (0.004) Batch 0.056 (0.061) Remain 00:00:13 Loss 0.1027 (0.4814) Accuracy 0.9822.
[2025-07-16 11:46:30,910 INFO test_sccan.py line 251 2195023] Test: [795/1000] Data 0.001 (0.004) Batch 0.057 (0.061) Remain 00:00:12 Loss 0.5187 (0.4775) Accuracy 0.9003.
[2025-07-16 11:46:31,485 INFO test_sccan.py line 251 2195023] Test: [805/1000] Data 0.001 (0.004) Batch 0.058 (0.061) Remain 00:00:11 Loss 0.0071 (0.4749) Accuracy 0.9980.
[2025-07-16 11:46:32,055 INFO test_sccan.py line 251 2195023] Test: [815/1000] Data 0.001 (0.004) Batch 0.057 (0.061) Remain 00:00:11 Loss 0.0397 (0.4808) Accuracy 0.9924.
[2025-07-16 11:46:32,620 INFO test_sccan.py line 251 2195023] Test: [825/1000] Data 0.001 (0.004) Batch 0.055 (0.061) Remain 00:00:10 Loss 3.8795 (0.4833) Accuracy 0.5581.
[2025-07-16 11:46:33,187 INFO test_sccan.py line 251 2195023] Test: [835/1000] Data 0.001 (0.004) Batch 0.057 (0.061) Remain 00:00:10 Loss 0.3418 (0.4865) Accuracy 0.8933.
[2025-07-16 11:46:33,762 INFO test_sccan.py line 251 2195023] Test: [845/1000] Data 0.001 (0.004) Batch 0.058 (0.061) Remain 00:00:09 Loss 0.2650 (0.4841) Accuracy 0.9682.
[2025-07-16 11:46:34,331 INFO test_sccan.py line 251 2195023] Test: [855/1000] Data 0.001 (0.003) Batch 0.057 (0.061) Remain 00:00:08 Loss 0.0197 (0.4803) Accuracy 0.9938.
[2025-07-16 11:46:34,899 INFO test_sccan.py line 251 2195023] Test: [865/1000] Data 0.001 (0.003) Batch 0.055 (0.060) Remain 00:00:08 Loss 0.0171 (0.4774) Accuracy 0.9935.
[2025-07-16 11:46:35,466 INFO test_sccan.py line 251 2195023] Test: [875/1000] Data 0.001 (0.003) Batch 0.057 (0.060) Remain 00:00:07 Loss 1.3178 (0.4813) Accuracy 0.7672.
[2025-07-16 11:46:36,038 INFO test_sccan.py line 251 2195023] Test: [885/1000] Data 0.001 (0.003) Batch 0.059 (0.060) Remain 00:00:06 Loss 2.4781 (0.4835) Accuracy 0.7006.
[2025-07-16 11:46:36,891 INFO test_sccan.py line 251 2195023] Test: [900/1000] Data 0.001 (0.003) Batch 0.059 (0.060) Remain 00:00:06 Loss 0.7101 (0.4810) Accuracy 0.8787.
[2025-07-16 11:46:37,463 INFO test_sccan.py line 251 2195023] Test: [910/1000] Data 0.001 (0.003) Batch 0.058 (0.060) Remain 00:00:05 Loss 0.1448 (0.4839) Accuracy 0.9256.
[2025-07-16 11:46:38,030 INFO test_sccan.py line 251 2195023] Test: [920/1000] Data 0.001 (0.003) Batch 0.056 (0.060) Remain 00:00:04 Loss 0.0184 (0.4821) Accuracy 0.9932.
[2025-07-16 11:46:38,595 INFO test_sccan.py line 251 2195023] Test: [930/1000] Data 0.001 (0.003) Batch 0.056 (0.060) Remain 00:00:04 Loss 0.7808 (0.4787) Accuracy 0.9355.
[2025-07-16 11:46:39,168 INFO test_sccan.py line 251 2195023] Test: [940/1000] Data 0.001 (0.003) Batch 0.059 (0.060) Remain 00:00:03 Loss 0.0036 (0.4839) Accuracy 0.9989.
[2025-07-16 11:46:39,742 INFO test_sccan.py line 251 2195023] Test: [950/1000] Data 0.001 (0.003) Batch 0.059 (0.060) Remain 00:00:03 Loss 0.0179 (0.4900) Accuracy 0.9912.
[2025-07-16 11:46:40,310 INFO test_sccan.py line 251 2195023] Test: [960/1000] Data 0.001 (0.003) Batch 0.055 (0.060) Remain 00:00:02 Loss 0.0494 (0.4892) Accuracy 0.9812.
[2025-07-16 11:46:40,875 INFO test_sccan.py line 251 2195023] Test: [970/1000] Data 0.001 (0.003) Batch 0.055 (0.060) Remain 00:00:01 Loss 0.1889 (0.4894) Accuracy 0.9558.
[2025-07-16 11:46:41,446 INFO test_sccan.py line 251 2195023] Test: [980/1000] Data 0.001 (0.003) Batch 0.059 (0.060) Remain 00:00:01 Loss 2.1430 (0.4910) Accuracy 0.6164.
[2025-07-16 11:46:42,019 INFO test_sccan.py line 251 2195023] Test: [990/1000] Data 0.001 (0.003) Batch 0.059 (0.060) Remain 00:00:00 Loss 0.0530 (0.4875) Accuracy 0.9747.
[2025-07-16 11:46:42,587 INFO test_sccan.py line 251 2195023] Test: [1000/1000] Data 0.001 (0.003) Batch 0.056 (0.060) Remain 00:00:00 Loss 0.0518 (0.4850) Accuracy 0.9775.
[2025-07-16 11:46:42,588 INFO test_sccan.py line 265 2195023] meanIoU---Val result: mIoU 0.7318.
[2025-07-16 11:46:42,588 INFO test_sccan.py line 266 2195023] <<<<<<< Novel Results <<<<<<<
[2025-07-16 11:46:42,588 INFO test_sccan.py line 268 2195023] Class_1 Result: iou 0.8999.
[2025-07-16 11:46:42,588 INFO test_sccan.py line 268 2195023] Class_2 Result: iou 0.6484.
[2025-07-16 11:46:42,588 INFO test_sccan.py line 268 2195023] Class_3 Result: iou 0.8988.
[2025-07-16 11:46:42,588 INFO test_sccan.py line 268 2195023] Class_4 Result: iou 0.2924.
[2025-07-16 11:46:42,588 INFO test_sccan.py line 268 2195023] Class_5 Result: iou 0.9195.
[2025-07-16 11:46:42,588 INFO test_sccan.py line 270 2195023] FBIoU---Val result: FBIoU 0.8169.
[2025-07-16 11:46:42,588 INFO test_sccan.py line 272 2195023] Class_0 Result: iou_f 0.9012.
[2025-07-16 11:46:42,588 INFO test_sccan.py line 272 2195023] Class_1 Result: iou_f 0.7327.
[2025-07-16 11:46:42,588 INFO test_sccan.py line 273 2195023] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
total time: 60.0145, avg inference time: 0.0108, count: 1000

Total running time: 00h 01m 00s
Seed0: 123
Seed:  [321]
mIoU:  [0.7318]
FBIoU: [0.8169]
pIoU:  [0.7327]
-------------------------------------------
Best_Seed_m: 321 	 Best_Seed_F: 321 	 Best_Seed_p: 321
Best_mIoU: 0.7318 	 Best_FBIoU: 0.8169 	 Best_pIoU: 0.7327
Mean_mIoU: 0.7318 	 Mean_FBIoU: 0.8169 	 Mean_pIoU: 0.7327
