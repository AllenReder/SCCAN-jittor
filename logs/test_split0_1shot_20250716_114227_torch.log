[38;5;2m[i 0716 11:42:27.335948 48 compiler.py:956] Jittor(1.3.9.14) src: /data1/fanlyu/anaconda3/envs/jittor/lib/python3.7/site-packages/jittor[m
[38;5;2m[i 0716 11:42:27.338745 48 compiler.py:957] g++ at /usr/bin/g++(11.4.0)[m
[38;5;2m[i 0716 11:42:27.338792 48 compiler.py:958] cache_path: /data1/fanlyu/.cache/jittor/jt1.3.9/g++11.4.0/py3.7.16/Linux-5.15.0-1xe3/IntelRXeonRGolx62/efa5/default[m
[38;5;2m[i 0716 11:42:27.382337 48 install_cuda.py:96] cuda_driver_version: [12, 4][m
[38;5;2m[i 0716 11:42:27.382526 48 install_cuda.py:84] restart /data1/fanlyu/anaconda3/envs/jittor/bin/python ['test_sccan.py', '--config=config/pascal/pascal_split0_resnet50_torch.yaml', '--viz'][m
[38;5;2m[i 0716 11:42:27.555978 40 compiler.py:956] Jittor(1.3.9.14) src: /data1/fanlyu/anaconda3/envs/jittor/lib/python3.7/site-packages/jittor[m
[38;5;2m[i 0716 11:42:27.558750 40 compiler.py:957] g++ at /usr/bin/g++(11.4.0)[m
[38;5;2m[i 0716 11:42:27.558795 40 compiler.py:958] cache_path: /data1/fanlyu/.cache/jittor/jt1.3.9/g++11.4.0/py3.7.16/Linux-5.15.0-1xe3/IntelRXeonRGolx62/efa5/default[m
[38;5;2m[i 0716 11:42:27.604859 40 install_cuda.py:96] cuda_driver_version: [12, 4][m
[38;5;2m[i 0716 11:42:27.609986 40 __init__.py:412] Found /data1/fanlyu/.cache/jittor/jtcuda/cuda12.2_cudnn8_linux/bin/nvcc(12.2.140) at /data1/fanlyu/.cache/jittor/jtcuda/cuda12.2_cudnn8_linux/bin/nvcc.[m
[38;5;2m[i 0716 11:42:27.615064 40 __init__.py:412] Found addr2line(2.38) at /usr/bin/addr2line.[m
[38;5;2m[i 0716 11:42:27.723028 40 compiler.py:1013] cuda key:cu12.2.140_sm_89[m
[38;5;2m[i 0716 11:42:28.043644 40 __init__.py:227] Total mem: 503.54GB, using 16 procs for compiling.[m
[38;5;2m[i 0716 11:42:28.116549 40 jit_compiler.cc:28] Load cc_path: /usr/bin/g++[m
[38;5;2m[i 0716 11:42:28.207615 40 init.cc:63] Found cuda archs: [89,][m
[2025-07-16 11:42:29,967 INFO test_sccan.py line 102 2193550] => creating model ...
[38;5;2m[i 0716 11:42:30.212795 40 cuda_flags.cc:49] CUDA enabled.[m
[2025-07-16 11:42:30,313 INFO test_sccan.py line 66 2193550] => loading checkpoint 'exp/pascal/SCCAN/split0_1shot/resnet50/snapshot/torch_train_epoch_35_0.6672.pth'
[2025-07-16 11:42:30,423 INFO test_sccan.py line 76 2193550] => loaded checkpoint 'exp/pascal/SCCAN/split0_1shot/resnet50/snapshot/torch_train_epoch_35_0.6672.pth' (epoch 35)
[2025-07-16 11:42:35,431 INFO test_sccan.py line 104 2193550] OneModel(
    criterion: CrossEntropyLoss(None, ignore_index=255)
    criterion_dice: WeightedDiceLoss(1.0, reduction=sum)
    backbone: Backbone(
        backbone: ResNet(
            conv1: Conv(3, 64, (3, 3), (2, 2), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
            bn1: FrozenBatchNorm2d(None)
            relu1: relu()
            conv2: Conv(64, 64, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
            bn2: FrozenBatchNorm2d(None)
            relu2: relu()
            conv3: Conv(64, 128, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
            bn3: FrozenBatchNorm2d(None)
            relu3: relu()
            maxpool: Pool((3, 3), (2, 2), (1, 1), dilation=None, return_indices=None, ceil_mode=False, count_include_pad=True, op=maximum, item=None)
            layer1: Sequential(
                0: Bottleneck(
                    conv1: Conv(128, 64, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(64, 64, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(64, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                    downsample: Sequential(
                        0: Conv(128, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                        1: FrozenBatchNorm2d(None)
                    )
                )
                1: Bottleneck(
                    conv1: Conv(256, 64, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(64, 64, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(64, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                2: Bottleneck(
                    conv1: Conv(256, 64, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(64, 64, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(64, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
            )
            layer2: Sequential(
                0: Bottleneck(
                    conv1: Conv(256, 128, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(128, 128, (3, 3), (2, 2), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(128, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                    downsample: Sequential(
                        0: Conv(256, 512, (1, 1), (2, 2), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                        1: FrozenBatchNorm2d(None)
                    )
                )
                1: Bottleneck(
                    conv1: Conv(512, 128, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(128, 128, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(128, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                2: Bottleneck(
                    conv1: Conv(512, 128, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(128, 128, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(128, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                3: Bottleneck(
                    conv1: Conv(512, 128, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(128, 128, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(128, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
            )
            layer3: Sequential(
                0: Bottleneck(
                    conv1: Conv(512, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(256, 256, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(256, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                    downsample: Sequential(
                        0: Conv(512, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                        1: FrozenBatchNorm2d(None)
                    )
                )
                1: Bottleneck(
                    conv1: Conv(1024, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(256, 256, (3, 3), (1, 1), (2, 2), (2, 2), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(256, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                2: Bottleneck(
                    conv1: Conv(1024, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(256, 256, (3, 3), (1, 1), (2, 2), (2, 2), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(256, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                3: Bottleneck(
                    conv1: Conv(1024, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(256, 256, (3, 3), (1, 1), (2, 2), (2, 2), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(256, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                4: Bottleneck(
                    conv1: Conv(1024, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(256, 256, (3, 3), (1, 1), (2, 2), (2, 2), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(256, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                5: Bottleneck(
                    conv1: Conv(1024, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(256, 256, (3, 3), (1, 1), (2, 2), (2, 2), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(256, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
            )
            layer4: Sequential(
                0: Bottleneck(
                    conv1: Conv(1024, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(512, 512, (3, 3), (1, 1), (2, 2), (2, 2), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(512, 2048, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                    downsample: Sequential(
                        0: Conv(1024, 2048, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                        1: FrozenBatchNorm2d(None)
                    )
                )
                1: Bottleneck(
                    conv1: Conv(2048, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(512, 512, (3, 3), (1, 1), (4, 4), (4, 4), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(512, 2048, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                2: Bottleneck(
                    conv1: Conv(2048, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(512, 512, (3, 3), (1, 1), (4, 4), (4, 4), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(512, 2048, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
            )
            avgpool: AdaptiveAvgPool2d((1, 1))
            fc: Linear(2048, 1000, float32[1000,], None)
        )
    )
    down_query: Sequential(
        0: Conv(1536, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
        1: relu()
        2: Dropout2d(0.5, is_train=False)
    )
    down_supp: Sequential(
        0: Conv(1536, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
        1: relu()
        2: Dropout2d(0.5, is_train=False)
    )
    init_merge_query: Sequential(
        0: Conv(513, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
    )
    init_merge_supp: Sequential(
        0: Conv(513, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
    )
    transformer: SwinTransformer(
        pos_drop: Dropout(0.0, is_train=False)
        layers: Sequential(
            0: BasicLayer(
                blocks: Sequential(
                    0: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    1: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    2: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    3: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    4: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    5: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    6: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    7: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                )
            )
        )
        norm0: LayerNorm((256,), 1e-05, elementwise_affine=True)
    )
    ASPP_meta: ASPP(
        layer6_0: Sequential(
            0: Conv(256, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, float32[256,], None, None, Kw=None, fan=None, i=None, bound=None)
            1: relu()
        )
        layer6_1: Sequential(
            0: Conv(256, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, float32[256,], None, None, Kw=None, fan=None, i=None, bound=None)
            1: relu()
        )
        layer6_2: Sequential(
            0: Conv(256, 256, (3, 3), (1, 1), (6, 6), (6, 6), 1, float32[256,], None, None, Kw=None, fan=None, i=None, bound=None)
            1: relu()
        )
        layer6_3: Sequential(
            0: Conv(256, 256, (3, 3), (1, 1), (12, 12), (12, 12), 1, float32[256,], None, None, Kw=None, fan=None, i=None, bound=None)
            1: relu()
        )
        layer6_4: Sequential(
            0: Conv(256, 256, (3, 3), (1, 1), (18, 18), (18, 18), 1, float32[256,], None, None, Kw=None, fan=None, i=None, bound=None)
            1: relu()
        )
    )
    res1_meta: Sequential(
        0: Conv(1280, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
        1: relu()
    )
    res2_meta: Sequential(
        0: Conv(256, 256, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
        1: relu()
        2: Conv(256, 256, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
        3: relu()
    )
    cls_meta: Sequential(
        0: Conv(256, 256, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
        1: relu()
        2: Dropout2d(0.1, is_train=False)
        3: Conv(256, 2, (1, 1), (1, 1), (0, 0), (1, 1), 1, float32[2,], None, None, Kw=None, fan=None, i=None, bound=None)
    )
    relu: relu()
)
[2025-07-16 11:42:35,435 INFO test_sccan.py line 166 2193550] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
SubEpoch_val: True
ann_type: mask
arch: SCCAN
aux_weight1: 1.0
aux_weight2: 1.0
base_lr: 0.005
batch_size: 8
batch_size_val: 1
classes: 2
data_root: ../data/VOCdevkit2012/VOC2012
data_set: pascal
epochs: 200
evaluate: True
fix_bn: True
fix_random_seed_val: True
ignore_label: 255
index_split: -1
kshot_trans_dim: 2
layers: 50
low_fea: layer2
manual_seed: 321
merge: final
merge_tau: 0.9
momentum: 0.9
opts: None
ori_resize: True
padding_label: 255
power: 0.9
print_freq: 10
resized_val: True
resume: None
rotate_max: 10
rotate_min: -10
save_freq: 10
scale_max: 1.1
scale_min: 0.9
seed_deterministic: False
shot: 1
split: 0
start_epoch: 0
stop_interval: 75
train_h: 473
train_list: ./lists/pascal/voc_sbd_merge_noduplicate.txt
train_w: 473
use_split_coco: False
val_list: ./lists/pascal/val.txt
val_size: 473
vgg: False
viz: True
warmup: False
weight: torch_train_epoch_35_0.6672.pth
weight_decay: 0.0001
workers: 8
zoom_factor: 8
Number of Parameters: 37107626
Number of Learnable Parameters: 11373314
sub_list:  [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
sub_val_list:  [1, 2, 3, 4, 5]
Val: [1/1] 	 Seed: 321
[2025-07-16 11:42:39,159 INFO test_sccan.py line 251 2193550] Test: [10/1000] Data 0.001 (0.229) Batch 0.048 (0.372) Remain 00:06:08 Loss 0.0727 (0.2288) Accuracy 0.9850.
[2025-07-16 11:42:39,669 INFO test_sccan.py line 251 2193550] Test: [20/1000] Data 0.001 (0.115) Batch 0.050 (0.212) Remain 00:03:27 Loss 0.0873 (0.2748) Accuracy 0.9750.
[2025-07-16 11:42:40,184 INFO test_sccan.py line 251 2193550] Test: [30/1000] Data 0.001 (0.077) Batch 0.051 (0.158) Remain 00:02:33 Loss 0.0770 (0.2182) Accuracy 0.9782.
[2025-07-16 11:42:40,706 INFO test_sccan.py line 251 2193550] Test: [40/1000] Data 0.001 (0.058) Batch 0.049 (0.132) Remain 00:02:06 Loss 0.1386 (0.2294) Accuracy 0.9527.
[2025-07-16 11:42:41,214 INFO test_sccan.py line 251 2193550] Test: [50/1000] Data 0.001 (0.046) Batch 0.049 (0.116) Remain 00:01:49 Loss 0.0907 (0.2230) Accuracy 0.9714.
[2025-07-16 11:42:41,723 INFO test_sccan.py line 251 2193550] Test: [60/1000] Data 0.001 (0.039) Batch 0.051 (0.105) Remain 00:01:38 Loss 1.3358 (0.2859) Accuracy 0.7183.
[2025-07-16 11:42:42,231 INFO test_sccan.py line 251 2193550] Test: [70/1000] Data 0.001 (0.033) Batch 0.050 (0.097) Remain 00:01:30 Loss 0.2714 (0.2709) Accuracy 0.9293.
[2025-07-16 11:42:42,744 INFO test_sccan.py line 251 2193550] Test: [80/1000] Data 0.001 (0.029) Batch 0.047 (0.091) Remain 00:01:24 Loss 0.0751 (0.2519) Accuracy 0.9807.
[2025-07-16 11:42:43,245 INFO test_sccan.py line 251 2193550] Test: [90/1000] Data 0.001 (0.026) Batch 0.049 (0.087) Remain 00:01:18 Loss 0.1069 (0.2392) Accuracy 0.9395.
[2025-07-16 11:42:43,752 INFO test_sccan.py line 251 2193550] Test: [100/1000] Data 0.001 (0.024) Batch 0.051 (0.083) Remain 00:01:14 Loss 0.2574 (0.2477) Accuracy 0.9159.
[2025-07-16 11:42:44,255 INFO test_sccan.py line 251 2193550] Test: [110/1000] Data 0.001 (0.022) Batch 0.050 (0.080) Remain 00:01:11 Loss 0.4011 (0.2429) Accuracy 0.8996.
[2025-07-16 11:42:44,776 INFO test_sccan.py line 251 2193550] Test: [120/1000] Data 0.001 (0.020) Batch 0.051 (0.078) Remain 00:01:08 Loss 0.1113 (0.2348) Accuracy 0.9824.
[2025-07-16 11:42:45,277 INFO test_sccan.py line 251 2193550] Test: [130/1000] Data 0.001 (0.018) Batch 0.048 (0.076) Remain 00:01:05 Loss 0.0115 (0.2374) Accuracy 0.9977.
[2025-07-16 11:42:45,787 INFO test_sccan.py line 251 2193550] Test: [140/1000] Data 0.001 (0.017) Batch 0.050 (0.074) Remain 00:01:03 Loss 0.1163 (0.2434) Accuracy 0.9559.
[2025-07-16 11:42:46,292 INFO test_sccan.py line 251 2193550] Test: [150/1000] Data 0.001 (0.016) Batch 0.050 (0.072) Remain 00:01:01 Loss 0.2653 (0.2386) Accuracy 0.8777.
[2025-07-16 11:42:46,793 INFO test_sccan.py line 251 2193550] Test: [160/1000] Data 0.001 (0.015) Batch 0.048 (0.071) Remain 00:00:59 Loss 0.0244 (0.2279) Accuracy 0.9921.
[2025-07-16 11:42:47,283 INFO test_sccan.py line 251 2193550] Test: [170/1000] Data 0.001 (0.014) Batch 0.048 (0.070) Remain 00:00:57 Loss 0.0125 (0.2270) Accuracy 0.9970.
[2025-07-16 11:42:47,780 INFO test_sccan.py line 251 2193550] Test: [180/1000] Data 0.001 (0.014) Batch 0.051 (0.069) Remain 00:00:56 Loss 0.0403 (0.2512) Accuracy 0.9909.
[2025-07-16 11:42:48,276 INFO test_sccan.py line 251 2193550] Test: [190/1000] Data 0.001 (0.013) Batch 0.050 (0.068) Remain 00:00:54 Loss 0.7469 (0.2465) Accuracy 0.8033.
[2025-07-16 11:42:48,770 INFO test_sccan.py line 251 2193550] Test: [200/1000] Data 0.001 (0.012) Batch 0.049 (0.067) Remain 00:00:53 Loss 3.5388 (0.2631) Accuracy 0.2277.
[2025-07-16 11:42:49,259 INFO test_sccan.py line 251 2193550] Test: [210/1000] Data 0.001 (0.012) Batch 0.048 (0.066) Remain 00:00:51 Loss 0.0502 (0.2543) Accuracy 0.9916.
[2025-07-16 11:42:49,758 INFO test_sccan.py line 251 2193550] Test: [220/1000] Data 0.001 (0.011) Batch 0.050 (0.065) Remain 00:00:50 Loss 0.4102 (0.2506) Accuracy 0.8129.
[2025-07-16 11:42:50,256 INFO test_sccan.py line 251 2193550] Test: [230/1000] Data 0.001 (0.011) Batch 0.050 (0.064) Remain 00:00:49 Loss 0.1541 (0.2481) Accuracy 0.9538.
[2025-07-16 11:42:50,750 INFO test_sccan.py line 251 2193550] Test: [240/1000] Data 0.001 (0.010) Batch 0.048 (0.064) Remain 00:00:48 Loss 0.0353 (0.2418) Accuracy 0.9891.
[2025-07-16 11:42:51,243 INFO test_sccan.py line 251 2193550] Test: [250/1000] Data 0.001 (0.010) Batch 0.048 (0.063) Remain 00:00:47 Loss 0.0168 (0.2389) Accuracy 0.9962.
[2025-07-16 11:42:51,739 INFO test_sccan.py line 251 2193550] Test: [260/1000] Data 0.001 (0.010) Batch 0.050 (0.063) Remain 00:00:46 Loss 3.2259 (0.2458) Accuracy 0.6117.
[2025-07-16 11:42:52,238 INFO test_sccan.py line 251 2193550] Test: [270/1000] Data 0.001 (0.009) Batch 0.050 (0.062) Remain 00:00:45 Loss 0.1935 (0.2441) Accuracy 0.9645.
[2025-07-16 11:42:52,734 INFO test_sccan.py line 251 2193550] Test: [280/1000] Data 0.001 (0.009) Batch 0.049 (0.062) Remain 00:00:44 Loss 0.0817 (0.2419) Accuracy 0.9742.
[2025-07-16 11:42:53,225 INFO test_sccan.py line 251 2193550] Test: [290/1000] Data 0.001 (0.009) Batch 0.048 (0.061) Remain 00:00:43 Loss 0.0704 (0.2509) Accuracy 0.9696.
[2025-07-16 11:42:53,723 INFO test_sccan.py line 251 2193550] Test: [300/1000] Data 0.001 (0.008) Batch 0.050 (0.061) Remain 00:00:42 Loss 0.0232 (0.2488) Accuracy 0.9937.
[2025-07-16 11:42:54,219 INFO test_sccan.py line 251 2193550] Test: [310/1000] Data 0.001 (0.008) Batch 0.050 (0.061) Remain 00:00:41 Loss 0.0068 (0.2456) Accuracy 0.9978.
[2025-07-16 11:42:54,714 INFO test_sccan.py line 251 2193550] Test: [320/1000] Data 0.001 (0.008) Batch 0.048 (0.060) Remain 00:00:40 Loss 1.2248 (0.2496) Accuracy 0.6650.
[2025-07-16 11:42:55,205 INFO test_sccan.py line 251 2193550] Test: [330/1000] Data 0.001 (0.008) Batch 0.048 (0.060) Remain 00:00:40 Loss 0.4291 (0.2524) Accuracy 0.8106.
[2025-07-16 11:42:55,701 INFO test_sccan.py line 251 2193550] Test: [340/1000] Data 0.001 (0.008) Batch 0.051 (0.060) Remain 00:00:39 Loss 0.3441 (0.2475) Accuracy 0.8696.
[2025-07-16 11:42:56,196 INFO test_sccan.py line 251 2193550] Test: [350/1000] Data 0.001 (0.007) Batch 0.050 (0.059) Remain 00:00:38 Loss 0.1987 (0.2548) Accuracy 0.8890.
[2025-07-16 11:42:56,692 INFO test_sccan.py line 251 2193550] Test: [360/1000] Data 0.001 (0.007) Batch 0.049 (0.059) Remain 00:00:37 Loss 0.3052 (0.2530) Accuracy 0.8662.
[2025-07-16 11:42:57,386 INFO test_sccan.py line 251 2193550] Test: [374/1000] Data 0.001 (0.007) Batch 0.051 (0.059) Remain 00:00:36 Loss 0.0765 (0.2664) Accuracy 0.9841.
[2025-07-16 11:42:57,880 INFO test_sccan.py line 251 2193550] Test: [384/1000] Data 0.001 (0.007) Batch 0.049 (0.058) Remain 00:00:35 Loss 0.0967 (0.2695) Accuracy 0.9757.
[2025-07-16 11:42:58,369 INFO test_sccan.py line 251 2193550] Test: [394/1000] Data 0.001 (0.007) Batch 0.048 (0.058) Remain 00:00:35 Loss 0.1096 (0.2650) Accuracy 0.9677.
[2025-07-16 11:42:58,854 INFO test_sccan.py line 251 2193550] Test: [404/1000] Data 0.001 (0.007) Batch 0.047 (0.058) Remain 00:00:34 Loss 0.1093 (0.2690) Accuracy 0.9726.
[2025-07-16 11:42:59,338 INFO test_sccan.py line 251 2193550] Test: [414/1000] Data 0.001 (0.006) Batch 0.050 (0.058) Remain 00:00:33 Loss 0.1008 (0.2665) Accuracy 0.9822.
[2025-07-16 11:42:59,824 INFO test_sccan.py line 251 2193550] Test: [424/1000] Data 0.001 (0.006) Batch 0.049 (0.058) Remain 00:00:33 Loss 1.2690 (0.2733) Accuracy 0.7865.
[2025-07-16 11:43:00,313 INFO test_sccan.py line 251 2193550] Test: [434/1000] Data 0.001 (0.006) Batch 0.049 (0.057) Remain 00:00:32 Loss 0.0883 (0.2846) Accuracy 0.9624.
[2025-07-16 11:43:00,792 INFO test_sccan.py line 251 2193550] Test: [444/1000] Data 0.001 (0.006) Batch 0.047 (0.057) Remain 00:00:31 Loss 0.0661 (0.2808) Accuracy 0.9848.
[2025-07-16 11:43:01,276 INFO test_sccan.py line 251 2193550] Test: [454/1000] Data 0.001 (0.006) Batch 0.050 (0.057) Remain 00:00:31 Loss 0.1299 (0.2782) Accuracy 0.9528.
[2025-07-16 11:43:01,768 INFO test_sccan.py line 251 2193550] Test: [464/1000] Data 0.001 (0.006) Batch 0.049 (0.057) Remain 00:00:30 Loss 0.1459 (0.2785) Accuracy 0.9309.
[2025-07-16 11:43:02,256 INFO test_sccan.py line 251 2193550] Test: [474/1000] Data 0.001 (0.006) Batch 0.049 (0.057) Remain 00:00:29 Loss 0.4305 (0.2766) Accuracy 0.9110.
[2025-07-16 11:43:02,742 INFO test_sccan.py line 251 2193550] Test: [484/1000] Data 0.001 (0.006) Batch 0.048 (0.056) Remain 00:00:29 Loss 0.0685 (0.2918) Accuracy 0.9820.
[2025-07-16 11:43:03,233 INFO test_sccan.py line 251 2193550] Test: [494/1000] Data 0.001 (0.005) Batch 0.050 (0.056) Remain 00:00:28 Loss 4.3767 (0.3018) Accuracy 0.4456.
[2025-07-16 11:43:03,731 INFO test_sccan.py line 251 2193550] Test: [504/1000] Data 0.001 (0.005) Batch 0.049 (0.056) Remain 00:00:27 Loss 0.2334 (0.3032) Accuracy 0.9391.
[2025-07-16 11:43:04,224 INFO test_sccan.py line 251 2193550] Test: [514/1000] Data 0.001 (0.005) Batch 0.048 (0.056) Remain 00:00:27 Loss 0.3072 (0.3013) Accuracy 0.9080.
[2025-07-16 11:43:04,725 INFO test_sccan.py line 251 2193550] Test: [524/1000] Data 0.001 (0.005) Batch 0.050 (0.056) Remain 00:00:26 Loss 0.0634 (0.2973) Accuracy 0.9878.
[2025-07-16 11:43:05,220 INFO test_sccan.py line 251 2193550] Test: [534/1000] Data 0.001 (0.005) Batch 0.051 (0.056) Remain 00:00:25 Loss 0.0115 (0.2959) Accuracy 0.9971.
[2025-07-16 11:43:05,717 INFO test_sccan.py line 251 2193550] Test: [544/1000] Data 0.001 (0.005) Batch 0.050 (0.056) Remain 00:00:25 Loss 0.0365 (0.2946) Accuracy 0.9910.
[2025-07-16 11:43:06,218 INFO test_sccan.py line 251 2193550] Test: [554/1000] Data 0.001 (0.005) Batch 0.048 (0.056) Remain 00:00:24 Loss 0.8254 (0.2965) Accuracy 0.8065.
[2025-07-16 11:43:06,717 INFO test_sccan.py line 251 2193550] Test: [564/1000] Data 0.001 (0.005) Batch 0.051 (0.055) Remain 00:00:24 Loss 7.0977 (0.3091) Accuracy 0.1390.
[2025-07-16 11:43:07,214 INFO test_sccan.py line 251 2193550] Test: [574/1000] Data 0.001 (0.005) Batch 0.051 (0.055) Remain 00:00:23 Loss 0.1754 (0.3046) Accuracy 0.9314.
[2025-07-16 11:43:07,711 INFO test_sccan.py line 251 2193550] Test: [584/1000] Data 0.001 (0.005) Batch 0.049 (0.055) Remain 00:00:22 Loss 0.3673 (0.3033) Accuracy 0.8234.
[2025-07-16 11:43:08,207 INFO test_sccan.py line 251 2193550] Test: [594/1000] Data 0.001 (0.005) Batch 0.048 (0.055) Remain 00:00:22 Loss 0.1471 (0.3029) Accuracy 0.9238.
[2025-07-16 11:43:08,704 INFO test_sccan.py line 251 2193550] Test: [604/1000] Data 0.001 (0.005) Batch 0.050 (0.055) Remain 00:00:21 Loss 0.0366 (0.3030) Accuracy 0.9898.
[2025-07-16 11:43:09,198 INFO test_sccan.py line 251 2193550] Test: [614/1000] Data 0.001 (0.005) Batch 0.049 (0.055) Remain 00:00:21 Loss 0.0141 (0.3002) Accuracy 0.9963.
[2025-07-16 11:43:09,693 INFO test_sccan.py line 251 2193550] Test: [624/1000] Data 0.001 (0.005) Batch 0.049 (0.055) Remain 00:00:20 Loss 3.7024 (0.3032) Accuracy 0.6199.
[2025-07-16 11:43:10,186 INFO test_sccan.py line 251 2193550] Test: [634/1000] Data 0.001 (0.004) Batch 0.049 (0.055) Remain 00:00:20 Loss 0.2277 (0.3028) Accuracy 0.9616.
[2025-07-16 11:43:10,685 INFO test_sccan.py line 251 2193550] Test: [644/1000] Data 0.001 (0.004) Batch 0.050 (0.055) Remain 00:00:19 Loss 0.1795 (0.3003) Accuracy 0.9184.
[2025-07-16 11:43:11,183 INFO test_sccan.py line 251 2193550] Test: [654/1000] Data 0.001 (0.004) Batch 0.051 (0.055) Remain 00:00:18 Loss 0.0632 (0.3000) Accuracy 0.9731.
[2025-07-16 11:43:11,681 INFO test_sccan.py line 251 2193550] Test: [664/1000] Data 0.001 (0.004) Batch 0.049 (0.055) Remain 00:00:18 Loss 0.0221 (0.2986) Accuracy 0.9941.
[2025-07-16 11:43:12,172 INFO test_sccan.py line 251 2193550] Test: [674/1000] Data 0.001 (0.004) Batch 0.049 (0.054) Remain 00:00:17 Loss 0.0063 (0.2998) Accuracy 0.9980.
[2025-07-16 11:43:12,670 INFO test_sccan.py line 251 2193550] Test: [684/1000] Data 0.001 (0.004) Batch 0.050 (0.054) Remain 00:00:17 Loss 1.2663 (0.3019) Accuracy 0.7885.
[2025-07-16 11:43:13,167 INFO test_sccan.py line 251 2193550] Test: [694/1000] Data 0.001 (0.004) Batch 0.050 (0.054) Remain 00:00:16 Loss 0.2240 (0.3011) Accuracy 0.9458.
[2025-07-16 11:43:13,666 INFO test_sccan.py line 251 2193550] Test: [704/1000] Data 0.001 (0.004) Batch 0.049 (0.054) Remain 00:00:16 Loss 0.1724 (0.2975) Accuracy 0.9412.
[2025-07-16 11:43:14,159 INFO test_sccan.py line 251 2193550] Test: [714/1000] Data 0.001 (0.004) Batch 0.048 (0.054) Remain 00:00:15 Loss 0.2305 (0.2977) Accuracy 0.9096.
[2025-07-16 11:43:14,655 INFO test_sccan.py line 251 2193550] Test: [724/1000] Data 0.001 (0.004) Batch 0.050 (0.054) Remain 00:00:14 Loss 0.1465 (0.2954) Accuracy 0.9308.
[2025-07-16 11:43:15,347 INFO test_sccan.py line 251 2193550] Test: [738/1000] Data 0.001 (0.004) Batch 0.048 (0.054) Remain 00:00:14 Loss 0.0816 (0.3045) Accuracy 0.9841.
[2025-07-16 11:43:15,844 INFO test_sccan.py line 251 2193550] Test: [748/1000] Data 0.001 (0.004) Batch 0.051 (0.054) Remain 00:00:13 Loss 0.0713 (0.3022) Accuracy 0.9753.
[2025-07-16 11:43:16,343 INFO test_sccan.py line 251 2193550] Test: [758/1000] Data 0.001 (0.004) Batch 0.051 (0.054) Remain 00:00:13 Loss 0.0776 (0.2996) Accuracy 0.9791.
[2025-07-16 11:43:16,840 INFO test_sccan.py line 251 2193550] Test: [768/1000] Data 0.001 (0.004) Batch 0.049 (0.054) Remain 00:00:12 Loss 0.1412 (0.2992) Accuracy 0.9690.
[2025-07-16 11:43:17,332 INFO test_sccan.py line 251 2193550] Test: [778/1000] Data 0.001 (0.004) Batch 0.049 (0.054) Remain 00:00:11 Loss 0.1017 (0.2981) Accuracy 0.9822.
[2025-07-16 11:43:17,834 INFO test_sccan.py line 251 2193550] Test: [788/1000] Data 0.001 (0.004) Batch 0.053 (0.054) Remain 00:00:11 Loss 1.3275 (0.3018) Accuracy 0.8041.
[2025-07-16 11:43:18,331 INFO test_sccan.py line 251 2193550] Test: [798/1000] Data 0.001 (0.004) Batch 0.050 (0.054) Remain 00:00:10 Loss 0.0901 (0.3050) Accuracy 0.9667.
[2025-07-16 11:43:18,840 INFO test_sccan.py line 251 2193550] Test: [808/1000] Data 0.001 (0.004) Batch 0.048 (0.054) Remain 00:00:10 Loss 0.0569 (0.3027) Accuracy 0.9846.
[2025-07-16 11:43:19,340 INFO test_sccan.py line 251 2193550] Test: [818/1000] Data 0.001 (0.004) Batch 0.048 (0.054) Remain 00:00:09 Loss 0.1299 (0.3007) Accuracy 0.9528.
[2025-07-16 11:43:19,848 INFO test_sccan.py line 251 2193550] Test: [828/1000] Data 0.001 (0.004) Batch 0.051 (0.054) Remain 00:00:09 Loss 0.1025 (0.3005) Accuracy 0.9503.
[2025-07-16 11:43:20,355 INFO test_sccan.py line 251 2193550] Test: [838/1000] Data 0.001 (0.004) Batch 0.050 (0.054) Remain 00:00:08 Loss 0.3524 (0.2990) Accuracy 0.8876.
[2025-07-16 11:43:20,870 INFO test_sccan.py line 251 2193550] Test: [848/1000] Data 0.001 (0.004) Batch 0.047 (0.054) Remain 00:00:08 Loss 0.1566 (0.2971) Accuracy 0.9687.
[2025-07-16 11:43:21,373 INFO test_sccan.py line 251 2193550] Test: [858/1000] Data 0.001 (0.004) Batch 0.049 (0.054) Remain 00:00:07 Loss 0.0119 (0.2974) Accuracy 0.9977.
[2025-07-16 11:43:21,879 INFO test_sccan.py line 251 2193550] Test: [868/1000] Data 0.001 (0.003) Batch 0.050 (0.054) Remain 00:00:07 Loss 0.0886 (0.2976) Accuracy 0.9712.
[2025-07-16 11:43:22,383 INFO test_sccan.py line 251 2193550] Test: [878/1000] Data 0.001 (0.003) Batch 0.050 (0.053) Remain 00:00:06 Loss 0.3147 (0.2964) Accuracy 0.9081.
[2025-07-16 11:43:22,899 INFO test_sccan.py line 251 2193550] Test: [888/1000] Data 0.001 (0.003) Batch 0.049 (0.053) Remain 00:00:05 Loss 0.0497 (0.2937) Accuracy 0.9898.
[2025-07-16 11:43:23,401 INFO test_sccan.py line 251 2193550] Test: [898/1000] Data 0.001 (0.003) Batch 0.049 (0.053) Remain 00:00:05 Loss 0.0124 (0.2933) Accuracy 0.9971.
[2025-07-16 11:43:23,898 INFO test_sccan.py line 251 2193550] Test: [908/1000] Data 0.001 (0.003) Batch 0.051 (0.053) Remain 00:00:04 Loss 0.0396 (0.2934) Accuracy 0.9910.
[2025-07-16 11:43:24,396 INFO test_sccan.py line 251 2193550] Test: [918/1000] Data 0.001 (0.003) Batch 0.050 (0.053) Remain 00:00:04 Loss 0.7112 (0.2927) Accuracy 0.8078.
[2025-07-16 11:43:24,893 INFO test_sccan.py line 251 2193550] Test: [928/1000] Data 0.001 (0.003) Batch 0.049 (0.053) Remain 00:00:03 Loss 5.9981 (0.2977) Accuracy 0.1525.
[2025-07-16 11:43:25,385 INFO test_sccan.py line 251 2193550] Test: [938/1000] Data 0.001 (0.003) Batch 0.049 (0.053) Remain 00:00:03 Loss 0.2128 (0.2952) Accuracy 0.9172.
[2025-07-16 11:43:25,885 INFO test_sccan.py line 251 2193550] Test: [948/1000] Data 0.001 (0.003) Batch 0.051 (0.053) Remain 00:00:02 Loss 0.2818 (0.2943) Accuracy 0.8379.
[2025-07-16 11:43:26,382 INFO test_sccan.py line 251 2193550] Test: [958/1000] Data 0.001 (0.003) Batch 0.050 (0.053) Remain 00:00:02 Loss 0.2254 (0.2933) Accuracy 0.9230.
[2025-07-16 11:43:26,877 INFO test_sccan.py line 251 2193550] Test: [968/1000] Data 0.001 (0.003) Batch 0.048 (0.053) Remain 00:00:01 Loss 0.0368 (0.2910) Accuracy 0.9894.
[2025-07-16 11:43:27,371 INFO test_sccan.py line 251 2193550] Test: [978/1000] Data 0.001 (0.003) Batch 0.049 (0.053) Remain 00:00:01 Loss 0.0140 (0.2893) Accuracy 0.9964.
[2025-07-16 11:43:27,869 INFO test_sccan.py line 251 2193550] Test: [988/1000] Data 0.001 (0.003) Batch 0.051 (0.053) Remain 00:00:00 Loss 3.1888 (0.2906) Accuracy 0.6139.
[2025-07-16 11:43:28,368 INFO test_sccan.py line 251 2193550] Test: [998/1000] Data 0.001 (0.003) Batch 0.051 (0.053) Remain 00:00:00 Loss 0.2280 (0.2897) Accuracy 0.9631.
[2025-07-16 11:43:28,469 INFO test_sccan.py line 265 2193550] meanIoU---Val result: mIoU 0.6541.
[2025-07-16 11:43:28,469 INFO test_sccan.py line 266 2193550] <<<<<<< Novel Results <<<<<<<
[2025-07-16 11:43:28,469 INFO test_sccan.py line 268 2193550] Class_1 Result: iou 0.8415.
[2025-07-16 11:43:28,469 INFO test_sccan.py line 268 2193550] Class_2 Result: iou 0.3605.
[2025-07-16 11:43:28,469 INFO test_sccan.py line 268 2193550] Class_3 Result: iou 0.8447.
[2025-07-16 11:43:28,469 INFO test_sccan.py line 268 2193550] Class_4 Result: iou 0.7211.
[2025-07-16 11:43:28,469 INFO test_sccan.py line 268 2193550] Class_5 Result: iou 0.5029.
[2025-07-16 11:43:28,469 INFO test_sccan.py line 270 2193550] FBIoU---Val result: FBIoU 0.7855.
[2025-07-16 11:43:28,469 INFO test_sccan.py line 272 2193550] Class_0 Result: iou_f 0.9280.
[2025-07-16 11:43:28,469 INFO test_sccan.py line 272 2193550] Class_1 Result: iou_f 0.6430.
[2025-07-16 11:43:28,469 INFO test_sccan.py line 273 2193550] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
total time: 53.0304, avg inference time: 0.0113, count: 1000

Total running time: 00h 00m 53s
Seed0: 123
Seed:  [321]
mIoU:  [0.6541]
FBIoU: [0.7855]
pIoU:  [0.643]
-------------------------------------------
Best_Seed_m: 321 	 Best_Seed_F: 321 	 Best_Seed_p: 321
Best_mIoU: 0.6541 	 Best_FBIoU: 0.7855 	 Best_pIoU: 0.6430
Mean_mIoU: 0.6541 	 Mean_FBIoU: 0.7855 	 Mean_pIoU: 0.6430
