[i 0718 09:46:06.105248 08 compiler.py:956] Jittor(1.3.9.14) src: /data1/fanlyu/anaconda3/envs/jittor/lib/python3.7/site-packages/jittor
[i 0718 09:46:06.114745 08 compiler.py:957] g++ at /usr/bin/g++(11.4.0)
[i 0718 09:46:06.114796 08 compiler.py:958] cache_path: /data1/fanlyu/.cache/jittor/jt1.3.9/g++11.4.0/py3.7.16/Linux-5.15.0-1xe3/IntelRXeonRGolx62/efa5/default
[i 0718 09:46:06.189040 08 install_cuda.py:96] cuda_driver_version: [12, 4]
[i 0718 09:46:06.189662 08 install_cuda.py:84] restart /data1/fanlyu/anaconda3/envs/jittor/bin/python ['test_sccan.py', '--config=config/pascal/pascal_split1_resnet50.yaml', '--viz']
[i 0718 09:46:07.130579 64 compiler.py:956] Jittor(1.3.9.14) src: /data1/fanlyu/anaconda3/envs/jittor/lib/python3.7/site-packages/jittor
[i 0718 09:46:07.140387 64 compiler.py:957] g++ at /usr/bin/g++(11.4.0)
[i 0718 09:46:07.140447 64 compiler.py:958] cache_path: /data1/fanlyu/.cache/jittor/jt1.3.9/g++11.4.0/py3.7.16/Linux-5.15.0-1xe3/IntelRXeonRGolx62/efa5/default
[i 0718 09:46:07.193335 64 install_cuda.py:96] cuda_driver_version: [12, 4]
[i 0718 09:46:07.201654 64 __init__.py:412] Found /data1/fanlyu/.cache/jittor/jtcuda/cuda12.2_cudnn8_linux/bin/nvcc(12.2.140) at /data1/fanlyu/.cache/jittor/jtcuda/cuda12.2_cudnn8_linux/bin/nvcc.
[i 0718 09:46:07.209104 64 __init__.py:412] Found addr2line(2.38) at /usr/bin/addr2line.
[i 0718 09:46:07.351473 64 compiler.py:1013] cuda key:cu12.2.140_sm_89
[i 0718 09:46:07.702634 64 __init__.py:227] Total mem: 503.54GB, using 16 procs for compiling.
[i 0718 09:46:07.792159 64 jit_compiler.cc:28] Load cc_path: /usr/bin/g++
[i 0718 09:46:07.907772 64 init.cc:63] Found cuda archs: [89,]
[2025-07-18 09:46:09,066 INFO test_sccan.py line 101 3326053] => creating model ...
[i 0718 09:46:09.307537 64 cuda_flags.cc:49] CUDA enabled.
[2025-07-18 09:46:09,406 INFO test_sccan.py line 65 3326053] => loading checkpoint 'exp/pascal/SCCAN/split1_1shot/resnet50/snapshot/train_epoch_125_0.7319.pth'
[2025-07-18 09:46:09,548 INFO test_sccan.py line 75 3326053] => loaded checkpoint 'exp/pascal/SCCAN/split1_1shot/resnet50/snapshot/train_epoch_125_0.7319.pth' (epoch 125)
[2025-07-18 09:46:14,557 INFO test_sccan.py line 103 3326053] OneModel(
    criterion: CrossEntropyLoss(None, ignore_index=255)
    criterion_dice: WeightedDiceLoss(1.0, reduction=sum)
    backbone: Backbone(
        backbone: ResNet(
            conv1: Conv(3, 64, (3, 3), (2, 2), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
            bn1: FrozenBatchNorm2d(None)
            relu1: relu()
            conv2: Conv(64, 64, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
            bn2: FrozenBatchNorm2d(None)
            relu2: relu()
            conv3: Conv(64, 128, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
            bn3: FrozenBatchNorm2d(None)
            relu3: relu()
            maxpool: Pool((3, 3), (2, 2), (1, 1), dilation=None, return_indices=None, ceil_mode=False, count_include_pad=True, op=maximum, item=None)
            layer1: Sequential(
                0: Bottleneck(
                    conv1: Conv(128, 64, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(64, 64, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(64, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                    downsample: Sequential(
                        0: Conv(128, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                        1: FrozenBatchNorm2d(None)
                    )
                )
                1: Bottleneck(
                    conv1: Conv(256, 64, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(64, 64, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(64, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                2: Bottleneck(
                    conv1: Conv(256, 64, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(64, 64, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(64, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
            )
            layer2: Sequential(
                0: Bottleneck(
                    conv1: Conv(256, 128, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(128, 128, (3, 3), (2, 2), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(128, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                    downsample: Sequential(
                        0: Conv(256, 512, (1, 1), (2, 2), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                        1: FrozenBatchNorm2d(None)
                    )
                )
                1: Bottleneck(
                    conv1: Conv(512, 128, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(128, 128, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(128, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                2: Bottleneck(
                    conv1: Conv(512, 128, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(128, 128, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(128, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                3: Bottleneck(
                    conv1: Conv(512, 128, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(128, 128, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(128, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
            )
            layer3: Sequential(
                0: Bottleneck(
                    conv1: Conv(512, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(256, 256, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(256, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                    downsample: Sequential(
                        0: Conv(512, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                        1: FrozenBatchNorm2d(None)
                    )
                )
                1: Bottleneck(
                    conv1: Conv(1024, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(256, 256, (3, 3), (1, 1), (2, 2), (2, 2), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(256, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                2: Bottleneck(
                    conv1: Conv(1024, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(256, 256, (3, 3), (1, 1), (2, 2), (2, 2), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(256, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                3: Bottleneck(
                    conv1: Conv(1024, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(256, 256, (3, 3), (1, 1), (2, 2), (2, 2), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(256, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                4: Bottleneck(
                    conv1: Conv(1024, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(256, 256, (3, 3), (1, 1), (2, 2), (2, 2), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(256, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                5: Bottleneck(
                    conv1: Conv(1024, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(256, 256, (3, 3), (1, 1), (2, 2), (2, 2), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(256, 1024, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
            )
            layer4: Sequential(
                0: Bottleneck(
                    conv1: Conv(1024, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(512, 512, (3, 3), (1, 1), (2, 2), (2, 2), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(512, 2048, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                    downsample: Sequential(
                        0: Conv(1024, 2048, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                        1: FrozenBatchNorm2d(None)
                    )
                )
                1: Bottleneck(
                    conv1: Conv(2048, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(512, 512, (3, 3), (1, 1), (4, 4), (4, 4), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(512, 2048, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
                2: Bottleneck(
                    conv1: Conv(2048, 512, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn1: FrozenBatchNorm2d(None)
                    conv2: Conv(512, 512, (3, 3), (1, 1), (4, 4), (4, 4), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn2: FrozenBatchNorm2d(None)
                    conv3: Conv(512, 2048, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
                    bn3: FrozenBatchNorm2d(None)
                    relu: relu()
                )
            )
            avgpool: AdaptiveAvgPool2d((1, 1))
            fc: Linear(2048, 1000, float32[1000,], None)
        )
    )
    down_query: Sequential(
        0: Conv(1536, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
        1: relu()
        2: Dropout2d(0.5, is_train=False)
    )
    down_supp: Sequential(
        0: Conv(1536, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
        1: relu()
        2: Dropout2d(0.5, is_train=False)
    )
    init_merge_query: Sequential(
        0: Conv(513, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
    )
    init_merge_supp: Sequential(
        0: Conv(513, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
    )
    transformer: SwinTransformer(
        pos_drop: Dropout(0.0, is_train=False)
        layers: Sequential(
            0: BasicLayer(
                blocks: Sequential(
                    0: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    1: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    2: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    3: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    4: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    5: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    6: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                    7: SwinTransformerBlock(
                        norm1: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        attn_q: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        attn_s: CrossWindowAttention(
                            q: Linear(256, 256, float32[256,], None)
                            kv: Linear(256, 512, float32[512,], None)
                            attn_drop: Dropout(0.0, is_train=False)
                            proj: Linear(256, 256, float32[256,], None)
                            proj_drop: Dropout(0.0, is_train=False)
                            softmax: softmax()
                        )
                        drop_path: Identity(None, None)
                        norm2: LayerNorm((256,), 1e-05, elementwise_affine=True)
                        mlp_q: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                        mlp_s: Mlp(
                            fc1: Linear(256, 256, float32[256,], None)
                            act: gelu()
                            fc2: Linear(256, 256, float32[256,], None)
                            drop: Dropout(0.0, is_train=False)
                        )
                    )
                )
            )
        )
        norm0: LayerNorm((256,), 1e-05, elementwise_affine=True)
    )
    ASPP_meta: ASPP(
        layer6_0: Sequential(
            0: Conv(256, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, float32[256,], None, None, Kw=None, fan=None, i=None, bound=None)
            1: relu()
        )
        layer6_1: Sequential(
            0: Conv(256, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, float32[256,], None, None, Kw=None, fan=None, i=None, bound=None)
            1: relu()
        )
        layer6_2: Sequential(
            0: Conv(256, 256, (3, 3), (1, 1), (6, 6), (6, 6), 1, float32[256,], None, None, Kw=None, fan=None, i=None, bound=None)
            1: relu()
        )
        layer6_3: Sequential(
            0: Conv(256, 256, (3, 3), (1, 1), (12, 12), (12, 12), 1, float32[256,], None, None, Kw=None, fan=None, i=None, bound=None)
            1: relu()
        )
        layer6_4: Sequential(
            0: Conv(256, 256, (3, 3), (1, 1), (18, 18), (18, 18), 1, float32[256,], None, None, Kw=None, fan=None, i=None, bound=None)
            1: relu()
        )
    )
    res1_meta: Sequential(
        0: Conv(1280, 256, (1, 1), (1, 1), (0, 0), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
        1: relu()
    )
    res2_meta: Sequential(
        0: Conv(256, 256, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
        1: relu()
        2: Conv(256, 256, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
        3: relu()
    )
    cls_meta: Sequential(
        0: Conv(256, 256, (3, 3), (1, 1), (1, 1), (1, 1), 1, None, None, None, Kw=None, fan=None, i=None, bound=None)
        1: relu()
        2: Dropout2d(0.1, is_train=False)
        3: Conv(256, 2, (1, 1), (1, 1), (0, 0), (1, 1), 1, float32[2,], None, None, Kw=None, fan=None, i=None, bound=None)
    )
    relu: relu()
)
[2025-07-18 09:46:14,561 INFO test_sccan.py line 165 3326053] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
SubEpoch_val: True
ann_type: mask
arch: SCCAN
aux_weight1: 1.0
aux_weight2: 1.0
base_lr: 0.005
batch_size: 8
batch_size_val: 1
classes: 2
data_root: ../data/VOCdevkit2012/VOC2012
data_set: pascal
epochs: 200
evaluate: True
fix_bn: True
fix_random_seed_val: True
ignore_label: 255
index_split: -1
kshot_trans_dim: 2
layers: 50
low_fea: layer2
manual_seed: 321
merge: final
merge_tau: 0.9
momentum: 0.9
opts: None
ori_resize: True
padding_label: 255
power: 0.9
print_freq: 10
resized_val: True
resume: None
rotate_max: 10
rotate_min: -10
save_freq: 10
scale_max: 1.1
scale_min: 0.9
seed_deterministic: False
shot: 1
split: 1
start_epoch: 0
stop_interval: 75
train_h: 473
train_list: ./lists/pascal/voc_sbd_merge_noduplicate.txt
train_w: 473
use_split_coco: False
val_list: ./lists/pascal/val.txt
val_size: 473
vgg: False
viz: True
warmup: False
weight: train_epoch_125_0.7319.pth
weight_decay: 0.0001
workers: 8
zoom_factor: 8
Number of Parameters: 37107626
Number of Learnable Parameters: 11373314
sub_list:  [1, 2, 3, 4, 5, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
sub_val_list:  [6, 7, 8, 9, 10]
Val: [1/1] 	 Seed: 321
[2025-07-18 09:46:18,355 INFO test_sccan.py line 250 3326053] Test: [10/1000] Data 0.001 (0.229) Batch 0.050 (0.379) Remain 00:06:15 Loss 0.1840 (1.3540) Accuracy 0.9262.
[2025-07-18 09:46:18,882 INFO test_sccan.py line 250 3326053] Test: [20/1000] Data 0.001 (0.115) Batch 0.062 (0.216) Remain 00:03:31 Loss 0.1647 (0.9788) Accuracy 0.9374.
[2025-07-18 09:46:19,390 INFO test_sccan.py line 250 3326053] Test: [30/1000] Data 0.001 (0.077) Batch 0.047 (0.161) Remain 00:02:36 Loss 0.0195 (0.9537) Accuracy 0.9930.
[2025-07-18 09:46:19,898 INFO test_sccan.py line 250 3326053] Test: [40/1000] Data 0.001 (0.058) Batch 0.051 (0.133) Remain 00:02:08 Loss 0.6024 (0.9100) Accuracy 0.9342.
[2025-07-18 09:46:20,393 INFO test_sccan.py line 250 3326053] Test: [50/1000] Data 0.001 (0.047) Batch 0.050 (0.117) Remain 00:01:50 Loss 0.0096 (0.9044) Accuracy 0.9958.
[2025-07-18 09:46:20,886 INFO test_sccan.py line 250 3326053] Test: [60/1000] Data 0.001 (0.039) Batch 0.051 (0.105) Remain 00:01:39 Loss 0.0242 (0.8907) Accuracy 0.9895.
[2025-07-18 09:46:21,389 INFO test_sccan.py line 250 3326053] Test: [70/1000] Data 0.001 (0.033) Batch 0.051 (0.097) Remain 00:01:30 Loss 0.0740 (0.8366) Accuracy 0.9717.
[2025-07-18 09:46:21,913 INFO test_sccan.py line 250 3326053] Test: [80/1000] Data 0.001 (0.029) Batch 0.053 (0.092) Remain 00:01:24 Loss 0.5334 (0.8049) Accuracy 0.7758.
[2025-07-18 09:46:22,451 INFO test_sccan.py line 250 3326053] Test: [90/1000] Data 0.001 (0.026) Batch 0.053 (0.088) Remain 00:01:19 Loss 1.3704 (0.7761) Accuracy 0.6470.
[2025-07-18 09:46:22,952 INFO test_sccan.py line 250 3326053] Test: [100/1000] Data 0.001 (0.024) Batch 0.049 (0.084) Remain 00:01:15 Loss 0.1925 (0.7115) Accuracy 0.9046.
[2025-07-18 09:46:23,433 INFO test_sccan.py line 250 3326053] Test: [110/1000] Data 0.001 (0.022) Batch 0.047 (0.081) Remain 00:01:11 Loss 0.0504 (0.6624) Accuracy 0.9740.
[2025-07-18 09:46:23,916 INFO test_sccan.py line 250 3326053] Test: [120/1000] Data 0.001 (0.020) Batch 0.049 (0.078) Remain 00:01:08 Loss 1.5916 (0.6331) Accuracy 0.7782.
[2025-07-18 09:46:24,401 INFO test_sccan.py line 250 3326053] Test: [130/1000] Data 0.001 (0.018) Batch 0.049 (0.076) Remain 00:01:05 Loss 0.0379 (0.5992) Accuracy 0.9820.
[2025-07-18 09:46:24,882 INFO test_sccan.py line 250 3326053] Test: [140/1000] Data 0.001 (0.017) Batch 0.048 (0.074) Remain 00:01:03 Loss 0.0113 (0.5747) Accuracy 0.9962.
[2025-07-18 09:46:25,366 INFO test_sccan.py line 250 3326053] Test: [150/1000] Data 0.001 (0.016) Batch 0.048 (0.072) Remain 00:01:01 Loss 0.0087 (0.5494) Accuracy 0.9968.
[2025-07-18 09:46:25,852 INFO test_sccan.py line 250 3326053] Test: [160/1000] Data 0.001 (0.015) Batch 0.051 (0.071) Remain 00:00:59 Loss 0.7208 (0.5324) Accuracy 0.8597.
[2025-07-18 09:46:26,338 INFO test_sccan.py line 250 3326053] Test: [170/1000] Data 0.001 (0.014) Batch 0.050 (0.069) Remain 00:00:57 Loss 0.2059 (0.5197) Accuracy 0.9266.
[2025-07-18 09:46:26,827 INFO test_sccan.py line 250 3326053] Test: [180/1000] Data 0.001 (0.014) Batch 0.049 (0.068) Remain 00:00:55 Loss 0.0139 (0.5227) Accuracy 0.9967.
[2025-07-18 09:46:27,314 INFO test_sccan.py line 250 3326053] Test: [190/1000] Data 0.001 (0.013) Batch 0.049 (0.067) Remain 00:00:54 Loss 0.8051 (0.5122) Accuracy 0.8336.
[2025-07-18 09:46:27,803 INFO test_sccan.py line 250 3326053] Test: [200/1000] Data 0.001 (0.012) Batch 0.049 (0.066) Remain 00:00:52 Loss 0.0892 (0.5132) Accuracy 0.9858.
[2025-07-18 09:46:28,297 INFO test_sccan.py line 250 3326053] Test: [210/1000] Data 0.001 (0.012) Batch 0.049 (0.065) Remain 00:00:51 Loss 0.0323 (0.5458) Accuracy 0.9891.
[2025-07-18 09:46:28,785 INFO test_sccan.py line 250 3326053] Test: [220/1000] Data 0.001 (0.011) Batch 0.047 (0.065) Remain 00:00:50 Loss 0.1287 (0.5441) Accuracy 0.9745.
[2025-07-18 09:46:29,274 INFO test_sccan.py line 250 3326053] Test: [230/1000] Data 0.001 (0.011) Batch 0.050 (0.064) Remain 00:00:49 Loss 0.2383 (0.5409) Accuracy 0.9357.
[2025-07-18 09:46:29,766 INFO test_sccan.py line 250 3326053] Test: [240/1000] Data 0.001 (0.010) Batch 0.050 (0.063) Remain 00:00:48 Loss 2.2973 (0.5442) Accuracy 0.6124.
[2025-07-18 09:46:30,259 INFO test_sccan.py line 250 3326053] Test: [250/1000] Data 0.001 (0.010) Batch 0.048 (0.063) Remain 00:00:47 Loss 0.2231 (0.5325) Accuracy 0.8917.
[2025-07-18 09:46:30,753 INFO test_sccan.py line 250 3326053] Test: [260/1000] Data 0.001 (0.010) Batch 0.050 (0.062) Remain 00:00:46 Loss 0.0074 (0.5292) Accuracy 0.9974.
[2025-07-18 09:46:31,248 INFO test_sccan.py line 250 3326053] Test: [270/1000] Data 0.001 (0.009) Batch 0.049 (0.062) Remain 00:00:45 Loss 0.1343 (0.5187) Accuracy 0.9893.
[2025-07-18 09:46:31,740 INFO test_sccan.py line 250 3326053] Test: [280/1000] Data 0.001 (0.009) Batch 0.050 (0.061) Remain 00:00:44 Loss 0.0071 (0.5209) Accuracy 0.9981.
[2025-07-18 09:46:32,230 INFO test_sccan.py line 250 3326053] Test: [290/1000] Data 0.001 (0.009) Batch 0.048 (0.061) Remain 00:00:43 Loss 0.0403 (0.5451) Accuracy 0.9841.
[2025-07-18 09:46:32,721 INFO test_sccan.py line 250 3326053] Test: [300/1000] Data 0.001 (0.008) Batch 0.049 (0.061) Remain 00:00:42 Loss 0.0184 (0.5354) Accuracy 0.9950.
[2025-07-18 09:46:33,211 INFO test_sccan.py line 250 3326053] Test: [310/1000] Data 0.001 (0.008) Batch 0.049 (0.060) Remain 00:00:41 Loss 0.1324 (0.5208) Accuracy 0.9580.
[2025-07-18 09:46:33,703 INFO test_sccan.py line 250 3326053] Test: [320/1000] Data 0.001 (0.008) Batch 0.050 (0.060) Remain 00:00:40 Loss 0.0031 (0.5183) Accuracy 0.9990.
[2025-07-18 09:46:34,192 INFO test_sccan.py line 250 3326053] Test: [330/1000] Data 0.001 (0.008) Batch 0.048 (0.059) Remain 00:00:39 Loss 0.6935 (0.5333) Accuracy 0.8781.
[2025-07-18 09:46:34,683 INFO test_sccan.py line 250 3326053] Test: [340/1000] Data 0.001 (0.008) Batch 0.050 (0.059) Remain 00:00:39 Loss 0.0783 (0.5376) Accuracy 0.9752.
[2025-07-18 09:46:35,172 INFO test_sccan.py line 250 3326053] Test: [350/1000] Data 0.001 (0.007) Batch 0.048 (0.059) Remain 00:00:38 Loss 0.5136 (0.5288) Accuracy 0.9006.
[2025-07-18 09:46:35,665 INFO test_sccan.py line 250 3326053] Test: [360/1000] Data 0.001 (0.007) Batch 0.050 (0.059) Remain 00:00:37 Loss 0.0080 (0.5246) Accuracy 0.9974.
[2025-07-18 09:46:36,154 INFO test_sccan.py line 250 3326053] Test: [370/1000] Data 0.001 (0.007) Batch 0.048 (0.058) Remain 00:00:36 Loss 0.0455 (0.5240) Accuracy 0.9891.
[2025-07-18 09:46:36,645 INFO test_sccan.py line 250 3326053] Test: [380/1000] Data 0.001 (0.007) Batch 0.049 (0.058) Remain 00:00:36 Loss 3.4914 (0.5265) Accuracy 0.5732.
[2025-07-18 09:46:37,135 INFO test_sccan.py line 250 3326053] Test: [390/1000] Data 0.001 (0.007) Batch 0.049 (0.058) Remain 00:00:35 Loss 0.4079 (0.5309) Accuracy 0.8865.
[2025-07-18 09:46:37,627 INFO test_sccan.py line 250 3326053] Test: [400/1000] Data 0.001 (0.007) Batch 0.050 (0.058) Remain 00:00:34 Loss 0.2676 (0.5215) Accuracy 0.9662.
[2025-07-18 09:46:38,119 INFO test_sccan.py line 250 3326053] Test: [410/1000] Data 0.001 (0.006) Batch 0.048 (0.057) Remain 00:00:33 Loss 0.0221 (0.5170) Accuracy 0.9932.
[2025-07-18 09:46:38,613 INFO test_sccan.py line 250 3326053] Test: [420/1000] Data 0.001 (0.006) Batch 0.050 (0.057) Remain 00:00:33 Loss 0.0199 (0.5103) Accuracy 0.9934.
[2025-07-18 09:46:39,107 INFO test_sccan.py line 250 3326053] Test: [430/1000] Data 0.001 (0.006) Batch 0.049 (0.057) Remain 00:00:32 Loss 1.2173 (0.5128) Accuracy 0.7662.
[2025-07-18 09:46:39,598 INFO test_sccan.py line 250 3326053] Test: [440/1000] Data 0.001 (0.006) Batch 0.051 (0.057) Remain 00:00:31 Loss 0.0047 (0.5090) Accuracy 0.9987.
[2025-07-18 09:46:40,340 INFO test_sccan.py line 250 3326053] Test: [455/1000] Data 0.001 (0.006) Batch 0.049 (0.057) Remain 00:00:30 Loss 0.5973 (0.5229) Accuracy 0.8576.
[2025-07-18 09:46:40,835 INFO test_sccan.py line 250 3326053] Test: [465/1000] Data 0.001 (0.006) Batch 0.049 (0.056) Remain 00:00:30 Loss 0.1837 (0.5252) Accuracy 0.9356.
[2025-07-18 09:46:41,327 INFO test_sccan.py line 250 3326053] Test: [475/1000] Data 0.001 (0.006) Batch 0.049 (0.056) Remain 00:00:29 Loss 0.0197 (0.5206) Accuracy 0.9930.
[2025-07-18 09:46:41,818 INFO test_sccan.py line 250 3326053] Test: [485/1000] Data 0.001 (0.006) Batch 0.049 (0.056) Remain 00:00:28 Loss 0.7216 (0.5275) Accuracy 0.9345.
[2025-07-18 09:46:42,309 INFO test_sccan.py line 250 3326053] Test: [495/1000] Data 0.001 (0.005) Batch 0.050 (0.056) Remain 00:00:28 Loss 0.0056 (0.5344) Accuracy 0.9981.
[2025-07-18 09:46:42,801 INFO test_sccan.py line 250 3326053] Test: [505/1000] Data 0.001 (0.005) Batch 0.048 (0.056) Remain 00:00:27 Loss 0.0226 (0.5422) Accuracy 0.9906.
[2025-07-18 09:46:43,297 INFO test_sccan.py line 250 3326053] Test: [515/1000] Data 0.001 (0.005) Batch 0.049 (0.056) Remain 00:00:27 Loss 0.0710 (0.5415) Accuracy 0.9735.
[2025-07-18 09:46:43,791 INFO test_sccan.py line 250 3326053] Test: [525/1000] Data 0.001 (0.005) Batch 0.049 (0.056) Remain 00:00:26 Loss 0.1912 (0.5416) Accuracy 0.9395.
[2025-07-18 09:46:44,283 INFO test_sccan.py line 250 3326053] Test: [535/1000] Data 0.001 (0.005) Batch 0.050 (0.056) Remain 00:00:25 Loss 1.6965 (0.5427) Accuracy 0.5972.
[2025-07-18 09:46:44,776 INFO test_sccan.py line 250 3326053] Test: [545/1000] Data 0.001 (0.005) Batch 0.048 (0.055) Remain 00:00:25 Loss 0.5804 (0.5355) Accuracy 0.8463.
[2025-07-18 09:46:45,269 INFO test_sccan.py line 250 3326053] Test: [555/1000] Data 0.001 (0.005) Batch 0.050 (0.055) Remain 00:00:24 Loss 0.0486 (0.5297) Accuracy 0.9739.
[2025-07-18 09:46:45,763 INFO test_sccan.py line 250 3326053] Test: [565/1000] Data 0.001 (0.005) Batch 0.049 (0.055) Remain 00:00:24 Loss 1.7809 (0.5260) Accuracy 0.7676.
[2025-07-18 09:46:46,257 INFO test_sccan.py line 250 3326053] Test: [575/1000] Data 0.001 (0.005) Batch 0.050 (0.055) Remain 00:00:23 Loss 0.0416 (0.5278) Accuracy 0.9807.
[2025-07-18 09:46:46,750 INFO test_sccan.py line 250 3326053] Test: [585/1000] Data 0.001 (0.005) Batch 0.049 (0.055) Remain 00:00:22 Loss 0.0114 (0.5227) Accuracy 0.9962.
[2025-07-18 09:46:47,243 INFO test_sccan.py line 250 3326053] Test: [595/1000] Data 0.001 (0.005) Batch 0.050 (0.055) Remain 00:00:22 Loss 0.0092 (0.5173) Accuracy 0.9967.
[2025-07-18 09:46:47,738 INFO test_sccan.py line 250 3326053] Test: [605/1000] Data 0.001 (0.005) Batch 0.049 (0.055) Remain 00:00:21 Loss 0.3604 (0.5122) Accuracy 0.8369.
[2025-07-18 09:46:48,234 INFO test_sccan.py line 250 3326053] Test: [615/1000] Data 0.001 (0.005) Batch 0.054 (0.055) Remain 00:00:21 Loss 0.2160 (0.5114) Accuracy 0.9271.
[2025-07-18 09:46:48,722 INFO test_sccan.py line 250 3326053] Test: [625/1000] Data 0.001 (0.005) Batch 0.048 (0.055) Remain 00:00:20 Loss 0.0134 (0.5206) Accuracy 0.9968.
[2025-07-18 09:46:49,209 INFO test_sccan.py line 250 3326053] Test: [635/1000] Data 0.001 (0.004) Batch 0.050 (0.055) Remain 00:00:19 Loss 0.7165 (0.5158) Accuracy 0.8328.
[2025-07-18 09:46:49,692 INFO test_sccan.py line 250 3326053] Test: [645/1000] Data 0.001 (0.004) Batch 0.046 (0.054) Remain 00:00:19 Loss 0.0796 (0.5159) Accuracy 0.9864.
[2025-07-18 09:46:50,182 INFO test_sccan.py line 250 3326053] Test: [655/1000] Data 0.001 (0.004) Batch 0.050 (0.054) Remain 00:00:18 Loss 0.0336 (0.5268) Accuracy 0.9890.
[2025-07-18 09:46:50,670 INFO test_sccan.py line 250 3326053] Test: [665/1000] Data 0.001 (0.004) Batch 0.048 (0.054) Remain 00:00:18 Loss 0.1327 (0.5266) Accuracy 0.9743.
[2025-07-18 09:46:51,157 INFO test_sccan.py line 250 3326053] Test: [675/1000] Data 0.001 (0.004) Batch 0.050 (0.054) Remain 00:00:17 Loss 0.4548 (0.5272) Accuracy 0.8667.
[2025-07-18 09:46:51,643 INFO test_sccan.py line 250 3326053] Test: [685/1000] Data 0.001 (0.004) Batch 0.047 (0.054) Remain 00:00:17 Loss 2.2301 (0.5258) Accuracy 0.6181.
[2025-07-18 09:46:52,130 INFO test_sccan.py line 250 3326053] Test: [695/1000] Data 0.001 (0.004) Batch 0.050 (0.054) Remain 00:00:16 Loss 0.0538 (0.5224) Accuracy 0.9836.
[2025-07-18 09:46:52,616 INFO test_sccan.py line 250 3326053] Test: [705/1000] Data 0.001 (0.004) Batch 0.048 (0.054) Remain 00:00:15 Loss 0.0074 (0.5210) Accuracy 0.9974.
[2025-07-18 09:46:53,106 INFO test_sccan.py line 250 3326053] Test: [715/1000] Data 0.001 (0.004) Batch 0.050 (0.054) Remain 00:00:15 Loss 0.1315 (0.5167) Accuracy 0.9893.
[2025-07-18 09:46:53,588 INFO test_sccan.py line 250 3326053] Test: [725/1000] Data 0.001 (0.004) Batch 0.047 (0.054) Remain 00:00:14 Loss 0.0069 (0.5119) Accuracy 0.9983.
[2025-07-18 09:46:54,080 INFO test_sccan.py line 250 3326053] Test: [735/1000] Data 0.001 (0.004) Batch 0.049 (0.054) Remain 00:00:14 Loss 0.0369 (0.5217) Accuracy 0.9855.
[2025-07-18 09:46:54,567 INFO test_sccan.py line 250 3326053] Test: [745/1000] Data 0.001 (0.004) Batch 0.048 (0.054) Remain 00:00:13 Loss 0.0183 (0.5190) Accuracy 0.9953.
[2025-07-18 09:46:55,059 INFO test_sccan.py line 250 3326053] Test: [755/1000] Data 0.001 (0.004) Batch 0.050 (0.054) Remain 00:00:13 Loss 0.1299 (0.5143) Accuracy 0.9579.
[2025-07-18 09:46:55,543 INFO test_sccan.py line 250 3326053] Test: [765/1000] Data 0.001 (0.004) Batch 0.046 (0.054) Remain 00:00:12 Loss 0.0026 (0.5120) Accuracy 0.9992.
[2025-07-18 09:46:56,030 INFO test_sccan.py line 250 3326053] Test: [775/1000] Data 0.001 (0.004) Batch 0.050 (0.054) Remain 00:00:12 Loss 0.7878 (0.5173) Accuracy 0.8756.
[2025-07-18 09:46:56,515 INFO test_sccan.py line 250 3326053] Test: [785/1000] Data 0.001 (0.004) Batch 0.048 (0.053) Remain 00:00:11 Loss 0.0815 (0.5164) Accuracy 0.9815.
[2025-07-18 09:46:57,003 INFO test_sccan.py line 250 3326053] Test: [795/1000] Data 0.001 (0.004) Batch 0.050 (0.053) Remain 00:00:10 Loss 0.5571 (0.5117) Accuracy 0.8996.
[2025-07-18 09:46:57,487 INFO test_sccan.py line 250 3326053] Test: [805/1000] Data 0.001 (0.004) Batch 0.047 (0.053) Remain 00:00:10 Loss 0.0081 (0.5087) Accuracy 0.9972.
[2025-07-18 09:46:57,975 INFO test_sccan.py line 250 3326053] Test: [815/1000] Data 0.001 (0.004) Batch 0.050 (0.053) Remain 00:00:09 Loss 0.0459 (0.5077) Accuracy 0.9889.
[2025-07-18 09:46:58,459 INFO test_sccan.py line 250 3326053] Test: [825/1000] Data 0.001 (0.004) Batch 0.049 (0.053) Remain 00:00:09 Loss 3.8083 (0.5100) Accuracy 0.5594.
[2025-07-18 09:46:58,949 INFO test_sccan.py line 250 3326053] Test: [835/1000] Data 0.001 (0.004) Batch 0.050 (0.053) Remain 00:00:08 Loss 0.4058 (0.5147) Accuracy 0.8848.
[2025-07-18 09:46:59,432 INFO test_sccan.py line 250 3326053] Test: [845/1000] Data 0.001 (0.004) Batch 0.046 (0.053) Remain 00:00:08 Loss 0.2829 (0.5106) Accuracy 0.9657.
[2025-07-18 09:46:59,916 INFO test_sccan.py line 250 3326053] Test: [855/1000] Data 0.001 (0.004) Batch 0.048 (0.053) Remain 00:00:07 Loss 0.0221 (0.5097) Accuracy 0.9931.
[2025-07-18 09:47:00,402 INFO test_sccan.py line 250 3326053] Test: [865/1000] Data 0.001 (0.003) Batch 0.048 (0.053) Remain 00:00:07 Loss 0.0177 (0.5064) Accuracy 0.9936.
[2025-07-18 09:47:00,890 INFO test_sccan.py line 250 3326053] Test: [875/1000] Data 0.001 (0.003) Batch 0.050 (0.053) Remain 00:00:06 Loss 1.3074 (0.5079) Accuracy 0.7814.
[2025-07-18 09:47:01,377 INFO test_sccan.py line 250 3326053] Test: [885/1000] Data 0.001 (0.003) Batch 0.049 (0.053) Remain 00:00:06 Loss 0.0047 (0.5070) Accuracy 0.9987.
[2025-07-18 09:47:02,111 INFO test_sccan.py line 250 3326053] Test: [900/1000] Data 0.001 (0.003) Batch 0.051 (0.053) Remain 00:00:05 Loss 0.5840 (0.5137) Accuracy 0.8830.
[2025-07-18 09:47:02,595 INFO test_sccan.py line 250 3326053] Test: [910/1000] Data 0.001 (0.003) Batch 0.046 (0.053) Remain 00:00:04 Loss 0.1888 (0.5151) Accuracy 0.9367.
[2025-07-18 09:47:03,081 INFO test_sccan.py line 250 3326053] Test: [920/1000] Data 0.001 (0.003) Batch 0.049 (0.053) Remain 00:00:04 Loss 0.0194 (0.5129) Accuracy 0.9932.
[2025-07-18 09:47:03,567 INFO test_sccan.py line 250 3326053] Test: [930/1000] Data 0.001 (0.003) Batch 0.049 (0.053) Remain 00:00:03 Loss 0.6808 (0.5127) Accuracy 0.9348.
[2025-07-18 09:47:04,060 INFO test_sccan.py line 250 3326053] Test: [940/1000] Data 0.001 (0.003) Batch 0.051 (0.053) Remain 00:00:03 Loss 0.0088 (0.5146) Accuracy 0.9956.
[2025-07-18 09:47:04,552 INFO test_sccan.py line 250 3326053] Test: [950/1000] Data 0.001 (0.003) Batch 0.047 (0.053) Remain 00:00:02 Loss 0.0176 (0.5139) Accuracy 0.9927.
[2025-07-18 09:47:05,046 INFO test_sccan.py line 250 3326053] Test: [960/1000] Data 0.001 (0.003) Batch 0.049 (0.053) Remain 00:00:02 Loss 0.2217 (0.5140) Accuracy 0.9215.
[2025-07-18 09:47:05,535 INFO test_sccan.py line 250 3326053] Test: [970/1000] Data 0.001 (0.003) Batch 0.050 (0.053) Remain 00:00:01 Loss 0.6956 (0.5120) Accuracy 0.7327.
[2025-07-18 09:47:06,025 INFO test_sccan.py line 250 3326053] Test: [980/1000] Data 0.001 (0.003) Batch 0.050 (0.053) Remain 00:00:01 Loss 1.5213 (0.5136) Accuracy 0.6383.
[2025-07-18 09:47:06,509 INFO test_sccan.py line 250 3326053] Test: [990/1000] Data 0.001 (0.003) Batch 0.047 (0.052) Remain 00:00:00 Loss 0.0249 (0.5094) Accuracy 0.9900.
[2025-07-18 09:47:06,998 INFO test_sccan.py line 250 3326053] Test: [1000/1000] Data 0.001 (0.003) Batch 0.049 (0.052) Remain 00:00:00 Loss 0.0425 (0.5051) Accuracy 0.9781.
[2025-07-18 09:47:06,999 INFO test_sccan.py line 264 3326053] meanIoU---Val result: mIoU 0.7188.
[2025-07-18 09:47:06,999 INFO test_sccan.py line 265 3326053] <<<<<<< Novel Results <<<<<<<
[2025-07-18 09:47:06,999 INFO test_sccan.py line 267 3326053] Class_1 Result: iou 0.8952.
[2025-07-18 09:47:07,000 INFO test_sccan.py line 267 3326053] Class_2 Result: iou 0.6194.
[2025-07-18 09:47:07,000 INFO test_sccan.py line 267 3326053] Class_3 Result: iou 0.8860.
[2025-07-18 09:47:07,000 INFO test_sccan.py line 267 3326053] Class_4 Result: iou 0.2650.
[2025-07-18 09:47:07,000 INFO test_sccan.py line 267 3326053] Class_5 Result: iou 0.9286.
[2025-07-18 09:47:07,000 INFO test_sccan.py line 269 3326053] FBIoU---Val result: FBIoU 0.8010.
[2025-07-18 09:47:07,000 INFO test_sccan.py line 271 3326053] Class_0 Result: iou_f 0.8915.
[2025-07-18 09:47:07,000 INFO test_sccan.py line 271 3326053] Class_1 Result: iou_f 0.7105.
[2025-07-18 09:47:07,000 INFO test_sccan.py line 272 3326053] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
total time: 52.4349, avg inference time: 0.0111, count: 1000

Total running time: 00h 00m 52s
Seed0: 123
Seed:  [321]
mIoU:  [0.7188]
FBIoU: [0.801]
pIoU:  [0.7105]
-------------------------------------------
Best_Seed_m: 321 	 Best_Seed_F: 321 	 Best_Seed_p: 321
Best_mIoU: 0.7188 	 Best_FBIoU: 0.8010 	 Best_pIoU: 0.7105
Mean_mIoU: 0.7188 	 Mean_FBIoU: 0.8010 	 Mean_pIoU: 0.7105
